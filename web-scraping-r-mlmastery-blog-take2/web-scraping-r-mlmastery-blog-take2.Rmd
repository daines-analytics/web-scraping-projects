---
title: "Web Scraping of Machine Learning Blog Entries Using R Take 2"
author: "David Lowe"
date: "January 16, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.

INTRODUCTION: Dr. Jason Brownlee's Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery's web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.

Starting URLs: https://machinelearningmastery.com/blog

## Loading Libraries and Packages

```{r LIBRARY}
startTimeScript <- proc.time()
library(httr)
library(rvest)
library(jsonlite)
library(stringr)
library(mailR)
```

### Setting up the email notification function

```{r}
email_notify <- function(msg=""){
  sender <- "luozhi2488@gmail.com"
  receiver <- "dave@contactdavidlowe.com"
  sbj_line <- "Notification from R Script"
  password <- readLines("../email_credential.txt")
  send.mail(
    from = sender,
    to = receiver,
    subject= sbj_line,
    body = msg,
    smtp = list(host.name = "smtp.gmail.com", port = 465, user.name = sender, passwd = password, ssl = TRUE),
    authenticate = TRUE,
    send = TRUE)
}
```

## Setting up the necessary parameters

```{r INPUT}
# Specifying the URL of desired web page to be scrapped
blog_url <- 'https://machinelearningmastery.com/blog'
```

```{r}
email_notify(paste("The web scraping process has begun!",date()))
```

## Performing the Scraping and Processing

```{r ITEM EXTRACTION}
dataset_listing_df <- data.frame(title=character(0), author=character(0), post_date=character(0), link_url=character(0), description=character(0))

done <- FALSE

# Creating a html document from the URL
cat("Trying to access the starting web page: ",blog_url,"\n")
webpage <- read_html(blog_url)

while (!done){
  if (http_status(GET(blog_url))$reason=="OK"){
    dataset_records_css <- html_nodes(webpage, 'article')
    for (item in dataset_records_css) {
    
      title_css <- html_node(item, 'header > h2 > a')
      title <- html_text(title_css, trim=T)
  
      link_url <- html_attr(title_css, 'href')

      author_css <- html_node(item, 'div.post-meta span span.fn')
      author <- html_text(author_css, trim=T)
    
      post_date_css <- html_node(item, 'div.post-meta abbr')
      post_date <- html_text(post_date_css, trim=T)
    
      description_css <- html_node(item, 'section p')
      description <- html_text(description_css, trim=T)
  
      each_record_df <- data.frame(title, author, post_date, link_url, description)
      dataset_listing_df <- rbind(dataset_listing_df, each_record_df)
    }

    # Adding random wait time so we do not hammer the website needlessly
    waitTime <- runif(1, min=3, max=6)
    Sys.sleep(waitTime)
    cat("Waited", waitTime, "seconds before trying to retrieve the next URL\n")
    older_posts_css <- html_node(webpage, 'a.next.page-numbers')
    if (length(older_posts_css)!=0) {
      next_page_url <- html_attr(older_posts_css, 'href')
      cat("Currently accessing and parsing web page: ",next_page_url,"\n")
      webpage <- read_html(next_page_url)
      dataset_records_css <- html_nodes(webpage, 'article')
    }
    else {
      done <- TRUE
    }
  }
  else {
    done <- TRUE
  }
}
```

## Organizing Data and Producing Outputs

```{r OUTPUT}
cat("Number of blog items processed:", nrow(dataset_listing_df), "\n")
dataset_listing_json <- toJSON(dataset_listing_df)
write_json(dataset_listing_df, "web-scraping-r-mlmastery-blog-take2.json")
cat(dataset_listing_json)
```

```{r}
email_notify(paste("The web scraping process has completed!",date()))
proc.time()-startTimeScript
```