[{"blog_title":"Updated Machine Learning Templates v10 for Python","author_name":"David Lowe","blog_date_text":"Mon, 17 Jun 2019 12:53:55 +0000","blog_url":"https://dainesanalytics.blog/2019/06/17/updated-machine-learning-templates-v10-for-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br><br><br><br>Version 10 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added or updated the sample code to support:<br><br><br><br>Moved some of the data cleaning code to the<br>beginning of the scripts and included more sample code for data cleaning.Added a flag to stop sending status emails<br>(useful when testing of the scripts or when the email server is not available<br>for use)You will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Daines Analytics Blog Using BeautifulSoup Take 2","author_name":"David Lowe","blog_date_text":"Sun, 16 Jun 2019 12:41:37 +0000","blog_url":"https://dainesanalytics.blog/2019/06/16/web-scraping-of-daines-analytics-blog-using-beautifulsoup-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog.<br>The purpose of this exercise is to practice web scraping by gathering the blog<br>entries from Daines Analytics’ RSS feed. The script automatically traverses the<br>RSS feed to capture all blog entries in a JSON document.<br><br><br><br>For this second iteration, the script also will store the captured<br>information in a remote relational database.<br><br><br><br>Starting URLs: https://dainesanalytics.blog/feed or<br>https://dainesanalytics.blog/feed/?paged=1<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Coronary Artery Disease Using Python Take 2","author_name":"David Lowe","blog_date_text":"Fri, 14 Jun 2019 12:45:25 +0000","blog_url":"https://dainesanalytics.blog/2019/06/14/binary-classification-model-for-coronary-artery-disease-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Z-Alizadeh Sani CAD dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The researchers collected the data file for<br>coronary artery disease (CAD) diagnosis. Each patient could be in two possible<br>categories CAD or Normal. A patient is categorized as CAD, if his/her diameter<br>narrowing is greater than or equal to 50%, and otherwise as Normal. The<br>Z-Alizadeh Sani dataset contains the records of 303 patients, each of which has<br>59 features. The features can belong to one of four groups: demographic,<br>symptom and examination, ECG, and laboratory and echo features. In this<br>extension, the researchers add three features for the LAD, LCX, and RCA arteries.<br>CAD becomes true when at least one of these three arteries is stenotic. To properly<br>use this dataset for CAD classification only one of LAD, LCX, RCA or Cath<br>(Result of angiography) can be present in the dataset. This dataset not only<br>can be used for CAD detection, but also stenosis diagnosis of each LAD, LCX and<br>RCA arteries.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling.<br><br><br><br>In this iteration, we will examine the prediction accuracy<br>of the models for the three arteries (LAD, LCX, and RCA). We hope to gain<br>further insights on whether the data and models can be used to predict the<br>overall result of angiography by examining the major arteries individually.<br><br><br><br>CONCLUSION: For this iteration, predicting the result of<br>angiography appears to work the best by using the LAD artery. The model using<br>the LAD readings produced an accuracy rate of 83.51% on the test dataset. The<br>model using the LCX readings produced an accuracy rate of 71.42% on the test<br>dataset. The model using the RCA readings produced an accuracy rate of 65.93%<br>on the test dataset. For this dataset, using the LAD artery data and the Random<br>Forest algorithm should be considered for further modeling.<br><br><br><br>Dataset Used: Z-Alizadeh Sani Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/extention+of+Z-Alizadeh+sani+dataset<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Coronary Artery Disease Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 12 Jun 2019 12:50:16 +0000","blog_url":"https://dainesanalytics.blog/2019/06/12/binary-classification-model-for-coronary-artery-disease-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Z-Alizadeh Sani CAD dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The researchers collected the data file for<br>coronary artery disease (CAD) diagnosis. Each patient could be in two possible<br>categories CAD or Normal. A patient is categorized as CAD, if his/her diameter<br>narrowing is greater than or equal to 50%, and otherwise as Normal. The<br>Z-Alizadeh Sani dataset contains the records of 303 patients, each of which has<br>59 features. The features can belong to one of four groups: demographic,<br>symptom and examination, ECG, and laboratory and echo features. In this<br>extension, the researchers add three features for the LAD, LCX, and RCA arteries.<br>CAD becomes true when at least one of these three arteries is stenotic. To properly<br>use this dataset for CAD classification only one of LAD, LCX, RCA or Cath<br>(Result of angiography) can be present in the dataset. This dataset not only<br>can be used for CAD detection, but also stenosis diagnosis of each LAD, LCX and<br>RCA arteries.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling.<br><br><br><br>In this iteration, we will examine the prediction accuracy<br>of the models for the three arteries (LAD, LCX, and RCA). We hope to gain<br>further insights on whether the data and models can be used to predict the<br>overall result of angiography by examining the major arteries individually.<br><br><br><br>CONCLUSION: For this iteration, predicting the result of<br>angiography appears to work the best by using the LAD artery. The model using<br>the LAD readings produced an accuracy rate of 77.78% on the test dataset. The<br>model using the LCX readings produced an accuracy rate of 74.44% on the test<br>dataset. The model using the RCA readings produced an accuracy rate of 66.67%<br>on the test dataset. For this dataset, using the LAD artery data and the Random<br>Forest algorithm could be considered for further modeling.<br><br><br><br>Dataset Used: Z-Alizadeh Sani Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/extention+of+Z-Alizadeh+sani+dataset<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Coronary Artery Disease Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 10 Jun 2019 12:09:46 +0000","blog_url":"https://dainesanalytics.blog/2019/06/10/binary-classification-model-for-coronary-artery-disease-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Z-Alizadeh Sani CAD dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The researchers collected the data file for<br>coronary artery disease (CAD) diagnosis. Each patient could be in two possible<br>categories CAD or Normal. A patient is categorized as CAD, if his/her diameter<br>narrowing is greater than or equal to 50%, and otherwise as Normal. The<br>Z-Alizadeh Sani dataset contains the records of 303 patients, each of which has<br>59 features. The features can belong to one of four groups: demographic,<br>symptom and examination, ECG, and laboratory and echo features. In this<br>extension, the researchers add three features for the LAD, LCX, and RCA arteries.<br>CAD becomes true when at least one of these three arteries is stenotic. To properly<br>use this dataset for CAD classification only one of LAD, LCX, RCA or Cath<br>(Result of angiography) can be present in the dataset. This dataset not only<br>can be used for CAD detection, but also stenosis diagnosis of each LAD, LCX and<br>RCA arteries.<br><br><br><br>In this iteration, we plan to establish the baseline<br>prediction accuracy for further takes of modeling.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 79.88%. Two algorithms (Extra Trees<br>and Gradient Boosting) achieved the top accuracy metrics after the first round<br>of modeling. After a series of tuning trials, Extra Trees turned in the top<br>overall result and achieved an accuracy metric of 84.43%. By using the<br>optimized parameters, the Extra Trees algorithm processed the testing dataset<br>with an accuracy of 89.01%, which was even better than the prediction accuracy<br>gained from the training data.<br><br><br><br>CONCLUSION: For this iteration, the Extra Trees algorithm<br>achieved the best overall training and validation results. For this dataset,<br>the Extra Trees algorithm could be considered for further modeling.<br><br><br><br>Dataset Used: Z-Alizadeh Sani Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/extention+of+Z-Alizadeh+sani+dataset<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of SAS Global Forum 2019 Proceedings Using R","author_name":"David Lowe","blog_date_text":"Sun, 09 Jun 2019 12:06:16 +0000","blog_url":"https://dainesanalytics.blog/2019/06/09/web-scraping-of-sas-global-forum-2019-proceedings-using-r/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by<br>extracting specific pieces of information from a website. The web scraping R<br>code leverages the rvest package.<br><br><br><br>INTRODUCTION: On occasions we need to download a batch of documents listed<br>on a single web page without clicking on the download link one at a time. This<br>web scraping script will automatically traverse through the entire web page and<br>collect all links to the PDF documents. The script will also download the PDF<br>documents as part of the scraping process.<br><br><br><br>Starting URLs:<br>https://www.sas.com/en_us/events/sas-global-forum/program/proceedings.html<br><br><br><br>The source code and HTML output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 5","author_name":"David Lowe","blog_date_text":"Fri, 07 Jun 2019 12:44:13 +0000","blog_url":"https://dainesanalytics.blog/2019/06/07/binary-classification-model-for-heart-disease-study-using-r-take-5/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take3, we examined the Switzerland dataset and<br>created an Extra Trees model to fit the data.<br><br><br><br>In iteration Take4, we examined the Long Beach VA dataset<br>and created an Extra Trees model to fit the data.<br><br><br><br>In this iteration, we will combine all four datasets and<br>create a machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 80.56%. Two algorithms (Random<br>Forest and Gradient Boosting) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Gradient Boosting turned in<br>the top overall result and achieved an accuracy metric of 82.84%. By using the<br>optimized parameters, the Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 77.82%, which was somewhat below the prediction<br>accuracy gained from the training data and possibly due to overfitting.<br><br><br><br>CONCLUSION: For the combined dataset, the Gradient Boosting<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Gradient Boosting could be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 4","author_name":"David Lowe","blog_date_text":"Thu, 06 Jun 2019 12:22:34 +0000","blog_url":"https://dainesanalytics.blog/2019/06/06/binary-classification-model-for-heart-disease-study-using-r-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a machine learning model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and<br>created a machine learning model to fit the data.<br><br><br><br>In iteration Take3, we examined the Switzerland dataset and<br>created a machine learning model to fit the data.<br><br><br><br>In this iteration, we will examine the Long Beach VA dataset<br>and create a machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 73.74%. Two algorithms (Random<br>Forest and Gradient Boosting) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Random Forest turned in the<br>top overall result and achieved an accuracy metric of 74.71%. By using the<br>optimized parameters, the Random Forest algorithm processed the testing dataset<br>with an accuracy of 79.66%, which was even better than the prediction accuracy<br>gained from the training data.<br><br><br><br>CONCLUSION: For the Long Beach VA dataset, the Random Forest<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Random Forest could be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 3","author_name":"David Lowe","blog_date_text":"Wed, 05 Jun 2019 12:49:03 +0000","blog_url":"https://dainesanalytics.blog/2019/06/05/binary-classification-model-for-heart-disease-study-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and created a machine learning model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and created a machine learning model to fit the data.<br><br><br><br>In this iteration, we will examine the Switzerland dataset<br>and create a machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 90.93%. Two algorithms (Decision<br>Trees and Gradient Boosting) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Gradient Boosting turned in<br>the top overall result and achieved an accuracy metric of 93.40%. By using the<br>optimized parameters, the Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 94.40%, which was even better than the prediction<br>accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the Switzerland dataset, the Gradient<br>Boosting algorithm achieved the best overall results using the training and<br>testing datasets. For this dataset, Gradient Boosting could be considered for<br>further modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 2","author_name":"David Lowe","blog_date_text":"Tue, 04 Jun 2019 12:33:37 +0000","blog_url":"https://dainesanalytics.blog/2019/06/04/binary-classification-model-for-heart-disease-study-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and created a machine learning model to fit the data.<br><br><br><br>In this iteration, we will examine the Hungarian dataset and<br>create a machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 79.33%. Two algorithms (Logistic<br>Regression and Gradient Boosting) achieved the top accuracy metrics after the<br>first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 82.13%. By<br>using the optimized parameters, the Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 82.76%, which was even better than the<br>prediction accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the Cleveland dataset, the Gradient Boosting<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Gradient Boosting could be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 03 Jun 2019 12:10:36 +0000","blog_url":"https://dainesanalytics.blog/2019/06/03/binary-classification-model-for-heart-disease-study-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In this iteration, we will examine the Cleveland dataset and<br>create a machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 84.78%. Two algorithms (Random<br>Forest and Gradient Boosting) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Gradient Boosting turned in<br>the top overall result and achieved an accuracy metric of 87.95%. By using the<br>optimized parameters, the Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 82.22%, which was slightly below the prediction<br>accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the Cleveland dataset, the Gradient Boosting<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Gradient Boosting could be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of SAS Global Forum 2019 Proceedings Using BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 02 Jun 2019 12:30:36 +0000","blog_url":"https://dainesanalytics.blog/2019/06/02/web-scraping-of-sas-global-forum-2019-proceedings-using-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by<br>extracting specific pieces of information from a website. The web scraping<br>python code leverages the BeautifulSoup module.<br><br><br><br>INTRODUCTION: On occasions we need to download a batch of documents listed<br>on a single web page without clicking on the download link one at a time. This<br>web scraping script will automatically traverse through the entire web page and<br>collect all links to the PDF documents. The script will also download the PDF<br>documents as part of the scraping process.<br><br><br><br>Starting URLs: https://www.sas.com/en_us/events/sas-global-forum/program/proceedings.html<br><br><br><br>The source code and HTML output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Coronary Artery Disease Using R Take 1","author_name":"David Lowe","blog_date_text":"Fri, 31 May 2019 12:09:35 +0000","blog_url":"https://dainesanalytics.blog/2019/05/31/binary-classification-model-for-coronary-artery-disease-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Z-Alizadeh Sani CAD dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The researchers collected the data file for<br>coronary artery disease (CAD) diagnosis. Each patient could be in two possible<br>categories CAD or Normal. A patient is categorized as CAD, if his/her diameter<br>narrowing is greater than or equal to 50%, and otherwise as Normal. The<br>Z-Alizadeh Sani dataset contains the records of 303 patients, each of which has<br>59 features. The features can belong to one of four groups: demographic,<br>symptom and examination, ECG, and laboratory and echo features. In this<br>extension, the researchers add three features for the LAD, LCX, and RCA arteries.<br>CAD becomes true when at least one of these three arteries is stenotic. To properly<br>use this dataset for CAD classification only one of LAD, LCX, RCA or Cath<br>(Result of angiography) can be present in the dataset. This dataset not only<br>can be used for CAD detection, but also stenosis diagnosis of each LAD, LCX and<br>RCA arteries.<br><br><br><br>In this iteration, we plan to establish the baseline<br>prediction accuracy for further takes of modeling.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 83.07%. Two algorithms (Random<br>Forest and Gradient Boosting) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Gradient Boosting turned in<br>the top overall result and achieved an accuracy metric of 89.19%. By using the<br>optimized parameters, the Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 77.78%, which was significantly below the<br>prediction accuracy gained from the training data and possibly due to<br>over-fitting.<br><br><br><br>CONCLUSION: For this iteration, the Gradient Boosting<br>algorithm achieved the best overall training and validation results. For this<br>dataset, the Gradient Boosting algorithm could be considered for further<br>modeling.<br><br><br><br>Dataset Used: Z-Alizadeh Sani Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/extention+of+Z-Alizadeh+sani+dataset<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Using Docker to Build Data Science Environments with RStudio","author_name":"David Lowe","blog_date_text":"Wed, 29 May 2019 12:13:00 +0000","blog_url":"https://dainesanalytics.blog/2019/05/29/using-docker-to-build-data-science-environments-with-rstudio/","blog_text":"I have been using Docker to create environments for data<br>science work. With Docker, I was able to painlessly create the environments<br>with a degree of accuracy and consistency. After getting exposed to using<br>Docker for environment creation, it is hard to imagine doing it any other ways.<br><br><br><br>For more information, I encourage you to check out the Rocker images and this blog post about using RStudio and Docker.<br><br><br><br>Step 1: Create VM and update OS as necessary<br><br><br><br>I created virtual machines on VMware using CentOS 7 and make<br>it accessible through bridged networking. I also used CentOS’ minimum<br>installation as it just needs the basic components to run Docker. We will need<br>to access the VM via SSH with the port<br>8787 opened for the RStudio server instance.<br><br><br><br>Step 2: Access the VM via SSH (through a non-root user call docker_admin) and install Git with the sudo command.<br><br><br><br>Step 3: Install Docker for the non-root docker_admin user. Verify the installation with the command “docker image ls.”<br><br><br><br>More information on installing Docker CE can be found at here and here.<br><br><br><br>It boils down to:<br><br><br><br>sudo yum install -y yum-utils device-mapper-persistent-data lvm2<br><br><br><br>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo<br><br><br><br>sudo yum -y install docker-ce<br><br><br><br>sudo systemctl start docker && sudo systemctl enable docker<br><br><br><br>sudo usermod -aG docker docker_admin<br><br><br><br>Step 4: For my environments, I need to clone some R template scripts. This step is not mandatory if you do not require it.<br><br><br><br>git clone https://github.com/daines-analytics/template-latest.git examples<br><br><br><br>For my environments, I also need to make some environment<br>variables accessible to the scripts. Again, this step may not be mandatory for<br>your installation.<br><br><br><br>scp .Renviron cloud_user@<IP_Address>:/home/cloud_user<br><br><br><br>Step 5: Create<br>the Dockerfile or use the one from the template directory<br><br><br><br>FROM rocker/verse<br>LABEL com.dainesanalytics.rstudio.version=v1.0<br>RUN Rscript -e \"install.packages(c('knitr', 'tidyverse', 'caret', 'corrplot', 'mailR', 'DMwR', 'ROCR', 'Hmisc', 'randomForest', 'e1071', 'elasticnet', 'gbm', 'xgboost'))\"<br>COPY --chown=rstudio:rstudio .Renviron /home/rstudio<br>COPY --chown=rstudio:rstudio examples/ /home/rstudio<br><br><br><br>My<br>environments require many of the machine learning packages, but these<br>packages may not be mandatory for your installation.<br><br><br><br>Step 6: Build the Docker image with the command:<br><br><br><br>docker image build -t rstudio/nonroot:v1 .<br><br><br><br>Step 7: Run the Docker container with the command:<br><br><br><br>docker container run --rm -e PASSWORD=rserver -p 8787:8787 --name<br>rstudio-server rstudio/nonroot:v1<br><br><br><br>The password can be any string, and the RStudio Server just<br>requires one.<br><br><br><br>Step 8: After we are done with the container and/or the virtual machine, we can shut down the container with the command:<br><br><br><br>docker container stop [container ID]<br><br><br><br>The templates (R and Docker) can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Using Docker to Build Data Science Environments with Anaconda","author_name":"David Lowe","blog_date_text":"Mon, 27 May 2019 12:56:05 +0000","blog_url":"https://dainesanalytics.blog/2019/05/27/using-docker-to-build-data-science-environments-with-anaconda/","blog_text":"I have been using Docker to create environments for data<br>science work. With Docker, I was able to painlessly create the environments<br>with a degree of accuracy and consistency. After getting exposed to using<br>Docker for environment creation, it is hard to imagine doing it any other ways.<br><br><br><br>For more information, I encourage you to check out the Anaconda images and this blog post about using Anaconda and Docker.<br><br><br><br>Step 1: Create VM and update OS as necessary<br><br><br><br>I created virtual machines on VMware using CentOS 7 and make it accessible through bridged networking. I also used CentOS’ minimum installation as it just needs the basic components to run Docker. We will need to access the VM via SSH with the port (8080 in my case) opened for the Jupyter notebook instance.<br><br><br><br>Step 2: Access the VM via SSH (through a non-root user call docker_admin) and install Git with the sudo command.<br><br><br><br>Step 3: Install Docker for the non-root docker_admin user. Verify the installation with the command “docker image ls.”<br><br><br><br>More information on installing Docker CE can be found at here and here.<br><br><br><br>It boils down to:<br><br><br><br>sudo yum install -y yum-utils device-mapper-persistent-data lvm2<br><br><br><br>sudo yum-config-manager --add-repo \\<br>  https://download.docker.com/linux/centos/docker-ce.repo<br><br><br><br>sudo yum -y install docker-ce<br><br><br><br>sudo systemctl start docker && sudo systemctl enable docker<br><br><br><br>sudo usermod -aG docker docker_admin<br><br><br><br>Step 4: For my environments, I need to clone some Python template scripts. This step is not mandatory if you do not require it.<br><br><br><br>git clone https://github.com/daines-analytics/template-latest.git examples<br><br><br><br>For my environments, I also need to make some environment variables accessible to the scripts. Again, this step may not be mandatory for your installation.<br><br><br><br>scp docker_env.txt cloud_user@<IP_Address>:/home/cloud_user<br><br><br><br>Step 5: Create the Dockerfile or use the one from the template directory<br><br><br><br>FROM continuumio/anaconda3<br>LABEL com.dainesanalytics.anaconda.version=v1.0<br>EXPOSE 8080<br>RUN conda install -c conda-forge -y --freeze-installed imbalanced-learn xgboost<br>RUN useradd -ms /bin/bash dev_user<br>USER dev_user<br>WORKDIR /home/dev_user<br>COPY --chown=dev_user:dev_user examples/ /home/dev_user<br>CMD /opt/conda/bin/jupyter notebook --ip=0.0.0.0 --port=8080 --no-browser --notebook-dir=/home/dev_user<br><br><br><br>Step 6: Build the Docker image with the command:<br><br><br><br>docker image build -t anaconda3/nonroot:v1 .<br><br><br><br>Step 7: Run the Docker container with the command:<br><br><br><br>docker container run --rm --env-file docker_env.txt -p 8080:8080<br>--name jupyter-server anaconda3/nonroot:v1<br><br><br><br>Step 8: After we are done with the container and/or the virtual machine, we can shut down the container with the command:<br><br><br><br>docker container stop [container ID]<br><br><br><br>The templates (Python and Docker) can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Daines Analytics Blog Using BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 26 May 2019 12:09:35 +0000","blog_url":"https://dainesanalytics.blog/2019/05/26/web-scraping-of-daines-analytics-blog-using-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by<br>gathering specific pieces of information from a website. The web scraping code<br>was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog.<br>The purpose of this exercise is to practice web scraping by gathering the blog<br>entries from Daines Analytics’ RSS feed. This iteration of the script<br>automatically traverses the RSS feed to capture all blog entries.<br><br><br><br>Starting URLs: https://dainesanalytics.blog/feed or<br>https://dainesanalytics.blog/feed/?paged=1<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 5","author_name":"David Lowe","blog_date_text":"Fri, 24 May 2019 12:39:16 +0000","blog_url":"https://dainesanalytics.blog/2019/05/24/binary-classification-model-for-heart-disease-study-using-python-take-5/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take3, we examined the Switzerland dataset and<br>created an Extra Trees model to fit the data.<br><br><br><br>In iteration Take4, we examined the Long Beach VA dataset<br>and created an Extra Trees model to fit the data.<br><br><br><br>In this iteration, we will combine all four datasets and<br>look for a suitable machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 76.33%. Two algorithms (Logistic<br>Regression and Gradient Boosting) achieved the top accuracy metrics after the<br>first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 80.43%. By<br>using the optimized parameters, the Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 80.79%, which was slightly better than the<br>prediction accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the combined dataset, the Gradient Boosting<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Gradient Boosting should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 4","author_name":"David Lowe","blog_date_text":"Thu, 23 May 2019 12:27:54 +0000","blog_url":"https://dainesanalytics.blog/2019/05/23/binary-classification-model-for-heart-disease-study-using-python-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take3, we examined the Switzerland dataset and<br>created an Extra Trees model to fit the data.<br><br><br><br>In this iteration, we will examine the Long Beach VA dataset<br>and look for a suitable machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 73.45%. Two algorithms (Logistic<br>Regression and Extra Trees) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Extra Trees turned in the<br>top overall result and achieved an accuracy metric of 77.85%. By using the<br>optimized parameters, the Extra Trees algorithm processed the testing dataset<br>with an accuracy of 66.66%, which was significantly below the prediction<br>accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the Long Beach VA dataset, the Extra Trees<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Extra Trees should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 22 May 2019 12:00:05 +0000","blog_url":"https://dainesanalytics.blog/2019/05/22/binary-classification-model-for-heart-disease-study-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In iteration Take2, we examined the Hungarian dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In this iteration, we will examine the Switzerland dataset<br>and look for a suitable machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 88.10%. Two algorithms (Extra Trees<br>and Gradient Boosting) achieved the top accuracy metrics after the first round<br>of modeling. After a series of tuning trials, Extra Trees turned in the top<br>overall result and achieved an accuracy metric of 93.02%. By using the<br>optimized parameters, the Extra Trees algorithm processed the testing dataset<br>with an accuracy of 91.89%, which was slightly below the prediction accuracy<br>gained from the training data.<br><br><br><br>CONCLUSION: For the Switzerland dataset, the Extra Trees<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Extra Trees should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 2","author_name":"David Lowe","blog_date_text":"Tue, 21 May 2019 12:08:30 +0000","blog_url":"https://dainesanalytics.blog/2019/05/21/binary-classification-model-for-heart-disease-study-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Heart Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>In iteration Take1, we examined the Cleveland dataset and<br>created a Logistic Regression model to fit the data.<br><br><br><br>In this iteration, we will examine the Hungarian dataset and<br>look for a suitable machine learning model to fit the data.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 80.04%. Two algorithms (Logistic<br>Regression and Extra Trees) achieved the top accuracy metrics after the first<br>round of modeling. After a series of tuning trials, Logistic Regression turned<br>in the top overall result and achieved an accuracy metric of 83.90%. By using<br>the optimized parameters, the Logistic Regression algorithm processed the<br>testing dataset with an accuracy of 80.68%, which was slightly below the<br>prediction accuracy gained from the training data.<br><br><br><br>CONCLUSION: For the Hungarian dataset, the Logistic<br>Regression algorithm achieved the best overall results using the training and<br>testing datasets. For this dataset, Logistic Regression should be considered<br>for further modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 20 May 2019 12:25:23 +0000","blog_url":"https://dainesanalytics.blog/2019/05/20/binary-classification-model-for-heart-disease-study-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Heart Disease dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The original database contains 76 attributes,<br>but all published experiments refer to using a subset of 14 of them. In<br>particular, the Cleveland database is the only one that has been used by<br>machine learning researchers to this date. The “num” field refers to<br>the presence of heart disease in the patient. It is integer valued from 0 (no<br>presence) to 4. Experiments with the Cleveland database have concentrated on<br>simply attempting to distinguish presence (values 1,2,3,4) from absence (value<br>0).<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 74.69%. Two algorithms (Logistic<br>Regression and Stochastic Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, Logistic<br>Regression turned in the top overall result and achieved an accuracy metric of<br>82.07%. By using the optimized parameters, the Logistic Regression algorithm<br>processed the testing dataset with an accuracy of 90.10%, which was even better<br>than the prediction accuracy gained from the training data.<br><br><br><br>CONCLUSION: For this iteration, the Logistic Regression<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Logistic Regression should be considered for<br>further modeling or production use.<br><br><br><br>Dataset Used: Heart Disease Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and<br>categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Heart+Disease<br><br><br><br>One potential source of performance benchmark:<br>https://www.kaggle.com/ronitf/heart-disease-uci<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping Templates for Python with BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 19 May 2019 12:56:16 +0000","blog_url":"https://dainesanalytics.blog/2019/05/19/web-scraping-templates-for-python-with-beautifulsoup/","blog_text":"As I work on practicing and solving web scraping problems, I<br>find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support web scraping tasks using Python.<br><br><br><br>The Python scripts leverage the BeautifulSoup module. You can find the web scraping templates from the Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Diabetes Readmission Prediction Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 17 May 2019 12:40:15 +0000","blog_url":"https://dainesanalytics.blog/2019/05/17/binary-classification-model-for-diabetes-readmission-prediction-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes reflected<br>in HbA1c determination may improve patient outcomes and lower cost of inpatient<br>care.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling. To limit the processing time and memory<br>requirements, we also limited the attributes used for this project by not<br>including those attributes that do not appear on the final model of the<br>research paper.<br><br><br><br>In iteration Take2, we further tested the machine learning<br>models by rearranging some of the features to be more consistent with the<br>research paper (Table 4). We had hoped to improve the overall accuracy and<br>applicability of the model by having features with a fewer number of<br>categories.<br><br><br><br>In this iteration, we will test the machine learning models<br>by reconfiguring the target variable to have only two categories, thus making<br>this a binary classification exercise. We hope to improve the overall accuracy<br>applicability of the model by predicting with just the “yes” and “no” outcomes.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 61.82%. Two algorithms (Logistic<br>Regression and Gradient Boosting) achieved the top accuracy metrics after the<br>first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 63.05%. By<br>using the optimized parameters, the Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 62.96%, which was slightly below the<br>prediction accuracy using the training data.<br><br><br><br>CONCLUSION: Restructuring the target variable to binary<br>outcomes yielded accuracy improvement and a reduction in the processing time.<br>For this iteration, the Gradient Boosting algorithm achieved the overall<br>results using the training and testing datasets. For this dataset, Gradient<br>Boosting should be considered for further modeling or production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks:<br>http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 15 May 2019 12:26:38 +0000","blog_url":"https://dainesanalytics.blog/2019/05/15/multi-class-classification-model-for-diabetes-readmission-prediction-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes reflected<br>in HbA1c determination may improve patient outcomes and lower cost of inpatient<br>care.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling. To limit the processing time and memory<br>requirements, we also limited the attributes used for this project by not<br>including those attributes that do not appear on the final model of the<br>research paper.<br><br><br><br>In this iteration, we further test the machine learning<br>models by rearranging some of the features to be more consistent with the<br>research papers (Table 4). We hope to improve the overall accuracy and<br>applicability of the model by having features with a fewer number of<br>categories.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 56.32%. Two algorithms (Linear<br>Discriminant Analysis and Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, Gradient<br>Boosting turned in the top overall result and achieved an accuracy metric of<br>58.07%. By using the optimized parameters, the Gradient Boosting algorithm<br>processed the testing dataset with an accuracy of 57.04%, which was slightly<br>below the prediction accuracy from the training data.<br><br><br><br>CONCLUSION: Restructuring the categorical variables did not<br>yield accuracy or processing time improvement. For this iteration, the Gradient<br>Boosting algorithm achieved the top-tier training and validation results. For<br>this dataset, Gradient Boosting should be considered for further modeling or<br>production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks:<br>http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 13 May 2019 12:46:04 +0000","blog_url":"https://dainesanalytics.blog/2019/05/13/multi-class-classification-model-for-diabetes-readmission-prediction-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes reflected<br>in HbA1c determination may improve patient outcomes and lower cost of inpatient<br>care.<br><br><br><br>In this iteration, we will establish the baseline prediction<br>accuracy for further takes of modeling. To limit the processing time and memory<br>requirements, we also will limit the attributes used for this project by not<br>including those attributes that do not appear on the final model of the<br>research paper.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 57.15%. Two algorithms (Linear<br>Discriminant Analysis and eXtreme Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>eXtreme Gradient Boosting turned in the top overall result and achieved an<br>accuracy metric of 58.74%. By using the optimized parameters, the eXtreme<br>Gradient Boosting algorithm processed the testing dataset with an accuracy of<br>58.85%, which was even better than the prediction accuracy from the training<br>data.<br><br><br><br>CONCLUSION: For this iteration, the eXtreme Gradient<br>Boosting algorithm achieved the best overall results. For this dataset, eXtreme<br>Gradient Boosting should be considered for further modeling or production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks: http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 4","author_name":"David Lowe","blog_date_text":"Sun, 12 May 2019 12:32:42 +0000","blog_url":"https://dainesanalytics.blog/2019/05/12/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-4/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by gathering specific pieces of information from a website. The web<br>scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: A demo website, created by Scrapinghub, lists<br>quotes from famous people. It has many endpoints showing the quotes in<br>different ways, and each endpoint presents a different scraping challenge for<br>practicing web scraping. For this Take4 iteration, the Python script attempts<br>to execute the login form and scrape the Goodreads links off each quote. The<br>Goodreads links appear only after a successful authentication.<br><br><br><br>Starting URLs: http://quotes.toscrape.com/login<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Diabetes Readmission Prediction Using Python Take 3","author_name":"David Lowe","blog_date_text":"Fri, 10 May 2019 12:29:10 +0000","blog_url":"https://dainesanalytics.blog/2019/05/10/binary-classification-model-for-diabetes-readmission-prediction-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes reflected<br>in HbA1c determination may improve patient outcomes and lower cost of inpatient<br>care.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling. To limit the processing time and memory<br>requirements, we also limited the attributes used for this project by not<br>including those attributes that do not appear on the final model of the<br>research paper.<br><br><br><br>In iteration Take2, we further tested the machine learning<br>models by rearranging some of the features to be more consistent with the<br>research papers (Table 4). We had hoped to improve the overall accuracy and<br>applicability of the model by having features with a fewer number of<br>categories.<br><br><br><br>In this iteration, we will test the machine learning models<br>by reconfiguring the target variable to have only two categories, thus making<br>this a binary classification exercise. We hope to improve the overall accuracy<br>and applicability of the model by predicting with just the “yes” and<br>“no” outcomes.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 58.93%. Two algorithms (Logistic<br>Regression and Gradient Boosting) achieved the top accuracy metrics after the<br>first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 63.14%. By<br>using the optimized parameters, the Bagged Decision Trees algorithm processed<br>the testing dataset with an accuracy of 62.85%, which was just slightly below<br>the prediction accuracy from the training data.<br><br><br><br>CONCLUSION: Restructuring the target variable to binary<br>outcomes yielded accuracy improvement and a reduction in the processing time.<br>For this iteration, the Gradient Boosting algorithm achieved the best overall<br>results using the training and testing datasets. For this dataset, Gradient<br>Boosting should be considered for further modeling or production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks:<br>http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using Python Take 2","author_name":"David Lowe","blog_date_text":"Wed, 08 May 2019 12:24:25 +0000","blog_url":"https://dainesanalytics.blog/2019/05/08/multi-class-classification-model-for-diabetes-readmission-prediction-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes reflected<br>in HbA1c determination may improve patient outcomes and lower cost of inpatient<br>care.<br><br><br><br>In iteration Take1, we established the baseline prediction<br>accuracy for further takes of modeling. To limit the processing time and memory<br>requirements, we also limited the attributes used for this project by not<br>including those attributes that do not appear on the final model of the<br>research paper.<br><br><br><br>In this iteration, we further test the machine learning<br>models by rearranging some of the features to be more consistent with the<br>research papers (Table 4). We hope to improve the overall accuracy and<br>applicability of the model by having features with a fewer number of<br>categories.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 53.04%. Two algorithms (Linear<br>Discriminant Analysis and Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, Gradient<br>Boosting turned in the top overall result and achieved an accuracy metric of<br>58.11%. By using the optimized parameters, the Bagged Decision Trees algorithm<br>processed the testing dataset with an accuracy of 57.73%, which was just<br>slightly below the prediction accuracy from the training data.<br><br><br><br>CONCLUSION: Restructuring the categorical variables did not<br>yield accuracy improvement but certainly had a positive impact on the<br>processing time. For this iteration, the Gradient Boosting algorithm achieved<br>the best overall results using the training and testing datasets. For this<br>dataset, Gradient Boosting should be considered for further modeling or<br>production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks:<br>http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 06 May 2019 12:52:52 +0000","blog_url":"https://dainesanalytics.blog/2019/05/06/multi-class-classification-model-for-diabetes-readmission-prediction-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Diabetes Readmission Prediction is a<br>multi-class classification situation where we are trying to predict one of the<br>several possible outcomes.<br><br><br><br>INTRODUCTION: Management of hyperglycemia in hospitalized<br>patients has a significant bearing on the outcome, in terms of both morbidity<br>and mortality. However, there are few national assessments of diabetes care<br>during hospitalization which could serve as a baseline for change. This<br>analysis of a large clinical database was undertaken to provide such an<br>assessment and to find future directions which might lead to improvements in<br>patient safety. The statistical model suggests that the relationship between<br>the probability of readmission and the HbA1c measurement depends on the primary<br>diagnosis. The data suggest further that the greater attention to diabetes<br>reflected in HbA1c determination may improve patient outcomes and lower cost of<br>inpatient care.<br><br><br><br>In this iteration, we plan to establish the baseline prediction accuracy for further takes of modeling. To limit the processing time and memory requirements, we also will limit the attributes used for this project by not including those attributes that do not appear on the final model of the research paper.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning algorithms achieved an average accuracy of 53.71%. Two algorithms (Linear Discriminant Analysis and Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Gradient Boosting turned in the top overall result and achieved an accuracy metric of 58.78%. By using the optimized parameters, the Bagged Decision Trees algorithm processed the testing dataset with an accuracy of 58.41%, which was just slightly below the prediction accuracy from the training data.<br><br><br><br>CONCLUSION: For this iteration, the Gradient Boosting<br>algorithm achieved the best overall results using the training and testing<br>datasets. For this dataset, Gradient Boosting should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Diabetes 130-US hospitals for years 1999-2008<br>Data Set<br><br><br><br>Dataset ML Model: Multi-Class classification with numerical<br>and categorical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008<br><br><br><br>One source of potential performance benchmarks:<br>http://www.hindawi.com/journals/bmri/2014/781670/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 3","author_name":"David Lowe","blog_date_text":"Sun, 05 May 2019 12:53:47 +0000","blog_url":"https://dainesanalytics.blog/2019/05/05/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-3/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by gathering specific pieces of information from a website. The web<br>scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: A demo website, created by Scrapinghub, lists<br>quotes from famous people. It has many endpoints showing the quotes in<br>different ways, and each endpoint presents a different scraping challenge for<br>practicing web scraping. For this Take3 iteration, the Python script attempts<br>to scrape the displayed quote information via an infinite scrolling page.<br><br><br><br>Note: For this iteration, the website returns the data in<br>JSON format when using the API URL format. As a result, the BeautifulSoup<br>module is not necessary for parsing the web pages for this iteration.<br><br><br><br>Starting URLs: http://quotes.toscrape.com/<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 03 May 2019 12:44:54 +0000","blog_url":"https://dainesanalytics.blog/2019/05/03/binary-classification-model-for-parkinsons-disease-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction<br>model using various machine learning algorithms and to document the end-to-end<br>steps using a template. Parkinson’s Disease dataset is a binary classification<br>situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.<br>The control group consists of 64 healthy individuals (23 men and 41 women) with<br>ages varying between 41 and 82. During the data collection process, the<br>microphone is set to 44.1 KHz and following the physician’s examination, the<br>sustained phonation of the vowel /a/ was collected from each subject with three<br>repetitions.<br><br><br><br>In the first iteration, the script focused on evaluating<br>various machine learning algorithms and identifying the model that produces the<br>best overall metrics. The first iteration established the performance baseline<br>for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of attribute importance ranking by using the Gradient Boosting<br>algorithm. By selecting only the most important attributes, we decreased the<br>processing time and maintained a similar level of prediction accuracy compared<br>to the first iteration.<br><br><br><br>In iteration Take3, we will examine the feature selection<br>technique of Recursive Feature Elimination (RFE) by using the Random Forest<br>algorithm. By selecting only the most relevant attributes, we hoped to decrease<br>the processing time and maintain a similar level of prediction accuracy<br>compared to the first iteration.<br><br><br><br>ANALYSIS: In the first iteration, the baseline performance<br>of the machine learning algorithms achieved an average accuracy of 77.84%. Two<br>algorithms (Random Forest and Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>Random Forest turned in the top overall result and achieved an accuracy metric<br>of 88.24%. By using the optimized parameters, the Random Forest algorithm<br>processed the testing dataset with an accuracy of 83.63%, which was just<br>slightly below the prediction accuracy using the training data.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 82.14%. Two algorithms<br>(Random Forest and Gradient Boosting) achieved the top accuracy metrics after<br>the first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 88.92%. By<br>using the optimized parameters, the Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 88.05%, which was just slightly below the<br>prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 541, from 753 down to 212. The processing time went<br>from 2 hours 16 minutes in the first iteration down to 27 minutes in Take2,<br>which was a decrease of 80.1%.<br><br><br><br>In iteration Take3, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 81.69%. Two algorithms<br>(Random Forest and Gradient Boosting) achieved the top accuracy metrics after<br>the first round of modeling. After a series of tuning trials, Random Forest turned<br>in the top overall result and achieved an accuracy metric of 89.11%. By using<br>the optimized parameters, the Random Forest algorithm processed the testing<br>dataset with an accuracy of 84.96%, which was just slightly below the<br>prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 571, from 753 down to 182. The processing time went<br>from 2 hours 16 minutes in the first iteration down to 1 hour 43 minutes in<br>Take3, which was a decrease of 24.2%.<br><br><br><br>CONCLUSION: For this iteration, using the RFE technique and<br>the Random Forest algorithm achieved the best overall modeling results. Using a<br>feature selection technique further reduced the processing time while achieving<br>an even better prediction accuracy overall. For this dataset, either Gradient<br>Boosting or Random Forest, combined with a feature selection technique, should<br>be considered for further modeling or production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinson’s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 01 May 2019 12:44:56 +0000","blog_url":"https://dainesanalytics.blog/2019/05/01/binary-classification-model-for-parkinsons-disease-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction<br>model using various machine learning algorithms and to document the end-to-end<br>steps using a template. Parkinson’s Disease dataset is a binary classification<br>situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.<br>The control group consists of 64 healthy individuals (23 men and 41 women) with<br>ages varying between 41 and 82. During the data collection process, the<br>microphone is set to 44.1 KHz and following the physician’s examination, the<br>sustained phonation of the vowel /a/ was collected from each subject with three<br>repetitions.<br><br><br><br>In the first iteration, the script focused on evaluating<br>various machine learning algorithms and identifying the model that produces the<br>best overall metrics. The first iteration established the performance baseline<br>for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of attribute importance ranking by using the Gradient Boosting<br>algorithm. By selecting only the most important attributes, we decreased the<br>processing time and maintained a similar level of prediction accuracy compared<br>to the first iteration.<br><br><br><br>In iteration Take3, we will examine the feature selection<br>technique of Recursive Feature Elimination (RFE) by using the Random Forest<br>algorithm. By selecting only the most relevant attributes, we hoped to decrease<br>the processing time and maintain a similar level of prediction accuracy<br>compared to the first iteration.<br><br><br><br>ANALYSIS: In the first iteration, the baseline performance<br>of the machine learning algorithms achieved an average accuracy of 81.58%. Two<br>algorithms (Extra Trees and Stochastic Gradient Boosting) achieved the top<br>accuracy metrics after the first round of modeling. After a series of tuning<br>trials, Extra Trees turned in the top overall result and achieved an accuracy<br>metric of 88.09%. By using the optimized parameters, the Extra Trees algorithm<br>processed the testing dataset with an accuracy of 87.22%, which was just<br>slightly below the prediction accuracy using the training data.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 82.97%. Two algorithms<br>(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>Extra Trees turned in the top overall result and achieved an accuracy metric of<br>90.17%. By using the optimized parameters, the Extra Trees algorithm processed<br>the testing dataset with an accuracy of 89.42%, which was just slightly below<br>the prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 585, from 753 down to 168. The processing time went<br>from 18 minutes 16 seconds in the first iteration down to 11 minutes 33 seconds<br>in Take2, which was a decrease of 36.7%.<br><br><br><br>In iteration Take3, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 80.96%. Two algorithms<br>(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>Extra Trees turned in the top overall result and achieved an accuracy metric of<br>88.27%. By using the optimized parameters, the Extra Trees algorithm processed<br>the testing dataset with an accuracy of 88.10%, which was just slightly below<br>the prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 503, from 753 down to 250. The processing time went<br>from 18 minutes 16 seconds in the first iteration down to 14 minutes 37 seconds<br>in Take3, which was a decrease of 19.9%.<br><br><br><br>CONCLUSION: For this iteration, using the RFE technique and<br>the Extra Trees algorithm achieved the best overall modeling results. Using<br>feature selection technique further reduced the processing time while achieving<br>an even better prediction accuracy overall. For this dataset, Extra Trees,<br>combined with a feature selection technique, should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinson’s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using R Take 2","author_name":"David Lowe","blog_date_text":"Mon, 29 Apr 2019 12:40:13 +0000","blog_url":"https://dainesanalytics.blog/2019/04/29/binary-classification-model-for-parkinsons-disease-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction<br>model using various machine learning algorithms and to document the end-to-end<br>steps using a template. Parkinson’s Disease dataset is a binary classification<br>situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.<br>The control group consists of 64 healthy individuals (23 men and 41 women) with<br>ages varying between 41 and 82. During the data collection process, the<br>microphone is set to 44.1 KHz and following the physician’s examination, the<br>sustained phonation of the vowel /a/ was collected from each subject with three<br>repetitions.<br><br><br><br>In the first iteration, the script focused on evaluating<br>various machine learning algorithms and identifying the model that produces the<br>best overall metrics. The first iteration established the performance baseline<br>for accuracy and processing time.<br><br><br><br>In iteration Take2, we will examine the feature selection<br>technique of attribute importance ranking by using the Gradient Boosting<br>algorithm. By selecting only the most important attributes, we hoped to<br>decrease the processing time and maintain a similar level of prediction<br>accuracy compared to the first iteration.<br><br><br><br>ANALYSIS: In the first iteration, the baseline performance<br>of the machine learning algorithms achieved an average accuracy of 77.84%. Two<br>algorithms (Random Forest and Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>Random Forest turned in the top overall result and achieved an accuracy metric<br>of 88.24%. By using the optimized parameters, the Random Forest algorithm<br>processed the testing dataset with an accuracy of 83.63%, which was just<br>slightly below the prediction accuracy using the training data.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 82.14%. Two algorithms<br>(Random Forest and Gradient Boosting) achieved the top accuracy metrics after<br>the first round of modeling. After a series of tuning trials, Gradient Boosting<br>turned in the top overall result and achieved an accuracy metric of 88.92%. By<br>using the optimized parameters, the Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 88.05%, which was just slightly below the<br>prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 541, from 753 down to 212. The processing time went<br>from 2 hours 16 minutes in the first iteration down to 27 minutes in Take2,<br>which was a decrease of 80.1%.<br><br><br><br>CONCLUSION: For this iteration, using the Attribute<br>Importance Ranking technique and the Gradient Boosting algorithm achieved the<br>best overall modeling results. Using feature selection technique further<br>reduced the processing time while achieving an even better prediction accuracy<br>overall. For this dataset, Gradient Boosting should be considered for further<br>modeling or production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinson’s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of AWS Documentation using BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 28 Apr 2019 12:58:40 +0000","blog_url":"https://dainesanalytics.blog/2019/04/28/web-scraping-of-aws-documentation-using-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by extracting specific information from a website. Using the extracted<br>information, the script further completes other tasks (downloading files in<br>this case). The web scraping python code leverages the BeautifulSoup module.<br><br><br><br>INTRODUCTION: On occasions, there is a need to download a<br>batch of documents off web pages without clicking on the download links one at<br>a time. This web scraping script will automatically traverse through the<br>necessary web pages and collect all links with the PDF document format. The<br>script will also download the PDF documents as part of the scraping process.<br><br><br><br>For this script to work, it requires the use of Selenium<br>browser automation software and one of its WebDrivers (Firefox in this case).<br><br><br><br>Starting URLs: https://docs.aws.amazon.com/<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using Python","author_name":"David Lowe","blog_date_text":"Fri, 26 Apr 2019 12:45:29 +0000","blog_url":"https://dainesanalytics.blog/2019/04/26/binary-classification-model-for-parkinsons-disease-using-python-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction<br>model using various machine learning algorithms and to document the end-to-end<br>steps using a template. Parkinson’s Disease dataset is a binary classification<br>situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.<br>The control group consists of 64 healthy individuals (23 men and 41 women) with<br>ages varying between 41 and 82. During the data collection process, the<br>microphone is set to 44.1 KHz and following the physician’s examination, the<br>sustained phonation of the vowel /a/ was collected from each subject with three<br>repetitions.<br><br><br><br>In the first iteration, the script focused on evaluating<br>various machine learning algorithms and identifying the model that produces the<br>best overall metrics. The first iteration established the performance baseline<br>for accuracy and processing time.<br><br><br><br>In iteration Take2, we will examine the feature selection<br>technique of attribute importance ranking by using the Gradient Boosting<br>algorithm. By selecting only the most important attributes, we hoped to<br>decrease the processing time and maintain a similar level of prediction<br>accuracy compared to the first iteration.<br><br><br><br>ANALYSIS: In the first iteration, the baseline performance<br>of the machine learning algorithms achieved an average accuracy of 81.58%. Two<br>algorithms (Extra Trees and Stochastic Gradient Boosting) achieved the top<br>accuracy metrics after the first round of modeling. After a series of tuning<br>trials, Extra Trees turned in the top overall result and achieved an accuracy<br>metric of 88.09%. By using the optimized parameters, the Extra Trees algorithm<br>processed the testing dataset with an accuracy of 87.22%, which was just<br>slightly below the prediction accuracy using the training data.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 82.97%. Two algorithms<br>(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>Extra Trees turned in the top overall result and achieved an accuracy metric of<br>90.17%. By using the optimized parameters, the Extra Trees algorithm processed<br>the testing dataset with an accuracy of 89.42%, which was just slightly below<br>the prediction accuracy using the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 585, from 753 down to 168. The processing time went<br>from 18 minutes 16 seconds in the first iteration down to 11 minutes 33 seconds<br>in Take2, which was a decrease of 36.7%.<br><br><br><br>CONCLUSION: For this iteration, using the Attribute<br>Importance Ranking technique and the Extra Trees algorithm achieved the best<br>overall modeling results. Using feature selection technique further reduced the<br>processing time while achieving an even better prediction accuracy overall. For<br>this dataset, Extra Trees should be considered for further modeling or<br>production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinson’s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using R","author_name":"David Lowe","blog_date_text":"Wed, 24 Apr 2019 12:23:40 +0000","blog_url":"https://dainesanalytics.blog/2019/04/24/binary-classification-model-for-parkinsons-disease-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction<br>model using various machine learning algorithms and to document the end-to-end<br>steps using a template. Parkinson’s Disease dataset is a binary classification<br>situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul<br>University. The control group consists of 64 healthy individuals (23 men and 41<br>women) with ages varying between 41 and 82. During the data collection process,<br>the microphone is set to 44.1 KHz and following the physician’s examination,<br>the sustained phonation of the vowel was collected from each subject with three<br>repetitions.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 77.84%. Two algorithms (Random<br>Forest and Stochastic Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, Random<br>Forest turned in the top overall result and achieved an accuracy metric of<br>88.24%. By using the optimized parameters, the Random Forest algorithm<br>processed the testing dataset with an accuracy of 83.63%, which was just<br>slightly below the prediction accuracy using the training data.<br><br><br><br>CONCLUSION: For this iteration, the Random Forest algorithm<br>achieved the best overall results using the training and testing datasets. For<br>this dataset, Random Forest should be considered for further modeling or<br>production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinson’s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Parkinson’s Disease Using Python","author_name":"David Lowe","blog_date_text":"Tue, 23 Apr 2019 12:16:47 +0000","blog_url":"https://dainesanalytics.blog/2019/04/23/binary-classification-model-for-parkinsons-disease-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Parkinson’s Disease dataset is a binary<br>classification situation where we are trying to predict one of the two possible<br>outcomes.<br><br><br><br>INTRODUCTION: The data used in this study were gathered from<br>188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at<br>the Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul<br>University. The control group consists of 64 healthy individuals (23 men and 41<br>women) with ages varying between 41 and 82. During the data collection process,<br>the microphone is set to 44.1 KHz and following the physician’s examination,<br>the sustained phonation of the vowel /a/ was collected from each subject with<br>three repetitions.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 81.58%. Two algorithms (Extra Trees<br>and Stochastic Gradient Boosting) achieved the top accuracy metrics after the<br>first round of modeling. After a series of tuning trials, Extra Trees turned in<br>the top overall result and achieved an accuracy metric of 88.09%. By using the<br>optimized parameters, the Extra Trees algorithm processed the testing dataset<br>with an accuracy of 87.22%, which was just slightly below the prediction<br>accuracy using the training data.<br><br><br><br>CONCLUSION: For this iteration, the Extra Trees algorithm<br>achieved the best overall results using the training and testing datasets. For<br>this dataset, Extra Trees should be considered for further modeling or<br>production use.<br><br><br><br>Dataset Used: Parkinson’s Disease Classification Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification<br><br><br><br>Sakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,<br>Sakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A<br>comparative analysis of speech signal processing algorithms for Parkinsonâ€™s<br>disease classification and the use of the tunable Q-factor wavelet transform.<br>Applied Soft Computing, DOI: https://doi.org/10.1016/j.asoc.2018.10.022<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Song Year Prediction Using Python","author_name":"David Lowe","blog_date_text":"Mon, 22 Apr 2019 12:17:37 +0000","blog_url":"https://dainesanalytics.blog/2019/04/22/multi-class-classification-model-for-song-year-prediction-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Song Year Prediction dataset is a multi-class<br>classification situation where we are trying to predict one of the ten possible<br>outcomes.<br><br><br><br>INTRODUCTION: This data is a subset of the Million Song<br>Dataset, http://labrosa.ee.columbia.edu/millionsong/, a collaboration between<br>LabROSA (Columbia University) and The Echo Nest. The purpose of this exercise<br>is to predict the release year of a song from audio features. Songs are mostly<br>western, commercial tracks ranging from 1922 to 2011, with a peak in the year<br>2000s. The data preparer recommended the train/test split of first 463,715<br>examples for training and the last 51,630 examples for testing. This approach<br>avoids the ‘producer effect’ by making sure no song from a given artist ends up<br>in both the train and test set.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning<br>algorithms achieved an average accuracy of 57.24%. Two algorithms (Stochastic<br>Gradient Boosting and eXtreme Gradient Boosting) achieved the top accuracy<br>metrics after the first round of modeling. After a series of tuning trials,<br>eXtreme Gradient Boosting turned in the top overall result and achieved an<br>accuracy metric of 63.85%. By using the optimized parameters, the eXtreme<br>Gradient Boosting algorithm processed the testing dataset with an accuracy of<br>63.65%, which was just slightly below the training data.<br><br><br><br>CONCLUSION: For this iteration, the eXtreme Gradient<br>Boosting algorithm achieved the best overall results using the training and<br>testing datasets. For this dataset, eXtreme Gradient Boosting should be<br>considered for further modeling or production use.<br><br><br><br>Dataset Used: YearPredictionMSD Data Set<br><br><br><br>Dataset ML Model: Classification with numerical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD<br><br><br><br>Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,<br>and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th<br>International Society for Music Information Retrieval Conference (ISMIR 2011),<br>2011.<br><br><br><br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/msd-audio-features/home<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates v9 for R","author_name":"David Lowe","blog_date_text":"Fri, 19 Apr 2019 12:40:06 +0000","blog_url":"https://dainesanalytics.blog/2019/04/19/updated-machine-learning-templates-v9-for-r/","blog_text":"As I work on practicing and solving machine learning (ML)<br>problems, I find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br><br><br><br>Version 9 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added or updated the sample code to support:<br><br><br><br>More streamlined email function using the operating system’s environment variablesThe addition of eXtreme Gradient Boosting (XGBOOST) algorithmYou will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Machine Learning Templates for Google Colaboratory","author_name":"David Lowe","blog_date_text":"Wed, 17 Apr 2019 12:19:37 +0000","blog_url":"https://dainesanalytics.blog/2019/04/17/machine-learning-templates-for-google-colaboratory/","blog_text":"Google’s Colaboratory (Colab) is a Jupyter-based research tool for machine learning education and research. The Colab Jupyter notebook environment is a standard virtual machine that requires no setup to provision and to use.<br><br><br><br>The Colab environment’s virtual machine has access to 12 GB<br>of RAM, which is sufficient to practice many machine learning problems. The intended<br>use for the virtual machine is interactive modeling and learning. The virtual<br>machine can terminate after some time, and we will need to reconnect to another<br>Colab virtual machine. Another word, the Colab environment is not suitable for long-running<br>machine learning jobs, but the Colab virtual machine does include access to a GPU.<br><br><br><br>As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly. Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression and classification ML problems using Python in Colab.<br><br><br><br>You will find the Python templates (including the ones for Colab) from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Song Year Prediction Using Python","author_name":"David Lowe","blog_date_text":"Mon, 15 Apr 2019 12:41:08 +0000","blog_url":"https://dainesanalytics.blog/2019/04/15/regression-model-for-song-year-prediction-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Song Year Prediction dataset is a<br>classic regression situation where we are trying to predict the value of a<br>continuous variable.<br><br><br><br>INTRODUCTION: This data is a subset of the Million Song<br>Dataset, http://labrosa.ee.columbia.edu/millionsong/, a collaboration between<br>LabROSA (Columbia University) and The Echo Nest. The purpose of this exercise<br>is to predict the release year of a song from audio features. Songs are mostly<br>western, commercial tracks ranging from 1922 to 2011, with a peak in the year<br>2000s. The data preparer recommended the train/test split of first 463,715<br>examples for training and the last 51,630 examples for testing. This approach<br>avoids the ‘producer effect’ by making sure no song from a given artist ends up<br>in both the train and test set.<br><br><br><br>ANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 10.16. Two algorithms (Stochastic Gradient Boosting and eXtreme Gradient Boosting) achieved the top RMSE metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an RMSE metric of 9.04. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with a RMSE of 9.06, which was just slightly above the training data.<br><br><br><br>CONCLUSION: For this iteration, the eXtreme Gradient<br>Boosting algorithm achieved the best overall results using the training and<br>testing datasets. For this dataset, eXtreme Gradient Boosting should be<br>considered for further modeling or production use.<br><br><br><br>Dataset Used: YearPredictionMSD Data Set<br><br><br><br>Dataset ML Model: Regression with numerical attributes<br><br><br><br>Dataset Reference:<br>https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD<br><br><br><br>Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,<br>and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th<br>International Society for Music Information Retrieval Conference (ISMIR 2011),<br>2011.<br><br><br><br>One potential source of performance benchmarks:<br>https://www.kaggle.com/uciml/msd-audio-features/home<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 2","author_name":"David Lowe","blog_date_text":"Sun, 14 Apr 2019 12:53:07 +0000","blog_url":"https://dainesanalytics.blog/2019/04/14/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by gathering specific pieces of information from a website. The web<br>scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: A demo website, created by Scrapinghub, lists<br>quotes from famous people. It has many endpoints showing the quotes in<br>different ways, and each endpoint presents a different scraping challenge for<br>practicing web scraping. For this Take2 iteration, the Python script attempts<br>to follow the links to the author page and scrape the author information.<br><br><br><br>Starting URLs: http://quotes.toscrape.com/<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of File Download Using Python and BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 14 Apr 2019 12:47:13 +0000","blog_url":"https://dainesanalytics.blog/2019/04/14/web-scraping-of-file-download-using-python-and-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by extracting specific pieces of information from a website. The web<br>scraping python code leverages the BeautifulSoup module.<br><br><br><br>INTRODUCTION: On occasions we need to download a batch of documents from a single web page without clicking on the download link one at a time. This web scraping script will automatically traverse through the entire web page and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.<br><br><br><br>Starting URL: https://www.knime.com/about/events/knime-spring-summit-2019-berlin<br><br><br><br>The source code and JSON output can be found here for Python and here for R on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates v9 for Python","author_name":"David Lowe","blog_date_text":"Fri, 12 Apr 2019 12:33:25 +0000","blog_url":"https://dainesanalytics.blog/2019/04/12/updated-machine-learning-templates-v9-for-python/","blog_text":"As I work on practicing and solving machine learning (ML)<br>problems, I find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br><br><br><br>Version 9 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added or updated the sample code to support:<br><br><br><br>More streamlined email function using the<br>operating system’s environment variablesUpdated code for the Synthetic Minority<br>Over-sampling Technique (SMOTE) transformationThe addition of eXtreme Gradient Boosting<br>(XGBOOST) algorithmYou will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (XGBoost Tuning Batch #2)","author_name":"David Lowe","blog_date_text":"Wed, 10 Apr 2019 12:12:59 +0000","blog_url":"https://dainesanalytics.blog/2019/04/10/binary-classification-model-for-customer-transaction-prediction-using-python-xgboost-tuning-batch-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>eXtreme Gradient Boosting algorithm with the synthetic over-sampling technique<br>(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions<br>are evaluated on the area under the ROC<br>curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: We applied different values for the max_depth, min_child_weight,<br>subsample, and colsample_bytree parameters using fixed n_estimators (1000 or<br>100). The max_depth values vary from 10, 15, 20 to 25. The min_child_weight values<br>vary from 3 to 5 with different learning rates. The subsample and<br>colsample_bytree values vary from 0.6 to 1.0. The following output files are<br>available for comparison.<br><br><br><br>py-classification-santander-kaggle-XGB-take11py-classification-santander-kaggle-XGB-take12py-classification-santander-kaggle-XGB-take13py-classification-santander-kaggle-XGB-take14py-classification-santander-kaggle-XGB-take15py-classification-santander-kaggle-XGB-take16py-classification-santander-kaggle-XGB-take17py-classification-santander-kaggle-XGB-take18py-classification-santander-kaggle-XGB-take19py-classification-santander-kaggle-XGB-take21py-classification-santander-kaggle-XGB-take22py-classification-santander-kaggle-XGB-take23py-classification-santander-kaggle-XGB-take24py-classification-santander-kaggle-XGB-take25py-classification-santander-kaggle-XGB-take26py-classification-santander-kaggle-XGB-take27py-classification-santander-kaggle-XGB-take28py-classification-santander-kaggle-XGB-take29CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (XGBoost Tuning Batch #1)","author_name":"David Lowe","blog_date_text":"Mon, 08 Apr 2019 12:21:08 +0000","blog_url":"https://dainesanalytics.blog/2019/04/08/binary-classification-model-for-customer-transaction-prediction-using-python-xgboost-tuning-batch-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>eXtreme Gradient Boosting algorithm with the synthetic over-sampling technique<br>(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions<br>are evaluated on the area under the ROC<br>curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: We applied different values for the max_depth, learning_rate,<br>and n_estimators parameters. The max_depth values vary from 3 to 6. The learning_rate<br>values vary from 0.1 to 0.5. The n_estimators values vary from 1000 to 4000. The<br>following output files are available for comparison.<br><br><br><br>py-classification-santander-kaggle-XGB-take1py-classification-santander-kaggle-XGB-take2py-classification-santander-kaggle-XGB-take3py-classification-santander-kaggle-XGB-take4py-classification-santander-kaggle-XGB-take5py-classification-santander-kaggle-XGB-take6py-classification-santander-kaggle-XGB-take7-part1py-classification-santander-kaggle-XGB-take7-part2py-classification-santander-kaggle-XGB-take7-part3py-classification-santander-kaggle-XGB-take7-part4py-classification-santander-kaggle-XGB-take7-part5py-classification-santander-kaggle-XGB-take8-part1py-classification-santander-kaggle-XGB-take8-part2py-classification-santander-kaggle-XGB-take8-part3py-classification-santander-kaggle-XGB-take8-part4py-classification-santander-kaggle-XGB-take8-part5py-classification-santander-kaggle-XGB-take9-part1py-classification-santander-kaggle-XGB-take9-part2py-classification-santander-kaggle-XGB-take9-part3py-classification-santander-kaggle-XGB-take9-part4py-classification-santander-kaggle-XGB-take9-part5py-classification-santander-kaggle-XGB-take10-part1py-classification-santander-kaggle-XGB-take10-part2py-classification-santander-kaggle-XGB-take10-part3py-classification-santander-kaggle-XGB-take10-part4py-classification-santander-kaggle-XGB-take10-part5CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 3","author_name":"David Lowe","blog_date_text":"Sun, 07 Apr 2019 12:30:21 +0000","blog_url":"https://dainesanalytics.blog/2019/04/07/kaggle-competition-banco-santander-customer-transaction-prediction-update-3/","blog_text":"If you are new to Python machine learning like me, you might find the current Kaggle competition “Santander Customer Transaction Prediction” interesting.<br><br><br><br>The competition is essentially a binary classification<br>problem with a decently large dataset (200 attributes and 200,000 rows of<br>training data). I have not participated in Kaggle competition before and will<br>use this one to get some learning under the belt.<br><br><br><br>I had run the training data through a list of machine<br>learning algorithms (see below) and iterate them through three stages. This<br>blog post will serve as the meta post that summarizes the progress.<br><br><br><br>The current plan with the milestones is as follow:<br><br><br><br>Stage 1: Gather the Baseline Performance.<br><br><br><br>LogisticRegression: completed and posted on Monday<br>25 February 2019DecisionTreeClassifier: completed and posted on Wednesday<br>27 February 2019KNeighborsClassifier: completed and posted on Friday<br>1 March 2019BaggingClassifier: completed and posted on Sunday<br>3 March 2019RandomForestClassifier: completed and posted on<br>Monday 4 March 2019ExtraTreesClassifier: completed and posted on<br>Wednesday 6 March 2019GradientBoostingClassifier: completed and posted<br>on Friday 8 March 2019Stage 2: Feature Selection using the Attribute Importance Ranking<br>technique<br><br><br><br>LogisticRegression: completed and posted on Monday 11 March 2019BaggingClassifier: completed and posted on Wednesday 13 March 2019RandomForestClassifier: completed and posted on Friday 15 March 2019ExtraTreesClassifier: completed and posted on Sunday 17 March 2019GradientBoostingClassifier: completed and posted on Monday 18 March 2019Stage 3: Over-Sampling (SMOTE) and Balancing Ensembles techniques<br><br><br><br>LogisticRegression: completed and posted on Wednesday<br>20 March 2019ExtraTreesClassifier: completed and posted on Friday<br>22 March 2019RandomForestClassifier: completed and posted on Monday<br>25 March 2019GradientBoostingClassifier: completed and posted<br>on Wednesday 27 March 2019Balanced Bagging: completed and posted on Friday<br>29 March 2019Balanced Boosting: completed and posted on Sunday<br>31 March 2019Balanced Random Forest: completed and posted on Monday<br>1 April 2019XGBoost with Full Feature: completed and posted<br>on Wednesday 3 April 2019XGBoost with SMOTE: completed and posted on Friday<br>5 April 2019 Stage 4: eXtreme Gradient Boosting Tuning Batches<br><br><br><br>Batch #1: planned for Monday 8 April 2019Batch #2: planned for Monday 10 April 2019I have posted all Python scripts here on GitHub. The final submission deadline is 10 April 2019.<br><br><br><br>Feel free to take a look at the scripts and experiment. Who knows, you<br>might have something you can turn in by the time April comes around. Happy<br>learning and good luck!<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (eXtreme Gradient Boosting with SMOTE)","author_name":"David Lowe","blog_date_text":"Fri, 05 Apr 2019 12:59:00 +0000","blog_url":"https://dainesanalytics.blog/2019/04/05/binary-classification-model-for-customer-transaction-prediction-using-python-extreme-gradient-boosting-with-smote/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>eXtreme Gradient Boosting algorithm with the synthetic over-sampling technique<br>(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions<br>are evaluated on the area under the ROC curve between the predicted probability<br>and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.9129. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.9584. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.6560.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (eXtreme Gradient Boosting with Full Features)","author_name":"David Lowe","blog_date_text":"Wed, 03 Apr 2019 12:58:19 +0000","blog_url":"https://dainesanalytics.blog/2019/04/03/binary-classification-model-for-customer-transaction-prediction-using-python-extreme-gradient-boosting-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>eXtreme Gradient Boosting (XGBoost) algorithm with the full set of features for<br>this problem. Submissions are evaluated<br>on the area under the ROC curve between the predicted probability and the<br>observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8315. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8908. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.6496.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with SMOTE)","author_name":"David Lowe","blog_date_text":"Mon, 01 Apr 2019 12:03:34 +0000","blog_url":"https://dainesanalytics.blog/2019/04/01/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-smote/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Random Forest algorithm with the synthetic over-sampling technique (SMOTE) to<br>mitigate the effect of imbalanced data for this problem. Submissions are<br>evaluated on the area under the ROC curve between the predicted probability and<br>the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.9695. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.9972. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5096.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Gradient Boosting with SMOTE)","author_name":"David Lowe","blog_date_text":"Sun, 31 Mar 2019 12:00:49 +0000","blog_url":"https://dainesanalytics.blog/2019/03/31/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-smote-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Gradient Boosting algorithm with the synthetic over-sampling technique (SMOTE)<br>to mitigate the effect of imbalanced data for this problem. Submissions are<br>evaluated on the area under the ROC curve between the predicted probability and<br>the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.9092. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.9405. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.6289.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Bagging)","author_name":"David Lowe","blog_date_text":"Fri, 29 Mar 2019 12:45:38 +0000","blog_url":"https://dainesanalytics.blog/2019/03/29/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-bagging/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Balanced Bagging classifier (from the imbalanced-learn package) with inner<br>balancing samplers to mitigate the effect of imbalanced data for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.7144. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.7799. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.6659.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Boosting)","author_name":"David Lowe","blog_date_text":"Wed, 27 Mar 2019 12:39:48 +0000","blog_url":"https://dainesanalytics.blog/2019/03/27/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-boosting/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Balanced Boosting classifier (from the imbalanced-learn package) with inner<br>balancing samplers to mitigate the effect of imbalanced data for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average ROC-AUC score of 0.6844. After a series of tuning trials, the top result from the training data was a ROC-AUC score of 0.6860. By using the optimized parameters, the algorithm processed the test dataset with a ROC-AUC score of 0.5681.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Random Forest)","author_name":"David Lowe","blog_date_text":"Mon, 25 Mar 2019 12:28:42 +0000","blog_url":"https://dainesanalytics.blog/2019/03/25/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-random-forest/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Balanced Random Forest classifier (from the imbalanced-learn package) with<br>inner balancing samplers to mitigate the effect of imbalanced data for this<br>problem. Submissions are evaluated on the area under the ROC curve between the<br>predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8224. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8660. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.7761.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 2","author_name":"David Lowe","blog_date_text":"Sun, 24 Mar 2019 12:32:46 +0000","blog_url":"https://dainesanalytics.blog/2019/03/24/kaggle-competition-banco-santander-customer-transaction-prediction-update-2/","blog_text":"If you are new to Python machine learning like me, you might find the current Kaggle competition “Santander Customer Transaction Prediction” interesting.<br><br><br><br>The competition is essentially a binary classification<br>problem with a decently large dataset (200 attributes and 200,000 rows of<br>training data). I have not participated in Kaggle competition before and will<br>use this one to get some learning under the belt.<br><br><br><br>I plan to run the training data through a list of machine<br>learning algorithms (see below) and iterate them through three stages. This<br>blog post will serve as the meta post that summarizes the progress.<br><br><br><br>The current plan with the milestones is as follow:<br><br><br><br>Stage 1: Gather the Baseline Performance.<br><br><br><br>LogisticRegression: completed and posted on Monday<br>25 February 2019DecisionTreeClassifier: completed and posted on Wednesday<br>27 February 2019KNeighborsClassifier: completed and posted on Friday<br>1 March 2019BaggingClassifier: completed and posted on Sunday<br>3 March 2019RandomForestClassifier: completed and posted on<br>Monday 4 March 2019ExtraTreesClassifier: completed and posted on<br>Wednesday 6 March 2019GradientBoostingClassifier: completed and posted<br>on Friday 8 March 2019Stage 2: Feature Selection using the Attribute Importance Ranking<br>technique<br><br><br><br>BaggingClassifier: completed and posted on Wednesday<br>13 March 2019RandomForestClassifier: completed and posted on Friday<br>15 March 2019ExtraTreesClassifier: completed and posted on Sunday<br>17 March 2019GradientBoostingClassifier: completed and posted<br>on Monday 18 March 2019Stage 3: Over-Sampling (SMOTE) and Balancing Ensembles techniques<br><br><br><br>LogisticRegression: completed and posted on Wednesday<br>20 March 2019ExtraTreesClassifier: completed and posted on Friday<br>22 March 2019RandomForestClassifier: planned for Monday 25<br>March 2019GradientBoostingClassifier: planned for Wednesday<br>27 March 2019Balanced Bagging: planned for Friday 29 March<br>2019Balanced Boosting: planned for Sunday 31 March<br>2019Balanced Random Forest: planned for Monday 1 April<br>2019I post all Python scripts here on GitHub. The final submission deadline is 10 April 2019.<br><br><br><br>Feel free to take a look at the scripts and experiment. Who knows, you<br>might have something you can turn in by the time April comes around. Happy<br>learning and good luck!<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with SMOTE)","author_name":"David Lowe","blog_date_text":"Sat, 23 Mar 2019 00:52:12 +0000","blog_url":"https://dainesanalytics.blog/2019/03/22/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-smote/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Extra Trees algorithm with the synthetic over-sampling technique (SMOTE) to<br>mitigate the effect of imbalanced data for this problem. Submissions are<br>evaluated on the area under the ROC curve between the predicted probability and<br>the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.9769. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.9986. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5036.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with SMOTE)","author_name":"David Lowe","blog_date_text":"Wed, 20 Mar 2019 12:57:52 +0000","blog_url":"https://dainesanalytics.blog/2019/03/20/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-smote/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Logistic Regression algorithm with the synthetic over-sampling technique<br>(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions<br>are evaluated on the area under the ROC curve between the predicted probability<br>and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8765. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8788. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.7776.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Gradient Boosting with Attribute Importance Ranking)","author_name":"David Lowe","blog_date_text":"Mon, 18 Mar 2019 12:32:47 +0000","blog_url":"https://dainesanalytics.blog/2019/03/18/binary-classification-model-for-customer-transaction-prediction-using-python-gradient-boosting-with-attribute-importance-ranking/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Gradient Boosting algorithm with a reduced set of features (derived from using<br>the Attribute Importance Ranking technique with the GradientBoostingClassifier<br>algorithm) for this problem. Submissions are evaluated on the area under the<br>ROC curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8322. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8619. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5798.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with Attribute Importance Ranking)","author_name":"David Lowe","blog_date_text":"Sun, 17 Mar 2019 12:30:42 +0000","blog_url":"https://dainesanalytics.blog/2019/03/17/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-attribute-importance-ranking/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Random Forest algorithm with a reduced set of features (derived from using the<br>Attribute Importance Ranking technique with the GradientBoostingClassifier<br>algorithm) for this problem. Submissions are evaluated on the area under the<br>ROC curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.7208. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8330. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5013.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with Attribute Importance Ranking)","author_name":"David Lowe","blog_date_text":"Fri, 15 Mar 2019 12:42:22 +0000","blog_url":"https://dainesanalytics.blog/2019/03/15/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-attribute-importance-ranking/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Extra Trees algorithm with a reduced set of features (derived from using the<br>Attribute Importance Ranking technique with the GradientBoostingClassifier<br>algorithm) for this problem. Submissions are evaluated on the area under the<br>ROC curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.6658. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8441. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5000.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Bagged Decision Trees with Attribute Importance Ranking)","author_name":"David Lowe","blog_date_text":"Wed, 13 Mar 2019 12:37:50 +0000","blog_url":"https://dainesanalytics.blog/2019/03/13/binary-classification-model-for-customer-transaction-prediction-using-python-bagged-decision-trees-with-attribute-importance-ranking/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Bagged Decision Trees algorithm with a reduced set of features (derived from<br>using the Attribute Importance Ranking technique with the<br>GradientBoostingClassifier algorithm) for this problem. Submissions are<br>evaluated on the area under the ROC curve between the predicted probability and<br>the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.7280. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.7823. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5136.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with Attribute Importance Ranking)","author_name":"David Lowe","blog_date_text":"Mon, 11 Mar 2019 12:55:16 +0000","blog_url":"https://dainesanalytics.blog/2019/03/11/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-attribute-importance-ranking/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Logistic Regression algorithm with a reduced set of features (derived from<br>using the Attribute Importance Ranking technique with the<br>GradientBoostingClassifier algorithm) for this problem. Submissions are evaluated on the area under the ROC curve<br>between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8279. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8317. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5912.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 1","author_name":"David Lowe","blog_date_text":"Sun, 10 Mar 2019 12:14:53 +0000","blog_url":"https://dainesanalytics.blog/2019/03/10/kaggle-competition-banco-santander-customer-transaction-prediction-update-1/","blog_text":"If you are new to Python machine learning like me, you might find the current Kaggle competition “Santander Customer Transaction Prediction” interesting.<br><br><br><br>The competition is essentially a binary classification<br>problem with a decently large dataset (200 attributes and 200,000 rows of<br>training data). I have not participated in Kaggle competition before and will<br>use this one to get some learning under the belt.<br><br><br><br>I plan to run the training data through a list of machine<br>learning algorithms (see below) and iterate them through three stages. This<br>blog post will serve as the meta post that summarizes the progress.<br><br><br><br>The current plan with the milestones is as follow:<br><br><br><br>Stage 1: Gather the Baseline Performance.<br><br><br><br>LogisticRegression: completed and posted on Monday<br>25 February 2019DecisionTreeClassifier: completed and posted on Wednesday<br>27 February 2019KNeighborsClassifier: completed and posted on Friday<br>1 March 2019BaggingClassifier: completed and posted on Sunday<br>3 March 2019RandomForestClassifier: completed and posted on Monday<br>4 March 2019ExtraTreesClassifier: completed and posted on Wednesday<br>6 March 2019GradientBoostingClassifier: completed and posted<br>on Friday 8 March 2019Stage 2: Feature Selection using the Attribute Importance Ranking<br>technique<br><br><br><br>LogisticRegression: planned for Monday 11 March 2019BaggingClassifier: planned for Wednesday 13 March 2019RandomForestClassifier: planned for Friday 15 March 2019ExtraTreesClassifier: planned for Sunday 17 March 2019GradientBoostingClassifier: planned for Monday 18 March 2019Stage 3: Over-Sampling and Balancing Ensembles techniques (TBD)<br><br><br><br>I will post all Python script in a folder on GitHub. The final submission deadline is 10 April 2019.<br><br><br><br>Feel free to take a look at the scripts and experiment. Who knows, you<br>might have something you can turn in by the time April comes around. Happy<br>learning and good luck!<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Stochastic Gradient Boosting with Full Features)","author_name":"David Lowe","blog_date_text":"Fri, 08 Mar 2019 13:46:52 +0000","blog_url":"https://dainesanalytics.blog/2019/03/08/binary-classification-model-for-customer-transaction-prediction-using-python-stochastic-gradient-boosting-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Stochastic Gradient Boosting algorithm with the full set of features for this<br>problem. Submissions are evaluated on the area under the ROC curve between the<br>predicted probability and the observed target.<br><br><br><br> ANALYSIS: The baseline performance achieved an average ROC-AUC score of  0.8337. After a series of tuning trials, the top result from the  training data was a ROC-AUC score of 0.8720. By using the optimized  parameters, the algorithm processed the test dataset with a ROC-AUC  score of 0.5684. <br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with Full Features)","author_name":"David Lowe","blog_date_text":"Wed, 06 Mar 2019 13:59:23 +0000","blog_url":"https://dainesanalytics.blog/2019/03/06/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Random Forest algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.6984. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8377. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5000.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with Full Features)","author_name":"David Lowe","blog_date_text":"Mon, 04 Mar 2019 13:19:55 +0000","blog_url":"https://dainesanalytics.blog/2019/03/04/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Extra Trees algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.6298. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8482. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5000.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Bagged Decision Trees with Full Features)","author_name":"David Lowe","blog_date_text":"Sun, 03 Mar 2019 13:45:53 +0000","blog_url":"https://dainesanalytics.blog/2019/03/03/binary-classification-model-for-customer-transaction-prediction-using-python-bagged-decision-trees-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Bagged Decision Trees algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.7112. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.7696. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5054.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Decision Trees with Full Features)","author_name":"David Lowe","blog_date_text":"Fri, 01 Mar 2019 13:39:08 +0000","blog_url":"https://dainesanalytics.blog/2019/03/01/binary-classification-model-for-customer-transaction-prediction-using-python-decision-trees-with-full-features-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>k-Nearest Neighbors algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area under the ROC curve between the predicted<br>probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.5373. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.7238. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5000.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Decision Trees with Full Features)","author_name":"David Lowe","blog_date_text":"Wed, 27 Feb 2019 13:39:58 +0000","blog_url":"https://dainesanalytics.blog/2019/02/27/binary-classification-model-for-customer-transaction-prediction-using-python-decision-trees-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Decision Trees algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area<br>under the ROC curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.5525. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.5646. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.5604.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with Full Features)","author_name":"David Lowe","blog_date_text":"Mon, 25 Feb 2019 13:37:46 +0000","blog_url":"https://dainesanalytics.blog/2019/02/25/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-full-features/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Santander Bank Customer Transaction<br>Prediction competition is a binary classification situation where we are trying<br>to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: Santander Bank’s data science team wants to<br>identify which customers will make a specific transaction in the future,<br>irrespective of the amount of money transacted. The bank is continually<br>challenging its machine learning algorithms to make sure they can more<br>accurately identify new ways to solve its most common challenges such as: Will<br>a customer buy this product? Can a customer pay this loan?<br><br><br><br>For this iteration, we will examine the effectiveness of the<br>Logistic Regression algorithm with the full set of features for this problem.<br>Submissions are evaluated on the area<br>under the ROC curve between the predicted probability and the observed target.<br><br><br><br>ANALYSIS: The baseline performance achieved an average<br>ROC-AUC score of 0.8523. After a series of tuning trials, the top result from<br>the training data was a ROC-AUC score of 0.8593. By using the optimized<br>parameters, the algorithm processed the test dataset with a ROC-AUC score of<br>0.6276.<br><br><br><br>CONCLUSION: To be determined after comparing the results<br>from other machine learning algorithms.<br><br><br><br>Dataset Used: Santander Customer Transaction Prediction<br><br><br><br>Dataset ML Model: Binary classification with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://www.kaggle.com/c/santander-customer-transaction-prediction/data<br><br><br><br>One potential source of performance benchmark: https://www.kaggle.com/c/santander-customer-transaction-prediction/overview<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction","author_name":"David Lowe","blog_date_text":"Sun, 24 Feb 2019 13:24:53 +0000","blog_url":"https://dainesanalytics.blog/2019/02/24/kaggle-competition-banco-santander-customer-transaction-prediction/","blog_text":"If you are new to Python machine learning like me, you might find the current Kaggle competition “Santander Customer Transaction Prediction” interesting.<br><br><br><br>The competition is essentially a binary classification<br>problem with a decently large dataset (200 attributes and 200,000 rows of<br>training data). I have not participated in Kaggle competition before and will<br>use this one to get some learning under the belt.<br><br><br><br>I plan to run the training data through a list of machine<br>learning algorithms (see below) and iterate them through three stages. This blog<br>post will serve as the meta post that summarizes the progress.<br><br><br><br>The current plan with the milestones are as follow:<br><br><br><br>Stage 1: Gather the Baseline Performance.<br><br><br><br>LogisticRegression: targeted Monday 25 February<br>2019DecisionTreeClassifier: targeted Wednesday 27 February<br>2019KNeighborsClassifier: targeted Friday 1 March<br>2019BaggingClassifier: targeted Monday 4 March 2019RandomForestClassifier: targeted Wednesday 6<br>March 2019ExtraTreesClassifier: targeted Friday 8 March<br>2019GradientBoostingClassifier: TBDStage 2: Feature Selection using the Attribute Importance Ranking technique (TBD)<br><br><br><br>Stage 2: Feature Selection using the Recursive Feature Elimination technique (TBD)<br><br><br><br>I will post all Python script in a folder on GitHub. The final submission deadline is 10 April 2019.<br><br><br><br>Feel free to take a look at the scripts and experiment. Who knows, you<br>might have something you can turn in by the time April comes around. Happy<br>learning and good luck!<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates v8 for R","author_name":"David Lowe","blog_date_text":"Fri, 22 Feb 2019 13:03:49 +0000","blog_url":"https://dainesanalytics.blog/2019/02/22/updated-machine-learning-templates-v8-for-r/","blog_text":"As I work on practicing and solving machine learning (ML)<br>problems, I find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br><br><br><br>Version 8 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support:<br><br><br><br>Downloading the data file from an URLUnzipping the data file archive if necessaryYou will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates v8 for Python","author_name":"David Lowe","blog_date_text":"Wed, 20 Feb 2019 13:07:53 +0000","blog_url":"https://dainesanalytics.blog/2019/02/20/updated-machine-learning-templates-v8-for-python/","blog_text":"As I work on practicing and solving machine learning (ML)<br>problems, I find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br><br><br><br>Version 8 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support:<br><br><br><br>Downloading the data file from an URLUnzipping the data file archive if necessaryYou will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 4","author_name":"David Lowe","blog_date_text":"Mon, 18 Feb 2019 13:50:14 +0000","blog_url":"https://dainesanalytics.blog/2019/02/18/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning<br>algorithms and to document the end-to-end steps using a template. The Human<br>Activities and Postural Transitions dataset is a classic multi-class<br>classification situation where we are trying to predict one of the 12 possible<br>outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are<br>stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and<br>lie-to-stand. All the participants were wearing a smartphone on the waist<br>during the experiment execution. The research team also video-recorded the<br>activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and<br>30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of eliminating collinear features. We performed iterative modeling at<br>collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear<br>features, we decreased the processing time and maintained a comparable level of<br>model accuracy comparing to iteration Take1.<br><br><br><br>In iteration Take3, we examined the feature selection<br>technique of attribute importance ranking by using the Random Forest algorithm.<br>By selecting only the most important attributes, we hoped to decrease the<br>processing time and maintain a similar level of accuracy compared to iteration<br>Take1.<br><br><br><br>In the current iteration Take4, we will examine the feature<br>selection technique of Recursive Feature Elimination by using the Random Forest<br>algorithm. By limiting to only the 300 most relevant attributes, we hope to<br>decrease the processing time and maintain a similar level of accuracy compared<br>to iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 89.61%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Stochastic Gradient Boosting turned in the top overall<br>result and achieved an accuracy metric of 97.70%. By using the optimized<br>parameters, the Stochastic Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 92.85%, which was below the training data and<br>possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 89.48%. Two algorithms<br>(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, eXtreme<br>Gradient Boosting turned in the top overall result and achieved an accuracy<br>metric of 98.31%. By using the optimized parameters, the eXtreme Gradient<br>Boosting algorithm processed the testing dataset with an accuracy of 94.97%,<br>which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 278, from 561 down to 283. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in<br>Take2, which was a reduction of 23.9%.<br><br><br><br>In iteration Take3, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 90.04%. Two algorithms<br>(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, eXtreme<br>Gradient Boosting turned in the top overall result and achieved an accuracy<br>metric of 98.35%. By using the optimized parameters, the eXtreme Gradient<br>Boosting algorithm processed the testing dataset with an accuracy of 93.86%,<br>which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 62, from 561 down to 499. The processing time went from<br>10 hours 17 minutes in iteration Take1 up to 16 hours 59 minutes in Take3,<br>which was an increase of 65.1%.<br><br><br><br>In the current iteration Take4, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 89.91%. Two<br>algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top<br>accuracy metrics after the first round of modeling. After a series of tuning<br>trials, eXtreme Gradient Boosting turned in the top overall result and achieved<br>an accuracy metric of 98.40%. By using the optimized parameters, the eXtreme<br>Gradient Boosting algorithm processed the testing dataset with an accuracy of<br>93.77%, which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 311, from 561 down to 250. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 7 hours 56 minutes in<br>Take4, which was a decrease of 22.8%.<br><br><br><br>CONCLUSION: For this iteration, the Recursive Feature<br>Elimination technique and using the eXtreme Gradient Boosting algorithm<br>achieved the best overall result. For this dataset, we should consider using<br>the eXtreme Gradient Boosting algorithm for further modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 1","author_name":"David Lowe","blog_date_text":"Sun, 17 Feb 2019 13:30:51 +0000","blog_url":"https://dainesanalytics.blog/2019/02/17/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-1/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the Python script attempts to follow the page links and scrape the quote information off each page.<br><br><br><br>Starting URLs: http://quotes.toscrape.com/<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 4","author_name":"David Lowe","blog_date_text":"Fri, 15 Feb 2019 13:42:55 +0000","blog_url":"https://dainesanalytics.blog/2019/02/15/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Human Activities and Postural<br>Transitions dataset is a classic multi-class classification situation where we<br>are trying to predict one of the 12 possible outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,<br>lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a<br>smartphone on the waist during the experiment execution. The research team also<br>video-recorded the activities to label the data manually. The research team<br>randomly partitioned the obtained data into two sets, 70% for the training data<br>and 30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of eliminating collinear features. We performed iterative modeling at<br>collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear<br>features, we decreased the processing time and maintained a comparable level of<br>model accuracy comparing to iteration Take1.<br><br><br><br>In iteration Take3, we examined the feature selection<br>technique of attribute importance ranking by using the Random Forest algorithm.<br>By selecting only the most important attributes, we hoped to decrease the<br>processing time and to maintain a similar level of accuracy compared to<br>iteration Take1.<br><br><br><br>In the current iteration Take4, we will examine the feature<br>selection technique of Recursive Feature Elimination by using the Linear<br>Discriminant Analysis algorithm. By limiting to only the 300 most relevant<br>attributes, we hope to decrease the processing time and maintain a similar<br>level of accuracy compared to iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 88.52%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 94.19%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 94.71%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 88.04%. Two algorithms<br>(Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the<br>top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, Linear Discriminant Analysis turned in the top overall result<br>and achieved an accuracy metric of 92.32%. By using the optimized parameters,<br>the Linear Discriminant Analysis algorithm processed the testing dataset with<br>an accuracy of 93.89%, which was even better than the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 278, from 561 down to 283. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 4 hours 31 minutes in Take2,<br>which was a reduction of 35.9%.<br><br><br><br>In iteration Take3, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 89.02%. Two algorithms<br>(Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the<br>top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, Linear Discriminant Analysis turned in the top overall result<br>and achieved an accuracy metric of 94.42%. By using the optimized parameters,<br>the Linear Discriminant Analysis algorithm processed the testing dataset with<br>an accuracy of 94.97%, which was even better than the training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 107, from 561 down to 454. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 6 hours 06 minutes in Take3,<br>which was a reduction of 13.4%.<br><br><br><br>In the current iteration Take4, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 87.22%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 91.45%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 91.42%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 261, from 561 down to 300. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 4 hours 57 minutes in Take4,<br>which was a reduction of 29.7%.<br><br><br><br>CONCLUSION: For this iteration, the Recursive Feature<br>Elimination technique and the Linear Discriminant Analysis algorithm achieved<br>the best overall results while reducing the processing time. For this dataset,<br>we should consider using the Linear Discriminant Analysis algorithm for further<br>modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 3","author_name":"David Lowe","blog_date_text":"Wed, 13 Feb 2019 13:18:31 +0000","blog_url":"https://dainesanalytics.blog/2019/02/13/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning<br>algorithms and to document the end-to-end steps using a template. The Human<br>Activities and Postural Transitions dataset is a classic multi-class<br>classification situation where we are trying to predict one of the 12 possible<br>outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are<br>stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and<br>lie-to-stand. All the participants were wearing a smartphone on the waist<br>during the experiment execution. The research team also video-recorded the<br>activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and<br>30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of eliminating collinear features. We performed iterative modeling at<br>collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear<br>features, we decreased the processing time and maintained a comparable level of<br>model accuracy comparing to iteration Take1.<br><br><br><br>In the current iteration Take3, we will examine the feature<br>selection technique of attribute importance ranking by using the Random Forest<br>algorithm. By selecting only the most important attributes, we hope to decrease<br>the processing time and maintain a similar level of accuracy compared to<br>iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 89.61%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Stochastic Gradient Boosting turned in the top overall<br>result and achieved an accuracy metric of 97.70%. By using the optimized<br>parameters, the Stochastic Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 92.85%, which was below the training data and<br>possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 89.48%. Two algorithms<br>(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics<br>after the first round of modeling. After a series of tuning trials, eXtreme<br>Gradient Boosting turned in the top overall result and achieved an accuracy<br>metric of 98.31%. By using the optimized parameters, the eXtreme Gradient<br>Boosting algorithm processed the testing dataset with an accuracy of 94.97%,<br>which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 278, from 561 down to 283. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in<br>Take2, which was a reduction of 23.9%.<br><br><br><br>In the current iteration Take3, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 90.04%. Two<br>algorithms (Random Forest and eXtreme Gradient Boosting) achieved the top<br>accuracy metrics after the first round of modeling. After a series of tuning<br>trials, eXtreme Gradient Boosting turned in the top overall result and achieved<br>an accuracy metric of 98.35%. By using the optimized parameters, the eXtreme<br>Gradient Boosting algorithm processed the testing dataset with an accuracy of<br>93.86%, which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 62, from 561 down to 499. The processing time went from<br>10 hours 17 minutes in iteration Take1 up to 16 hours 59 minutes in Take3,<br>which was an increase of 65.1%.<br><br><br><br>CONCLUSION: For this iteration, the attribute importance<br>ranking technique and using the eXtreme Gradient Boosting algorithm achieved<br>the best overall results. For this dataset, we should consider using the<br>eXtreme Gradient Boosting algorithm for further modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 3","author_name":"David Lowe","blog_date_text":"Mon, 11 Feb 2019 13:12:09 +0000","blog_url":"https://dainesanalytics.blog/2019/02/11/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Human Activities and Postural<br>Transitions dataset is a classic multi-class classification situation where we<br>are trying to predict one of the 12 possible outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,<br>lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a<br>smartphone on the waist during the experiment execution. The research team also<br>video-recorded the activities to label the data manually. The research team<br>randomly partitioned the obtained data into two sets, 70% for the training data<br>and 30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In iteration Take2, we examined the feature selection<br>technique of eliminating collinear features. We performed iterative modeling at<br>collinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear<br>features, we decreased the processing time and maintained a comparable level of<br>model accuracy comparing to iteration Take1.<br><br><br><br>In the current iteration Take3, we will examine the feature<br>selection technique of attribute importance ranking by using the Random Forest<br>algorithm. By selecting only the most important attributes, we hope to decrease<br>the processing time and maintain a similar level of accuracy compared to<br>iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 88.52%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 94.19%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 94.71%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>In iteration Take2, the baseline performance of the machine<br>learning algorithms achieved an average accuracy of 88.04%. Two algorithms (Linear<br>Discriminant Analysis and Stochastic Gradient Boosting) achieved the top<br>accuracy metrics after the first round of modeling. After a series of tuning<br>trials, Linear Discriminant Analysis turned in the top overall result and<br>achieved an accuracy metric of 92.32%. By using the optimized parameters, the<br>Linear Discriminant Analysis algorithm processed the testing dataset with an<br>accuracy of 93.89%, which was even better than the training data.<br><br><br><br>From the model-building perspective, the number of attributes<br>decreased by 278, from 561 down to 283. The processing time went from 7 hours 3<br>minutes in iteration Take1 down to 4 hours 31 minutes in Take2, which was a<br>reduction of 35.9%.<br><br><br><br>In the current iteration Take3, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 89.02%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 94.42%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 94.97%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 107, from 561 down to 454. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 6 hours 06 minutes in Take3,<br>which was a reduction of 13.4%.<br><br><br><br>CONCLUSION: For this iteration, the attribute importance<br>ranking technique and the Linear Discriminant Analysis algorithm achieved the<br>best overall results while reducing the processing time. For this dataset, we<br>should consider using the Linear Discriminant Analysis algorithm for further<br>modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping Templates for R and Python","author_name":"David Lowe","blog_date_text":"Sun, 10 Feb 2019 13:05:13 +0000","blog_url":"https://dainesanalytics.blog/2019/02/10/web-scraping-templates-for-r-and-python/","blog_text":"As I work on practicing and solving web scraping problems, I<br>find myself repeating a set of steps and activities repeatedly.<br><br><br><br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support web scraping tasks using R or Python.<br><br><br><br>The R scripts leverage the rvest package, while the Python scripts leverage the Scrapy framework. You will find the web scraping templates from the Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 2","author_name":"David Lowe","blog_date_text":"Fri, 08 Feb 2019 13:06:31 +0000","blog_url":"https://dainesanalytics.blog/2019/02/08/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning<br>algorithms and to document the end-to-end steps using a template. The Human<br>Activities and Postural Transitions dataset is a classic multi-class<br>classification situation where we are trying to predict one of the 12 possible<br>outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are<br>stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and<br>lie-to-stand. All the participants were wearing a smartphone on the waist<br>during the experiment execution. The research team also video-recorded the<br>activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and<br>30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In the current iteration Take2, we will examine the feature<br>selection technique of eliminating collinear features. We will perform<br>iterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By<br>eliminating the collinear features, we hope to decrease the processing time and<br>maintain a comparable level of model accuracy comparing to iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 89.61%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Stochastic Gradient Boosting turned in the top overall<br>result and achieved an accuracy metric of 97.70%. By using the optimized<br>parameters, the Stochastic Gradient Boosting algorithm processed the testing<br>dataset with an accuracy of 92.85%, which was below the training data and<br>possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>COL_75%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>89.75%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved<br>the top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, eXtreme Gradient Boosting turned in the top overall result and<br>achieved an accuracy metric of 97.84%. By using the optimized parameters, the<br>eXtreme Gradient Boosting algorithm processed the testing dataset with an<br>accuracy of 94.75%, which was below the training data and possibly due to<br>over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 408, from 561 down to 153. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 4 hours 26 minutes in<br>Take2, which was a reduction of 66.6%.<br><br><br><br>COL_80%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>90.27%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved<br>the top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved<br>an accuracy metric of 98.00%. By using the optimized parameters, the eXtreme<br>Gradient Boosting algorithm processed the testing dataset with an accuracy of<br>94.91%, which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 385, from 561 down to 176. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 4 hours 56 minutes in<br>Take2, which was a reduction of 47.9%.<br><br><br><br>COL_85%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>89.31%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved<br>the top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, eXtreme Gradient Boosting turned in the top overall result and<br>achieved an accuracy metric of 98.14%. By using the optimized parameters, the<br>eXtreme Gradient Boosting algorithm processed the testing dataset with an<br>accuracy of 94.15%, which was below the training data and possibly due to<br>over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 362, from 561 down to 199. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 5 hours 27 minutes in<br>Take2, which was a reduction of 47.0%.<br><br><br><br>COL_90%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>89.38%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved<br>the top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, eXtreme Gradient Boosting turned in the top overall result and<br>achieved an accuracy metric of 98.17%. By using the optimized parameters, the<br>eXtreme Gradient Boosting algorithm processed the testing dataset with an<br>accuracy of 94.12%, which was below the training data and possibly due to<br>over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 338, from 561 down to 223. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 6 hours 2 minutes in Take2,<br>which was a reduction of 41.3%.<br><br><br><br>COL_95%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>89.48%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved<br>the top accuracy metrics after the first round of modeling. After a series of<br>tuning trials, eXtreme Gradient Boosting turned in the top overall result and<br>achieved an accuracy metric of 98.31%. By using the optimized parameters, the<br>eXtreme Gradient Boosting algorithm processed the testing dataset with an<br>accuracy of 94.97%, which was below the training data and possibly due to<br>over-fitting.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 278, from 561 down to 283. The processing time went<br>from 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in<br>Take2, which was a reduction of 23.9%.<br><br><br><br>CONCLUSION: For this iteration, eliminating collinear<br>features at the 95% level and using the eXtreme Gradient Boosting algorithm<br>achieved the best overall results. For this dataset, we should consider using<br>the eXtreme Gradient Boosting algorithm for further modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Time Series Model for Monthly Sales of French Champagne Using Python","author_name":"David Lowe","blog_date_text":"Wed, 06 Feb 2019 13:05:26 +0000","blog_url":"https://dainesanalytics.blog/2019/02/06/time-series-model-for-annual-water-usage-in-baltimore-using-python-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>Code Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>PREFACE: This is a replication of Python code from Dr.<br>Brownlee’s blog post on time series. I have combined all the code snippets into<br>one script so that I can turn the whole process into a template. The comments<br>and analysis were also part of the blog post and annotated here to explain each<br>coding block.<br><br><br><br>SUMMARY: The purpose of this project is to construct a time<br>series prediction model and document the end-to-end steps using a template. The<br>Monthly Sales of French Champagne dataset is a time series situation where we<br>are trying to forecast future outcomes based on the past data points.<br><br><br><br>INTRODUCTION: The problem is to predict the number of<br>monthly sales of champagne for the Perrin Freres label (named for a region in<br>France). The dataset provides the number of monthly sales of champagne from<br>January 1964 to September 1972, or just under 10 years of data. The values are<br>a count of millions of sales and there are 105 observations. The dataset is<br>credited to Makridakis and Wheelwright, 1989.<br><br><br><br>ANALYSIS: The baseline prediction (or persistence) for the<br>dataset resulted in an RMSE of 3186.501. The manually configured model was<br>simplified to ARIMA(1,1,1) and produced an RMSE of 956.958, which is<br>dramatically better than the persistence RMSE of 3186.501. After applying the<br>grid search technique to the dataset, the final RMSE of the ARIMA(0,0,1) model<br>was 939.464, which is slightly lower than the manually configured ARIMA from the<br>previous section. This difference may or may not be statistically significant.<br>At the end, we selected ARIMA(0,0,1) as the final model.<br><br><br><br>CONCLUSION: The final RMSE for the validation period is<br>predicted at 361 million sales. This is much better than the expectation of an<br>error of a little more than 924 million sales per month. At this scale on the<br>plot, the 12 months of forecast sales figures look fantastic.<br><br><br><br>Dataset Used: Monthly Sales of French Champagne<br><br><br><br>Dataset ML Model: Time series forecast with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://datamarket.com/data/set/22r5/perrin-freres-monthly-champagne-sales-millions-64-72#!ds=22r5&display=line<br><br><br><br>One potential source of performance benchmark:<br>https://machinelearningmastery.com/time-series-forecast-study-python-monthly-sales-french-champagne/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 04 Feb 2019 13:33:45 +0000","blog_url":"https://dainesanalytics.blog/2019/02/04/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. <br><br><br><br>SUMMARY: The purpose of this project is to construct a<br>prediction model using various machine learning algorithms and to document the<br>end-to-end steps using a template. The Human Activities and Postural<br>Transitions dataset is a classic multi-class classification situation where we<br>are trying to predict one of the 12 possible outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with<br>a group of 30 volunteers who performed a protocol of activities composed of six<br>basic activities. There are three static postures (standing, sitting, lying)<br>and three dynamic activities (walking, walking downstairs and walking<br>upstairs). The experiment also included postural transitions that occurred<br>between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,<br>lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a<br>smartphone on the waist during the experiment execution. The research team also<br>video-recorded the activities to label the data manually. The research team<br>randomly partitioned the obtained data into two sets, 70% for the training data<br>and 30% for the testing.<br><br><br><br>In iteration Take1, the script focused on evaluating various<br>machine learning algorithms and identifying the model that produces the best<br>overall metrics. Because the dataset has many attributes that were collinear<br>with other attributes, we eliminated the attributes that have a collinearity<br>measurement of 99% or higher. Iteration Take1 established the performance<br>baseline for accuracy and processing time.<br><br><br><br>In the current iteration Take2, we will examine the feature<br>selection technique of eliminating collinear features. We will perform<br>iterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By<br>eliminating the collinear features, we hope to decrease the processing time and<br>maintain a comparable level of model accuracy comparing to iteration Take1.<br><br><br><br>ANALYSIS: In iteration Take1, the baseline performance of<br>the machine learning algorithms achieved an average accuracy of 88.52%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 94.19%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 94.71%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 108, from 561 down to 453.<br><br><br><br>COL_75%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>88.53%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient<br>Boosting) achieved the top accuracy metrics after the first round of modeling.<br>After a series of tuning trials, Stochastic Gradient Boosting turned in the top<br>overall result and achieved an accuracy metric of 91.63%. By using the<br>optimized parameters, the Stochastic Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 92.53%, which was even better than the<br>training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 408, from 561 down to 153. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 2 hours 48 minutes in Take2,<br>which was a reduction of 60.2%.<br><br><br><br>COL_80%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>85.96%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient<br>Boosting) achieved the top accuracy metrics after the first round of modeling.<br>After a series of tuning trials, Stochastic Gradient Boosting turned in the top<br>overall result and achieved an accuracy metric of 91.86%. By using the<br>optimized parameters, the Stochastic Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 93.13%, which was even better than the<br>training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 385, from 561 down to 176. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 3 hours 12 minutes in Take2,<br>which was a reduction of 54.6%.<br><br><br><br>COL_85%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>87.32%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient<br>Boosting) achieved the top accuracy metrics after the first round of modeling.<br>After a series of tuning trials, Stochastic Gradient Boosting turned in the top<br>overall result and achieved an accuracy metric of 91.16%. By using the<br>optimized parameters, the Stochastic Gradient Boosting algorithm processed the<br>testing dataset with an accuracy of 92.47%, which was even better than the<br>training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 362, from 561 down to 199. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 3 hours 37 minutes in Take2,<br>which was a reduction of 48.6%.<br><br><br><br>COL_90%: In the current iteration Take2, the baseline performance<br>of the machine learning algorithms achieved an average accuracy of 87.22%. Two<br>algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)<br>achieved the top accuracy metrics after the first round of modeling. After a<br>series of tuning trials, Linear Discriminant Analysis turned in the top overall<br>result and achieved an accuracy metric of 91.52%. By using the optimized<br>parameters, the Linear Discriminant Analysis algorithm processed the testing<br>dataset with an accuracy of 92.82%, which was even better than the training<br>data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 338, from 561 down to 223. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 3 hours 53 minutes in Take2,<br>which was a reduction of 44.9%.<br><br><br><br>COL_95%: In the current iteration Take2, the baseline<br>performance of the machine learning algorithms achieved an average accuracy of<br>88.04%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient<br>Boosting) achieved the top accuracy metrics after the first round of modeling.<br>After a series of tuning trials, Linear Discriminant Analysis turned in the top<br>overall result and achieved an accuracy metric of 92.32%. By using the<br>optimized parameters, the Linear Discriminant Analysis algorithm processed the<br>testing dataset with an accuracy of 93.89%, which was even better than the<br>training data.<br><br><br><br>From the model-building perspective, the number of<br>attributes decreased by 278, from 561 down to 283. The processing time went<br>from 7 hours 3 minutes in iteration Take1 down to 4 hours 31 minutes in Take2,<br>which was a reduction of 35.9%.<br><br><br><br>CONCLUSION: For this iteration, eliminating collinear<br>features at the 95% level and using the Linear Discriminant Analysis algorithm<br>achieved the best overall results. For this dataset, we should consider using<br>the Linear Discriminant Analysis algorithm for further modeling or production<br>use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human<br>Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical<br>attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using Python and BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 03 Feb 2019 13:41:18 +0000","blog_url":"https://dainesanalytics.blog/2019/02/03/web-scraping-of-merely-do-it-blog-entries-using-python-and-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by gathering specific pieces of information from a website. The web<br>scraping code was written in Python and leveraged the BeautifulSoup module.<br><br><br><br>INTRODUCTION: David Lowe hosts his blog at merelydoit.blog.<br>The purpose of this exercise is to practice web scraping by gathering the blog<br>entries from Merely Do It’s RSS feed. This iteration of the script<br>automatically traverses the RSS feed to capture all entries from the blog site.<br><br><br><br>Starting URLs: https://merelydoit.blog/feed or<br>https://merelydoit.blog/feed/?paged=1<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 1","author_name":"David Lowe","blog_date_text":"Fri, 01 Feb 2019 13:13:05 +0000","blog_url":"https://dainesanalytics.blog/2019/02/01/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.<br><br><br><br>INTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.<br><br><br><br>In the current iteration Take1, the script will focus on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that are collinear with other attributes, we will eliminate the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 will establish the baseline performance for accuracy and processing time.<br><br><br><br>ANALYSIS: In the current iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.80%. Two algorithms (Linear Discriminant Analysis and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.59%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.67%, which was below the training data and possibly due to over-fitting.<br><br><br><br>From the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.<br><br><br><br>CONCLUSION: For this iteration, the eXtreme Gradient Boosting algorithm achieved the best overall results. For this dataset, we should consider using the eXtreme Gradient Boosting algorithm for further modeling or production use.<br><br><br><br>Dataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set<br><br><br><br>Dataset ML Model: Multi-class classification with numerical attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Time Series Model for Annual Water Usage in Baltimore Using Python","author_name":"David Lowe","blog_date_text":"Wed, 30 Jan 2019 13:22:39 +0000","blog_url":"https://dainesanalytics.blog/2019/01/30/time-series-model-for-annual-water-usage-in-baltimore-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>Code Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>PREFACE: This is a replication of Python code from Dr.<br>Brownlee’s blog post on time series. I have combined all the code snippets into<br>one script so that I can turn the whole<br>process into a template. The comments and analysis were also part of the blog<br>post and annotated here to explain each coding block.<br><br><br><br>SUMMARY: The purpose of this project is to construct a time<br>series prediction model and document the end-to-end steps using a template. The<br>Annual Water Usage in Baltimore dataset is a time series situation where we are<br>trying to forecast future outcomes based on the past data points.<br><br><br><br>INTRODUCTION: The problem is to predict annual water usage.<br>The dataset provides the annual water usage in Baltimore from 1885 to 1963, or<br>79 years of data. The dataset contains 79 observations in the units of liters<br>per capita per day and is credited to<br>Hipel and McLeod, 1994.<br><br><br><br>ANALYSIS: The baseline prediction (or persistence) for the<br>dataset resulted in an RMSE of 21.975. The manually configured model was<br>simplified to ARIMA(4,1,1) and produced an RMSE of 31.097, which was higher<br>than the persistent model. After applying the grid search technique to the<br>dataset, the final RMSE of the ARIMA(2,1,0) model was 21.733. This is only a<br>slightly smaller error than the persistent model, and it may or may not be<br>statistically different.<br><br><br><br>CONCLUSION: The final RMSE for the validation period is predicted at 16 liters per capita per day. This is not too different from the expected<br>error of 21, but we would expect that it is also not too different from a<br>simple persistence model. The forecast does have the characteristics of a<br>persistence forecast. This suggests that<br>although this time series does have an obvious trend, it is still a reasonably<br>difficult problem.<br><br><br><br>Dataset Used: Annual Water Usage in Baltimore<br><br><br><br>Dataset ML Model: Time series forecast with numerical<br>attributes<br><br><br><br>Dataset Reference:<br>https://datamarket.com/data/set/22sl/baltmore-city-annual-water-use-liters-per-capita-per-day-1885-1968#!ds=22sl&display=line<br><br><br><br>One potential source of performance benchmark:<br>https://machinelearningmastery.com/time-series-forecast-study-python-annual-water-usage-baltimore/<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 28 Jan 2019 13:11:07 +0000","blog_url":"https://dainesanalytics.blog/2019/01/28/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.<br>INTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.<br>In the current iteration Take1, the script will focus on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that are collinear with other attributes, we will eliminate the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 will establish the baseline performance for accuracy and processing time.<br>ANALYSIS: In the current iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 88.52%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 94.19%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 94.71%, which was even better than the training data.<br>From the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.<br>CONCLUSION: For this iteration, the Linear Discriminant Analysis algorithm achieved the best overall results. For this dataset, we should consider using the Linear Discriminant Analysis algorithm for further modeling or production use.<br>Dataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using Python and Scrapy","author_name":"David Lowe","blog_date_text":"Sun, 27 Jan 2019 13:35:32 +0000","blog_url":"https://dainesanalytics.blog/2019/01/27/web-scraping-of-merely-do-it-blog-entries-using-python-and-scrapy/","blog_text":"SUMMARY: The purpose of this project is to practice web<br>scraping by gathering specific pieces of information from a website. The web<br>scraping code was written in Python and leveraged the Scrapy framework.<br><br><br><br>INTRODUCTION: David Lowe hosts his blog at merelydoit.blog.<br>The purpose of this exercise is to practice web scraping by gathering the blog<br>entries from Merely Do It’s RSS feed. This iteration of the script<br>automatically traverses the RSS feed to capture all entries from the blog site.<br><br><br><br>Starting URLs: https://merelydoit.blog/feed or<br>https://merelydoit.blog/feed/?paged=1<br><br><br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 4","author_name":"David Lowe","blog_date_text":"Fri, 25 Jan 2019 13:26:42 +0000","blog_url":"https://dainesanalytics.blog/2019/01/25/binary-classification-model-for-miniboone-particle-identification-using-python-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>From the previous iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>From the previous iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.<br>From the previous iteration Take3, we examined the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.<br>In the current iteration Take4, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.<br>From the previous iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 91.04%. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.84%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was just slightly below the training data.<br>In the current iteration Take4, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.11%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.68%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.70%, which was just slightly below the training data.<br>From the model-building perspective, the number of attributes decreased by 10, from 50 down to 40 in iteration Take4. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 56 minutes in Take4, which was a reduction of 3.4% from Take1. It was a slight increase in comparison to Take3, which had a processing time of 6 hours 22 minutes. It was also a slight increase in comparison to Take2, which had a processing time of 6 hours 33 minutes.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results with an improved processing time after running Recursive Feature Elimination. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Time Series Model for Monthly Armed Robberies in Boston Using Python","author_name":"David Lowe","blog_date_text":"Wed, 23 Jan 2019 13:21:41 +0000","blog_url":"https://dainesanalytics.blog/2019/01/23/time-series-model-for-monthly-armed-robberies-in-boston-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Code Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>PREFACE: This is a replication of Python code from Dr. Brownlee’s blog post on time series. I have combined all the code snippets into one script so that I could turn the whole process into a template. The comments and analysis were also part of the blog post and annotated here to explain each coding block.<br>SUMMARY: The purpose of this project is to construct a time series prediction model and document the end-to-end steps using a template. The Monthly Armed Robberies in Boston dataset is a time series situation where we are trying to forecast future outcomes based on the past data points.<br>INTRODUCTION: The problem is to predict the number of monthly armed robberies in Boston, USA. The dataset provides the number of monthly armed robberies in Boston from January 1966 to October 1975, Monthly Boston armed robberies Jan.1966-Oct.1975 Deutsch and Alt (1977) and credited to McCleary & Hay (1980).<br>ANALYSIS: The baseline prediction (or persistence) for the dataset resulted in an RMSE of 51.8. The manually configured model was simplified to ARIMA(0,1,2) and produced an RMSE of 49.8, which was slightly better than the persistent model. After applying the power transformation (Box-Cox) to the dataset, the final RMSE of the model on the transformed data was 49.4. This is only a slightly smaller error than the ARIMA model on untransformed data, and it may or may not be statistically different.<br>CONCLUSION: The final RMSE for the validation period was predicted at 53 robberies, and it was not too different from the expected error of 49. Although the forecast appears to have the characteristic of a persistence forecast, this does suggest that, while this time series does have an obvious trend, it is still a reasonably difficult problem.<br>Dataset Used: Monthly Armed Robberies in Boston<br>Dataset ML Model: Time series forecast with numerical attributes<br>Dataset Reference: https://datamarket.com/data/set/22ob/monthly-boston-armed-robberies-jan1966-oct1975-deutsch-and-alt-1977#!ds=22ob&display=line<br>One potential source of performance benchmark: https://machinelearningmastery.com/time-series-forecast-case-study-python-monthly-armed-robberies-boston/<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 4","author_name":"David Lowe","blog_date_text":"Mon, 21 Jan 2019 13:20:25 +0000","blog_url":"https://dainesanalytics.blog/2019/01/21/binary-classification-model-for-miniboone-particle-identification-using-r-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>From the previous iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>From the previous iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.<br>From the previous iteration Take3, we examined the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.<br>In the current iteration Take4, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.<br>From the previous iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.49%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.52%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was even better than the training data.<br>In the current iteration Take4, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.79%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.??%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.??%, which was even better than the training data.<br>From the model-building perspective, the number of attributes decreased by 10, from 50 down to 40 in iteration Take4. The processing time went from 17 hours 18 minutes in iteration Take1 down to 30 hours 58 minutes in Take3, which was an increase of 21.5% from Take1. It was a significant increase in comparison to Take2, which had a processing time of 12 hours 17 minutes. It was also a significant increase in comparison to Take3, which had a processing time of 6 hours 48 minutes.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results with an increased processing time after running Recursive Feature Elimination. For this dataset, the Stochastic Gradient Boosting and Random Forest algorithms should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using R","author_name":"David Lowe","blog_date_text":"Sun, 20 Jan 2019 13:12:14 +0000","blog_url":"https://dainesanalytics.blog/2019/01/20/web-scraping-of-merely-do-it-blog-entries-using-r/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: David Lowe hosts his blog at merelydoit.blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Merely Do It’s RSS feed. This iteration of the script automatically traverses the RSS feed to capture all entries from the blog site.<br>Starting URLs: https://merelydoit.blog/feed or https://merelydoit.blog/feed/?paged=1<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 3","author_name":"David Lowe","blog_date_text":"Fri, 18 Jan 2019 13:56:15 +0000","blog_url":"https://dainesanalytics.blog/2019/01/18/binary-classification-model-for-miniboone-particle-identification-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>In iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.<br>For this iteration, we will examine the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.<br>In the current iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 91.04%. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.84%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was just slightly below the training data.<br>From the model-building perspective, the number of attributes decreased by 23, from 50 down to 27 in iteration Take3. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 22 minutes in Take3, which was a reduction of 11.3% from Take1. It was also a slight decrease in comparison to Take2, which had a processing time of 6 hours 33 minutes.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results with an improved processing time after eliminating the unimportant features. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 16 Jan 2019 13:46:43 +0000","blog_url":"https://dainesanalytics.blog/2019/01/16/web-scraping-of-machine-learning-mastery-blog-using-r-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: Dr. Jason Brownlee’s Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery’s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.<br>Starting URLs: https://machinelearningmastery.com/blog<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 3","author_name":"David Lowe","blog_date_text":"Mon, 14 Jan 2019 13:46:03 +0000","blog_url":"https://dainesanalytics.blog/2019/01/14/binary-classification-model-for-miniboone-particle-identification-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>In iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.<br>For this iteration, we will examine the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.<br>In the current iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.49%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.52%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was even better than the training data.<br>From the model-building perspective, the number of attributes decreased by 25, from 50 down to 25 in iteration Take3. The processing time went from 17 hours 18 minutes in iteration Take1 down to 6 hours 48 minutes in Take3, which was a reduction of 60.6% from Take1. It was also a significant decrease in comparison to Take2, which had a processing time of 12 hours 17 minutes.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results with an improved processing time after eliminating the unimportant features. For this dataset, the Stochastic Gradient Boosting and Random Forest algorithms should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Using R Take 1","author_name":"David Lowe","blog_date_text":"Sun, 13 Jan 2019 13:07:20 +0000","blog_url":"https://dainesanalytics.blog/2019/01/13/web-scraping-of-machine-learning-mastery-blog-using-r-take-1/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: Dr. Jason Brownlee’s Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery’s RSS feed. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.<br>Starting URLs: https://machinelearningmastery.com/feed or https:// machinelearningmastery.com /feed/?paged=1<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 2","author_name":"David Lowe","blog_date_text":"Fri, 11 Jan 2019 13:58:08 +0000","blog_url":"https://dainesanalytics.blog/2019/01/11/binary-classification-model-for-miniboone-particle-identification-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>For this iteration, we will explore the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hope to decrease the processing time while maintaining an acceptable level of accuracy loss.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.<br>In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.<br>From the model-building perspective, the number of attributes decreased by 13, from 50 down to 37. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 33 minutes in Take2, which was a reduction of 8.8% from Take1.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Daines Analytics Blog Entries Using R","author_name":"David Lowe","blog_date_text":"Wed, 09 Jan 2019 13:01:14 +0000","blog_url":"https://dainesanalytics.blog/2019/01/09/web-scraping-of-daines-analytics-blog-entries-using-r/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Daines Analytics’ RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries.<br>Starting URLs: https://dainesanalytics.blog/feed or https://dainesanalytics.blog/feed/?paged=1<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 2","author_name":"David Lowe","blog_date_text":"Mon, 07 Jan 2019 13:44:37 +0000","blog_url":"https://dainesanalytics.blog/2019/01/07/binary-classification-model-for-miniboone-particle-identification-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.<br>For this iteration, we will explore the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hope to decrease the processing time while maintaining an acceptable level of accuracy loss.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.<br>In the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.<br>From the model-building perspective, the number of attributes decreased by 13, from 50 down to 37. The processing time went from 17 hours 18 minutes in iteration Take1 down to 12 hours 17 minutes in Take2, which was a reduction of 28% from Take1.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boostin algorithm achieved the best overall results with an improved processing time after eliminating the collinear features. For this dataset, the Stochastic Gradient Boostin algorithm should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python and BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 06 Jan 2019 13:47:52 +0000","blog_url":"https://dainesanalytics.blog/2019/01/06/web-scraping-of-machine-learning-mastery-blog-entries-using-python-and-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the BeautifulSoup module.<br>INTRODUCTION: Dr. Jason Brownlee’s Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery’s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.<br>Starting URLs: https://machinelearningmastery.com/blog<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date_text":"Fri, 04 Jan 2019 13:02:41 +0000","blog_url":"https://dainesanalytics.blog/2019/01/04/updated-machine-learning-templates-for-r-4/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br>Version 7 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support the following data cleaning and transformation tasks:<br>Feature Selection via Collinear Features Removal, Attribute Importance Ranking, and Recursive Feature Elimination (RFE)<br>Improve unbalanced datasets via Synthetic Minority Over-sampling Technique (SMOTE)<br>You will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date_text":"Wed, 02 Jan 2019 13:00:25 +0000","blog_url":"https://dainesanalytics.blog/2019/01/02/updated-machine-learning-templates-for-python-4/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br>Version 7 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support the following data cleaning and transformation tasks:<br>One-Hot-Encoding for categorical variables<br>Feature Selection via Collinear Features Removal, Attribute Importance Ranking, and Recursive Feature Elimination (RFE)<br>Improve unbalanced datasets via Synthetic Minority Over-sampling Technique (SMOTE)<br>You will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 31 Dec 2018 13:34:19 +0000","blog_url":"https://dainesanalytics.blog/2018/12/31/binary-classification-model-for-miniboone-particle-identification-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>ANALYSIS: The baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results using the training and testing datasets. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of NeurIPS Proceedings Using Python and Scrapy","author_name":"David Lowe","blog_date_text":"Sun, 30 Dec 2018 13:41:56 +0000","blog_url":"https://dainesanalytics.blog/2018/12/30/web-scraping-of-neurips-proceedings-using-python-and-scrapy/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping python code leverages the Scrapy framework.<br>INTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https://papers.nips.cc/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2017 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.<br>Starting URLs: https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 1","author_name":"David Lowe","blog_date_text":"Fri, 28 Dec 2018 13:36:45 +0000","blog_url":"https://dainesanalytics.blog/2018/12/28/binary-classification-model-for-miniboone-particle-identification-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.<br>ANALYSIS: The baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results using the training and testing datasets. For this dataset, the Random Forest algorithm should be considered for further modeling or production use.<br>Dataset Used: MiniBooNE particle identification Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of NeurIPS Proceedings Using R","author_name":"David Lowe","blog_date_text":"Wed, 26 Dec 2018 13:27:08 +0000","blog_url":"https://dainesanalytics.blog/2018/12/26/web-scraping-of-neurips-proceedings-using-r/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping R code leverages the rvest package.<br>INTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https://papers.nips.cc/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2016 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.<br>Starting URLs: https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using R Take 2","author_name":"David Lowe","blog_date_text":"Mon, 24 Dec 2018 13:48:40 +0000","blog_url":"https://dainesanalytics.blog/2018/12/24/binary-classification-model-for-caravan-insurance-marketing-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.<br>The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.<br>The insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?<br>In iteration Take1, we had algorithms with high accuracy but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.<br>ANALYSIS: From the Take1 iteration, the baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0.7159. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5285, which was significant below the result from the training data.<br>From the current iteration, the baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0. 9243. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5746, which was significant below the result from the training data.<br>CONCLUSION: For this iteration, the SMOTE technique improved the unbalanced dataset we have but did not improve the algorithm’s final performance metric. Overall, the Random Forest algorithm achieved the leading ROC scores using the training dataset, but the model failed to perform adequately using the validation dataset. For this dataset, Random Forest still should be considered for further modeling and testing before making it available for production use.<br>Dataset Used: Insurance Company Benchmark (COIL 2000) Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+(COIL+2000)<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/caravan-insurance-challenge<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of NeurIPS Proceedings Using Python and BeautifulSoup","author_name":"David Lowe","blog_date_text":"Sun, 23 Dec 2018 13:13:57 +0000","blog_url":"https://dainesanalytics.blog/2018/12/23/web-scraping-of-neurips-proceedings-using-python-and-beautifulsoup/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping python code leverages the BeautifulSoup module.<br>INTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https://papers.nips.cc/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2015 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.<br>Starting URLs: https://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using Python Take 2","author_name":"David Lowe","blog_date_text":"Fri, 21 Dec 2018 13:18:14 +0000","blog_url":"https://dainesanalytics.blog/2018/12/21/binary-classification-model-for-caravan-insurance-marketing-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.<br>The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.<br>The insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?<br>In iteration Take1, we had algorithms with high accuracy but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.<br>ANALYSIS: From the previous Take1 iteration, the baseline performance of the ten algorithms achieved an average F1_Micro score of 0.9260. Two algorithms, Logistic Regression and Support Vector Machine, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an F1_Micro score of 0.9402. After using the optimized tuning parameters, the Support Vector Machine algorithm processed the validation dataset with an F1_Micro score of 0.9405, which was slightly better than using the training data.<br>From the current iteration, the baseline performance of the eight algorithms achieved an average F1_Micro score of 0.9326. Two algorithms, Random Forest and Extra Trees, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an F1_Micro score of 0.9595. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with an F1_Micro score of 0.9165, which was noticeably worse than using the training data and perhaps due to overfitting.<br>CONCLUSION: For this iteration, the SMOTE technique improved the unbalanced dataset we have but did not improve the algorithm’s final performance metric. Overall, the Random Forest algorithm achieved the leading F1_Micro scores using the training dataset, but the model failed to perform adequately using the validation dataset. For this dataset, Random Forest still should be considered for further modeling and testing before making it available for production use.<br>Dataset Used: Insurance Company Benchmark (COIL 2000) Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+(COIL+2000)<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/caravan-insurance-challenge<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python Take 2","author_name":"David Lowe","blog_date_text":"Wed, 19 Dec 2018 13:23:11 +0000","blog_url":"https://dainesanalytics.blog/2018/12/19/web-scraping-of-machine-learning-mastery-blog-entries-using-python-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: Dr. Jason Brownlee’s Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Machine Learning Mastery’s RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries and store all captured information in a JSON output file.<br>Starting URLs: https://machinelearningmastery.com/blog<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 17 Dec 2018 13:59:37 +0000","blog_url":"https://dainesanalytics.blog/2018/12/17/binary-classification-model-for-caravan-insurance-marketing-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.<br>The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.<br>The insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?<br>ANALYSIS: The baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0.7159. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5285, which was significant below the result from the training data.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the leading ROC scores using the training and validation datasets. For this dataset, the Random Forest algorithm does not appear to be adequate for production use. Further modeling and testing are recommended for the next step.<br>Dataset Used: Insurance Company Benchmark (COIL 2000) Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+(COIL+2000)<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/caravan-insurance-challenge<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python Take 1","author_name":"David Lowe","blog_date_text":"Sun, 16 Dec 2018 13:13:14 +0000","blog_url":"https://dainesanalytics.blog/2018/12/16/web-scraping-of-machine-learning-mastery-blog-entries-using-python-take-1/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: Dr. Jason Brownlee’s Machine Learning Mastery hosts its tutorial lessons at https://machinelearningmastery.com/blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Machine Learning Mastery’s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.<br>Starting URLs: https://machinelearningmastery.com/blog<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using Python Take 1","author_name":"David Lowe","blog_date_text":"Fri, 14 Dec 2018 13:07:49 +0000","blog_url":"https://dainesanalytics.blog/2018/12/14/binary-classification-model-for-caravan-insurance-marketing-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.<br>The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.<br>The insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?<br>ANALYSIS: The baseline performance of the ten algorithms achieved an average F1_Micro score of 0.9260. Two algorithms, Logistic Regression and Support Vector Machine, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an F1_Micro score of 0.9402. After using the optimized tuning parameters, the Support Vector Machine algorithm processed the validation dataset with an F1_Micro score of 0.9405, which was slightly better than using the training data.<br>CONCLUSION: For this iteration, the Support Vector Machine algorithm achieved the leading F1_Micro scores using the training and validation datasets. For this dataset, Support Vector Machine should be considered for further modeling or production use.<br>Dataset Used: Insurance Company Benchmark (COIL 2000) Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+(COIL+2000)<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/caravan-insurance-challenge<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date_text":"Wed, 12 Dec 2018 13:37:02 +0000","blog_url":"https://dainesanalytics.blog/2018/12/12/updated-machine-learning-templates-for-r-3/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br>Version 6 of the templates contain minor adjustments and corrections to the prevision version of the templates.<br>The new templates also added an email function that sends out email messages. The email message can aid the monitoring of the script, especially for those scripts with large datasets and long processing time.<br>You will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date_text":"Mon, 10 Dec 2018 13:35:27 +0000","blog_url":"https://dainesanalytics.blog/2018/12/10/updated-machine-learning-templates-for-python-3/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br>Version 6 of the templates contain minor adjustments and corrections to the prevision version of the templates.<br>The new templates also added an email function that sends out email messages. The email message can aid the monitoring of the script, especially for those scripts with large datasets and long processing time.<br>You will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Daines Analytics Blog Entries using Python Take 2","author_name":"David Lowe","blog_date_text":"Sun, 09 Dec 2018 13:52:59 +0000","blog_url":"https://dainesanalytics.blog/2018/12/09/web-scraping-of-daines-analytics-blog-entries-using-python-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Daines Analytics’ RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries, not just the first ten as in Take1.<br>Starting URLs: https://dainesanalytics.blog/feed/<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 07 Dec 2018 13:06:26 +0000","blog_url":"https://dainesanalytics.blog/2018/12/07/regression-model-for-online-news-popularity-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average RMSE of 10409. Two algorithms (ElasticNet and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved the best RMSE of 10312. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 13007, which was worse than the accuracy of the training data and possibly due to over-fitting.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 10503. Two algorithms (Ridge, LASSO, and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 10320. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 13049, which was worse than the accuracy of the training data and possibly due to over-fitting.<br>From the model-building activities, the number of attributes went from 58 down to 48 after eliminating ten attributes. The processing time went from 21 hours 7 minutes in iteration Take1 down to 14 hours 49 minutes in iteration Take3, which was a reduction of 29% from Take1. The processing time, however, was a noticeable increase from Take2, which processed the dataset in 11 hours 41 minutes.<br>CONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 05 Dec 2018 13:57:20 +0000","blog_url":"https://dainesanalytics.blog/2018/12/05/regression-model-for-online-news-popularity-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average RMSE of 13128. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11358. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12146, which was slightly worse than the accuracy of the training data.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 14468. Two algorithms (ElasticNet and Support Vector Machine) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11294. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12094, which was slightly worse than the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 58 down to 30 after eliminating 28 attributes. The processing time went from 15 minutes 1 second in iteration Take1 up to 58 minutes 16 seconds in iteration Take2, which was due to the additional time required for tuning the Support Vector Machine algorithm. It also was a significant increase in comparison to Take2, which had a processing time of 17 minutes 37 seconds.<br>CONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the ElasticNet algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using R Take 2","author_name":"David Lowe","blog_date_text":"Mon, 03 Dec 2018 13:41:25 +0000","blog_url":"https://dainesanalytics.blog/2018/12/03/regression-model-for-online-news-popularity-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 10409. Two algorithms (ElasticNet and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved the best RMSE of 10312. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 13007, which was worse than the accuracy of the training data and possibly due to over-fitting.<br>From the model-building activities, the number of attributes went from 58 down to 35 after eliminating 23 attributes. The processing time went from 21 hours 7 minutes in iteration Take1 down to 11 hours 41 minutes in iteration Take2, which was a reduction of 44% from Take1.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this iteration, the Stochastic Gradient Boosting algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, Stochastic Gradient Boosting should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 4","author_name":"David Lowe","blog_date_text":"Sun, 02 Dec 2018 13:04:59 +0000","blog_url":"https://dainesanalytics.blog/2018/12/02/web-scraping-of-quotes-from-famous-people-using-r-take-4/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take4 iteration, the R script attempts to execute the login form and scrape the Goodreads links off each quote. The Goodreads links appear only after a successful authentication.<br>Starting URLs: http://quotes.toscrape.com/login<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using Python Take 2","author_name":"David Lowe","blog_date_text":"Fri, 30 Nov 2018 13:39:46 +0000","blog_url":"https://dainesanalytics.blog/2018/11/30/regression-model-for-online-news-popularity-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with the Lasso algorithm. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 13128. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11358. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12146, which was slightly worse than the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 58 down to 30 after eliminating 28 attributes. The processing time went from 15 minutes 1 second in iteration Take1 up to 17 minutes 37 seconds in iteration Take2, which was due to the additional time required for the feature selection processing.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and yet still retained a comparable level of accuracy. For this dataset, ElasticNet should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using R Take 1","author_name":"David Lowe","blog_date_text":"Wed, 28 Nov 2018 13:10:15 +0000","blog_url":"https://dainesanalytics.blog/2018/11/28/regression-model-for-online-news-popularity-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>ANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, Random Forest should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Online News Popularity Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 26 Nov 2018 13:15:11 +0000","blog_url":"https://dainesanalytics.blog/2018/11/26/regression-model-for-online-news-popularity-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>ANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.<br>CONCLUSION: For this iteration, the ElasticNet algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, ElasticNet should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 3","author_name":"David Lowe","blog_date_text":"Sat, 24 Nov 2018 21:31:46 +0000","blog_url":"https://dainesanalytics.blog/2018/11/24/web-scraping-of-quotes-from-famous-people-using-r-take-3/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take3 iteration, the R script attempts to scrape the quote information that is displayed via an infinite scrolling page.<br>Starting URLs: http://quotes.toscrape.com/scroll<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 3","author_name":"David Lowe","blog_date_text":"Fri, 23 Nov 2018 13:13:45 +0000","blog_url":"https://dainesanalytics.blog/2018/11/23/binary-classification-model-for-online-news-popularity-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.<br>From the previous iteration Take2, the baseline performance of the algorithms achieved an average accuracy of 60.60%. Two ensemble algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.34%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.70%, which was just slightly below the accuracy of the training data.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 61.08%. Two algorithms (Support Vector Machine and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 65.25%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 64.17%, which was just slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 58 down to 44 after eliminating 14 attributes. The processing time went from 5 hours 56 minutes in iteration Take1 down to 1 hour 34 minutes in iteration Take3, which was a reduction of 73% from Take1. It also was a slight decrease in comparison to Take2, which reduced the processing time down to 1 hour 59 minutes.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and either the dimensionality reduction techniques should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 3","author_name":"David Lowe","blog_date_text":"Wed, 21 Nov 2018 13:15:09 +0000","blog_url":"https://dainesanalytics.blog/2018/11/21/binary-classification-model-for-online-news-popularity-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.<br>From the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.29%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.51%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.53%, which was just slightly below the accuracy of the training data.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.22%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.41%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.69%, which was just slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes was kept at 58 after the RFE processing. The processing time went from 6 hours 31 minutes in iteration Take1 up to 11 hours 17 minutes in iteration Take3, which was an increase of 87% from Take1.<br>CONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 19 Nov 2018 13:33:40 +0000","blog_url":"https://dainesanalytics.blog/2018/11/19/binary-classification-model-for-online-news-popularity-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.<br>In the current iteration, the baseline performance of the algorithms achieved an average accuracy of 60.60%. Two ensemble algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.34%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.70%, which was just slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 58 down to 44 after eliminating 14 attributes. The processing time went from 5 hours 56 minutes in iteration Take1 down to 1 hour 59 minutes in iteration Take2, which was a reduction of 66% from Take1.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 2","author_name":"David Lowe","blog_date_text":"Sun, 18 Nov 2018 13:24:16 +0000","blog_url":"https://dainesanalytics.blog/2018/11/18/web-scraping-of-quotes-from-famous-people-using-r-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take2 iteration, the R script attempts to follow the links to the author page and scrape the author information.<br>Starting URLs: http://quotes.toscrape.com/<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 2","author_name":"David Lowe","blog_date_text":"Fri, 16 Nov 2018 13:13:36 +0000","blog_url":"https://dainesanalytics.blog/2018/11/16/binary-classification-model-for-online-news-popularity-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.<br>In the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.29%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.51%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.53%, which was just slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 58 down to 42 after eliminating 16 attributes. The processing time went from 6 hours 31 minutes in iteration Take1 down to 3 hours 18 minutes in iteration Take2, which was a reduction of 49% from Take1.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 1","author_name":"David Lowe","blog_date_text":"Wed, 14 Nov 2018 13:38:41 +0000","blog_url":"https://dainesanalytics.blog/2018/11/14/binary-classification-model-for-online-news-popularity-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>ANALYSIS: The baseline performance of the ten algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the top-tier training and validation results. For this dataset, Stochastic Gradient Boosting should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 12 Nov 2018 13:37:10 +0000","blog_url":"https://dainesanalytics.blog/2018/11/12/binary-classification-model-for-online-news-popularity-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article’s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content be publicly accessed and retrieved using the provided URLs.<br>Many thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.<br>ANALYSIS: The baseline performance of the eight algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.<br>CONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the top-tier training and validation results. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: Online News Popularity Dataset<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 1","author_name":"David Lowe","blog_date_text":"Sun, 11 Nov 2018 14:18:05 +0000","blog_url":"https://dainesanalytics.blog/2018/11/11/web-scraping-of-quotes-from-famous-people-using-r-take-1/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the R script attempts to follow the page links and scrape the quote information off each page.<br>Starting URLs: http://quotes.toscrape.com/<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 09 Nov 2018 14:21:37 +0000","blog_url":"https://dainesanalytics.blog/2018/11/09/binary-classification-model-for-truck-aps-failure-detection-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will impute the blank cells with the value zero.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using a dimensionality reduction technique to reduce the processing time while still maintaining an adequate level of prediction accuracy. The technique was to eliminate collinear attributes based on a threshold of 75%.<br>For this iteration, we will explore the Recursive Feature Elimination (RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 50.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.<br>From the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 98.99%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.40%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.19%, which was slightly below the accuracy of the training data.<br>In the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.94%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.31%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.09%, which was slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 170 down to 25 after eliminating 145 attributes. The processing time went from 63 hours 22 minutes in iteration Take 1 down to 9 hours 33 minutes in iteration Take3, which was a reduction of 84% from Take1. That was also a noticeable reduction in comparison to Take2, which reduced the processing time down to 39 hours 52 minutes.<br>CONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained an acceptable level of accuracy. For this dataset, the Random Forest algorithm and the Recursive Feature Elimination (RFE) technique should be considered for further modeling or production use.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 07 Nov 2018 14:16:10 +0000","blog_url":"https://dainesanalytics.blog/2018/11/07/binary-classification-model-for-truck-aps-failure-detection-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will impute the blank cells with the value zero.<br>In iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For this iteration, we will examine the feasibility of using a dimensionality reduction technique to reduce the processing time while still maintaining an adequate level of prediction accuracy. The technique is to eliminate collinear attributes based on a threshold of 75%.<br>ANALYSIS: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.<br>In the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.99%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.40%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.19%, which was slightly below the accuracy of the training data.<br>From the model-building activities, the number of attributes went from 170 down to 100 after eliminating 70 attributes. The processing time went from 63 hours 22 minutes in iteration Take1 down to 39 hours 52 minutes in iteration Take2, which was a reduction of 37% from Take1.<br>CONCLUSION: The feature selection technique helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained an almost-identical level of accuracy. For this dataset, the Random Forest algorithm and the technique of eliminating collinear attributes should be considered for further modeling or production use.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 05 Nov 2018 14:40:30 +0000","blog_url":"https://dainesanalytics.blog/2018/11/05/binary-classification-model-for-truck-aps-failure-detection-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of baseline results by imputing the blank cells with the value zero.<br>ANALYSIS: The baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.<br>CONCLUSION: For this iteration, the Random Forest algorithm achieved the top-tier training and validation results. For this dataset, the Random Forest algorithm should be considered for further modeling or production use.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of ScrapingHub Blog Entries using R","author_name":"David Lowe","blog_date_text":"Sun, 04 Nov 2018 14:38:16 +0000","blog_url":"https://dainesanalytics.blog/2018/11/04/web-scraping-of-scrapinghub-blog-entries-using-r/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: ScrapingHub, the maker of the Scrapy framework, hosts its blog at blog.scrapinghub.com. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Scrapinghub. The script also would automatically traverse from one page of the blog entries to the next page.<br>Starting URLs: https://blog.scrapinghub.com/<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date_text":"Fri, 02 Nov 2018 12:07:41 +0000","blog_url":"https://dainesanalytics.blog/2018/11/02/updated-machine-learning-templates-for-python-2/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br>Version 5 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.<br>The new templates also standardized the dataframes used in the script as follow:<br>originalDataset: This dataframe contains the original data imported from the data source.<br>xy_train: Training dataframe that has the attributes and the target/class variable.<br>x_train: Training dataframe that has the attributes only.<br>y_train: Training dataframe that has the target/class variable only.<br>xy_test: Test dataframe that has the attributes and the target/class variable.<br>x_test: Test dataframe that has the attributes only.<br>y_test: Test dataframe that has the target/class variable only.<br>You will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date_text":"Wed, 31 Oct 2018 12:06:32 +0000","blog_url":"https://dainesanalytics.blog/2018/10/31/updated-machine-learning-templates-for-r-2/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br>Version 5 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.<br>The new templates also standardized the dataframes used in the script as follow:<br>originalDataset: This dataframe contains the original data imported from the data source.<br>xy_train: Training dataframe that has the attributes and the target/class variable.<br>x_train: Training dataframe that has the attributes only.<br>y_train: Training dataframe that has the target/class variable only.<br>xy_test: Test dataframe that has the attributes and the target/class variable.<br>x_test: Test dataframe that has the attributes only.<br>y_test: Test dataframe that has the target/class variable only.<br>You will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 4","author_name":"David Lowe","blog_date_text":"Mon, 29 Oct 2018 12:14:50 +0000","blog_url":"https://dainesanalytics.blog/2018/10/29/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we explored was to eliminate collinear attributes based on a threshold of 85%.<br>In iteration Take3, we explored the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to cumulative importance of 0.99.<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 50.<br>CONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.<br>From the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%, which was slightly worse than the accuracy from the training data and possibly due to over-fitting.<br>From the previous iteration Take3, the baseline performance of the ten algorithms achieved an average accuracy of 91.59%. the Random Forest and Stochastic Gradient Boosting algorithms achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.74%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.42%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 90.62%. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 97.75%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 87.21%. The accuracy on the validation dataset was noticeably worse than the training data and possibly due to over-fitting.<br>From the model-building activities, the number of attributes went from 561 down to 41 after eliminating 520 variables. The processing time went from 8 hours 16 minutes in iteration Take1 down to 2 hours and 25 minutes in iteration Take4. That was a noticeable reduction in comparison to Take2, which reduced the processing time down to 7 hours 15 minutes. It also was a noticeable reduction in comparison to Take3, which reduced the processing time down to 5 hours 22 minutes.<br>In conclusion, the attribute importance ranking technique helped by cutting down the attributes and reduce the training time. Furthermore, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm with attribute importance ranking should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 4","author_name":"David Lowe","blog_date_text":"Sun, 28 Oct 2018 12:12:18 +0000","blog_url":"https://dainesanalytics.blog/2018/10/28/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance in terms of accuracy and processing time.<br>In iteration Take2, we examined the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.<br>In iteration Take3, we explored the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to cumulative importance of 0.99.<br>For this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time managable, we will limit the number of attributes to 50.<br>CONCLUSION: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.<br>From the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.<br>From the previous iteration Take3, the baseline performance of the ten algorithms achieved an average accuracy of 85.49%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.52%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.06%, which was slightly better than the accuracy from the training data.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 86.76%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 95.83%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 94.19%, which was slightly below the accuracy from the training data.<br>From the model-building activities, the number of attributes went from 561 down to 50 after eliminating 511 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 1 hours and 16 minutes in iteration Take4. That was a minor reduction in comparison to Take2, which reduced the processing time down to 2 hours 7 minutes. It also was a noticeable reduction in comparison to Take3, which reduced the processing time down to 8 hours and 9 minutes.<br>In conclusion, the importance ranking technique should have benefited the tree methods the most, but the Linear Discriminant Analysis algorithm held its own for this modeling iteration. Furthermore, by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Linear Discriminant Analysis and Support Vector Machine algorithms should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 26 Oct 2018 12:07:50 +0000","blog_url":"https://dainesanalytics.blog/2018/10/26/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time.<br>For iteration Take2, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we explored was to eliminate collinear attributes based on a threshold of 85%.<br>For this iteration, we will explore the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Next, we eliminate the features which do not contribute to the cumulative importance of 0.99.<br>CONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.<br>From the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 91.59%. The Random Forest and Stochastic Gradient Boosting algorithms achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.74%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.42%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.<br>From the model-building activities, the number of attributes went from 561 down to 79 after eliminating 482 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 5 hours and 22 minutes in iteration Take3. That was also a noticeable reduction in comparison to Take2, which reduced the processing time down to 7 hours 15 minutes.<br>In conclusion, the importance ranking technique should have benefited the tree methods the most, and it did. Furthermore, by reducing the number of attributes, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 24 Oct 2018 12:38:52 +0000","blog_url":"https://dainesanalytics.blog/2018/10/24/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time. For this iteration, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.<br>CONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%, which was slightly worse than the accuracy from the training data and possibly due to over-fitting.<br>From the model-building activities, the number of attributes went from 561 down to 192 after eliminating 369 variables that are at least 85% collinear. The processing time went from 21 hours 43 minutes in iteration Take1 down to 7 hours and 15 minutes in iteration Take2. That was a reduction in model training and processing time of 66%.<br>In conclusion, the reduction in the number of attributes used still achieved an acceptable level of accuracy. by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 22 Oct 2018 12:00:52 +0000","blog_url":"https://dainesanalytics.blog/2018/10/22/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>For this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.<br>From the model-building activities, the Stochastic Gradient Boosting ensemble algorithm achieved the top-notch training and validation results. For the project, Stochastic Gradient Boosting should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 3","author_name":"David Lowe","blog_date_text":"Fri, 19 Oct 2018 12:11:23 +0000","blog_url":"https://dainesanalytics.blog/2018/10/19/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance in terms of accuracy and processing time.<br>In iteration Take2, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.<br>For this iteration, we will explore the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Next, we eliminate the features which do not contribute to cumulative importance of 0.99.<br>CONCLUSION: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.<br>From the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 85.49%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.52%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.06%, which was slightly better than the accuracy from the training data.<br>From the model-building activities, the number of attributes went from 561 down to 486 after eliminating 75 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 8 hours and 9 minutes in iteration Take3. That was a minor reduction in comparison to Take2, which reduced the processing time down to 2 hours 7 minutes.<br>In conclusion, the importance ranking technique should have benefited the tree methods the most, but the Linear Discriminant Analysis algorithm held its own for this modeling iteration. Furthermore, by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Linear Discriminant Analysis and Support Vector Machine algorithms should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 2","author_name":"David Lowe","blog_date_text":"Wed, 17 Oct 2018 12:36:11 +0000","blog_url":"https://dainesanalytics.blog/2018/10/17/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.<br>In iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time. For this iteration, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.<br>CONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.<br>From the model-building activities, the number of attributes went from 561 down to 172 after eliminating 389 variables that are at least 85% collinear. The processing time went from 8 hours 16 minutes in iteration Take1 down to 2 hours and 7 minutes in iteration Take2. That was a reduction in model training and processing time of 74%.<br>In conclusion, the reduction in the number of attributes used still achieved an acceptable level of accuracy. Furthermore, the Support Vector Machine algorithm achieved the top-notch training and validation results. For the project, Support Vector Machine should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 15 Oct 2018 12:12:22 +0000","blog_url":"https://dainesanalytics.blog/2018/10/15/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.<br>INTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data.<br>For this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.<br>From the model-building activities, the Linear Discriminant Analysis algorithm achieved the top-notch training and validation results. For the project, Linear Discriminant Analysis should be considered for further modeling or production use.<br>Dataset Used: Human Activity Recognition Using Smartphone Data Set<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Registry of Open Data on AWS Using R","author_name":"David Lowe","blog_date_text":"Fri, 12 Oct 2018 12:29:20 +0000","blog_url":"https://dainesanalytics.blog/2018/10/12/web-scraping-of-registry-of-open-data-on-aws-using-r/","blog_text":"SUMMARY: The purpose of this project is to gather data about the open datasets on AWS. The web scraping code was written in R and leveraged the rvest package.<br>INTRODUCTION: The Open Data registry exists to help people discover and share datasets that are available via AWS resources. This page lists all usage examples for datasets listed in the registry.<br>Starting URLs: https://registry.opendata.aws/<br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Daines Analytics Blog Entries using Python Take 1","author_name":"David Lowe","blog_date_text":"Wed, 10 Oct 2018 12:23:15 +0000","blog_url":"https://dainesanalytics.blog/2018/10/10/4487/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Daines Analytics’ RSS feed. This iteration of the script can capture only the most recent ten blog entries. A future iteration of the script would automatically traverse the RSS feed to capture all blog entries, not just the first ten.<br>Starting URLs: https://dainesanalytics.blog/feed/<br><br>import scrapy<br><br>class DainesBlogRSSSpider(scrapy.Spider):<br>    name = 'dainesblogrss'<br>    allowed_domains = ['dainesanalytics.blog/feed/']<br>    start_urls = ['https://dainesanalytics.blog/feed/']<br><br>    # Setting up for the JSON output file<br>    custom_settings = {<br>        'FEED_URI' : 'dainesblogrss.json'<br>    }<br><br>    def parse(self, response):<br>        self.log('I just visited: ' + response.url)<br><br>        # Remove the XML namespaces<br>        response.selector.remove_namespaces()<br><br>        # Extract article information<br>        titles = response.xpath('//item/title/text()').extract()<br>        authors = response.xpath('//item/creator/text()').extract()<br>        dates = response.xpath('//item/pubDate/text()').extract()<br>        links = response.xpath('//item/link/text()').extract()<br>        description = response.xpath('//item/description/text()').extract()<br><br>        for item in zip(titles, authors, dates, links, description):<br>            scraped_info = {<br>                'Title' : item[0],<br>                'Author' : item[1],<br>                'Publish_Date' : item[2],<br>                'Link' : item[3],<br>                'Description' : item[4]<br>            }<br>            yield scraped_info<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of ScrapingHub Blog Entries using Python","author_name":"David Lowe","blog_date_text":"Mon, 08 Oct 2018 12:54:55 +0000","blog_url":"https://dainesanalytics.blog/2018/10/08/web-scraping-of-scrapinghub-blog-entries-using-python/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: ScrapingHub, the maker of the Scrapy framework, hosts its blog at blog.scrapinghub.com. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Scrapinghub. The script also would automatically traverse from one page of the blog entries to the next page.<br>Starting URLs: https://blog.scrapinghub.com/<br><br>import scrapy<br><br>class ScrapinghubSpider(scrapy.Spider):<br>    name = 'scrapinghub'<br>    allowed_domains = ['scrapinghub.com']<br>    start_urls = ['https://blog.scrapinghub.com/']<br><br>    def parse(self, response):<br>        self.log('I just visited: ' + response.url)<br>        for blog in response.css('div.post-item'):<br>            item = {<br>                'blog_title': blog.css('div.post-header > h2 > a::text').extract_first(),<br>                'blog_url': blog.css('div.post-header > h2 > a::attr(href)').extract_first(),<br>                'date': blog.css('div.post-header > div.byline > span.date > a::text').extract_first(),<br>                'author': blog.css('div.post-header > div.byline > span.author > a::text').extract_first(),<br>                'summary': blog.css('div.post-content > p::text').extract_first(),<br>            }<br>            yield item<br><br>        # follow pagination link<br>        next_page_url = response.css('div.blog-pagination > a.next-posts-link').xpath('@href').extract_first()<br>        if next_page_url:<br>            self.log('Moving on to next page: ' + next_page_url)<br>            yield scrapy.Request(url=next_page_url, callback=self.parse)<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 4","author_name":"David Lowe","blog_date_text":"Fri, 05 Oct 2018 12:27:47 +0000","blog_url":"https://dainesanalytics.blog/2018/10/05/web-scraping-of-quotes-from-famous-people-using-python-take-4/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take4 iteration, the Python script attempts to execute the login form and scrape the Goodreads links off each quote. The Goodreads links appear only after a successful authentication.<br>Starting URLs: http://quotes.toscrape.com/login<br><br>import scrapy<br><br>class LoginSpider(scrapy.Spider):<br>    name = \"login\"<br>    login_url = 'http://quotes.toscrape.com/login'<br>    start_urls = [login_url]<br><br>    def parse(self, response):<br>        # Extract the CSRF token value<br>        token = response.css('input[name=\"csrf_token\"]::attr(value)').extract_first()<br>        # Create a Python dictionary with the form values<br>        data = {<br>            'csrf_token' : token,<br>            'username' : 'abc',<br>            'password' : 'abc',<br>        }<br>        # Submit a Post request to login<br>        yield scrapy.FormRequest(url=self.login_url, formdata=data, callback=self.parse_quotes)<br><br>    def parse_quotes(self, response):<br>        # Parse the items on page after login<br>        self.log('I just visited: ' + response.url)<br>        for quote in response.css('div.quote'):<br>            item = {<br>                'author_name': quote.css('small.author::text').extract_first(),<br>                'goodreads_url': quote.css('small.author ~ a[href*=\"goodreads.com\"]::attr(href)').extract_first(),<br>            }<br>            yield item<br><br>        # follow pagination link<br>        next_page_url = response.css('li.next > a::attr(href)').extract_first()<br>        if next_page_url:<br>            next_page_url = response.urljoin(next_page_url)<br>            self.log('Moving on to: ' + next_page_url)<br>            yield scrapy.Request(url=next_page_url, callback=self.parse_quotes)<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 03 Oct 2018 12:29:07 +0000","blog_url":"https://dainesanalytics.blog/2018/10/03/web-scraping-of-quotes-from-famous-people-using-python-take-3/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework [https://scrapy.org/] maintained by Scrapinghub [https://scrapinghub.com/].<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take3 iteration, the Python script attempts to scrape the quote information that is displayed via an infinite scrolling page.<br>Starting URLs: http://quotes.toscrape.com/scroll<br><br>import json<br>import scrapy<br><br>class ScrollSpider(scrapy.Spider):<br>    name = \"scroll\"<br>    api_url = 'http://quotes.toscrape.com/api/quotes?page={}'<br>    start_urls = [api_url.format(1)]<br><br>    def parse(self, response):<br>        data = json.loads(response.text)<br>        for quote in data['quotes']:<br>            yield {<br>                'author_name': quote['author']['name'],<br>                'text': quote['text'],<br>                'tags': quote['tags'],<br>                'author_url': quote['author']['goodreads_link'],<br>            }<br><br>        # follow pagination link<br>        if data['has_next']:<br>            next_page = data['page'] + 1<br>            yield scrapy.Request(url=self.api_url.format(next_page), callback=self.parse)<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 01 Oct 2018 12:46:42 +0000","blog_url":"https://dainesanalytics.blog/2018/10/01/web-scraping-of-quotes-from-famous-people-using-python-take-2/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take2 iteration, the Python script attempts to follow the links to the author page and scrape the author information.<br>Starting URLs: http://quotes.toscrape.com/<br><br>import scrapy<br><br>class AuthorSpider(scrapy.Spider):<br>    name = 'authors'<br>    start_urls = ['http://quotes.toscrape.com/']<br><br>    def parse(self, response):<br>        # follow links to author pages<br>        for href in response.css('.author + a::attr(href)'):<br>            yield response.follow(href, self.parse_author)<br><br>        # follow pagination links<br>        for href in response.css('li.next a::attr(href)'):<br>            yield response.follow(href, self.parse)<br><br>    def parse_author(self, response):<br>        def extract_with_css(query):<br>            return response.css(query).extract_first().strip()<br><br>        yield {<br>            'name': extract_with_css('h3.author-title::text'),<br>            'birthdate': extract_with_css('.author-born-date::text'),<br>            'bio': extract_with_css('.author-description::text'),<br>        }<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 1","author_name":"David Lowe","blog_date_text":"Fri, 28 Sep 2018 12:20:58 +0000","blog_url":"https://dainesanalytics.blog/2018/09/28/web-scraping-of-quotes-from-famous-people-using-python-take-1/","blog_text":"SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the Python script attempts to follow the page links and scrape the quote information off each page.<br>Starting URLs: http://quotes.toscrape.com/<br><br>import scrapy<br>class QuotesSpider(scrapy.Spider):<br>    name = \"quotes\"<br>    start_urls = ['http://quotes.toscrape.com/']<br>    def parse(self, response):<br>        self.log('I just visited: ' + response.url)<br>        for quote in response.css('div.quote'):<br>            item = {<br>                'author_name': quote.css('small.author::text').extract_first(),<br>                'text': quote.css('span.text::text').extract_first(),<br>                'tags': quote.css('div.tags a.tag::text').extract(),<br>                'author_url': quote.css('div.quote > span > a::attr(href)').extract_first(),<br>            }<br>            yield item<br>        # follow pagination link<br>        next_page_url = response.css('li.next > a::attr(href)').extract_first()<br>        if next_page_url:<br>            next_page_url = response.urljoin(next_page_url)<br>            yield scrapy.Request(url=next_page_url, callback=self.parse)<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using Python Take 2","author_name":"David Lowe","blog_date_text":"Wed, 26 Sep 2018 12:45:18 +0000","blog_url":"https://dainesanalytics.blog/2018/09/26/binary-class-classification-model-for-german-credit-risks-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.<br>Because the case study also stipulated that it is worse to classify a customer as good when they are bad (weight of 5), than it is to classify a customer as bad when they are good (weight of 1). For this iteration, the script focuses on tuning various machine learning algorithms and identify the algorithm that can produce the best cost-and-accuracy tradeoffs.<br>CONCLUSION: From the previous iteration Take 1, the baseline performance of the eight algorithms achieved an average accuracy of 71.80%. Three algorithms (Logistic Regression, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 76.14%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.66%, which was slightly better than the accuracy from the training data.<br>From the cost vs accuracy comparison, both the Logistic Regression and Stochastic Gradient Boosting achieved high accuracy while keeping the costs of incorrect predictions low. Either algorithm should be considered for further modeling or production use.<br>Dataset Used: German Credit Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/german-credit/home<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 24 Sep 2018 12:02:53 +0000","blog_url":"https://dainesanalytics.blog/2018/09/24/binary-class-classification-model-for-german-credit-risks-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br><br><br><br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br><br><br><br>INTRODUCTION: This dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.<br><br><br><br>For this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.<br><br><br><br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 71.80%. Three algorithms (Logistic Regression, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 76.14%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.66%, which was slightly better than the accuracy from the training data.<br><br><br><br>From the model-building activities, the Stochastic Gradient Boosting ensemble algorithm yielded the top-notch training and validation results. It is the recommended algorithm to use from the accuracy perspective.<br><br><br><br>Dataset Used: German Credit Data Set<br><br><br><br>Dataset ML Model: Binary classification with numerical and categorical attributes<br><br><br><br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29<br><br><br><br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/german-credit/home<br><br><br><br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Web Scraping of Registry of Open Data on AWS Using Python","author_name":"David Lowe","blog_date_text":"Fri, 21 Sep 2018 12:58:09 +0000","blog_url":"https://dainesanalytics.blog/2018/09/21/web-scraping-of-registry-of-open-data-on-aws/","blog_text":"SUMMARY: The purpose of this project is to gather data about the open datasets on AWS. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.<br>INTRODUCTION: The Open Data registry exists to help people discover and share datasets that are available via AWS resources. This page lists all usage examples for datasets listed in the registry.<br>Starting URLs: https://registry.opendata.aws/<br><br>import scrapy<br>class ListdatasetsSpider(scrapy.Spider):<br>    name = 'listdatasets'<br>    start_urls = ['https://registry.opendata.aws/']<br><br>    def parse(self, response):<br>        for dataset in response.css('div.dataset'):<br>            item = {<br>                'dataset_name': dataset.css('h3 > a::text').extract_first(),<br>                'detail_url': response.urljoin(dataset.css('h3 > a::attr(href)').extract_first()),<br>                'tags': dataset.css('p > span::text').extract(),<br>                'description': dataset.css('p')[1].extract(),<br>            }<br>            yield item<br><br>The source code and JSON output can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 19 Sep 2018 12:49:49 +0000","blog_url":"https://dainesanalytics.blog/2018/09/19/binary-class-classification-model-for-german-credit-risks-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.<br>Because the case study also stipulated that it is worse to classify a customer as good when they are bad (weight of 5), than it is to classify a customer as bad when they are good (weight of 1). For this iteration, the script focuses on tuning various machine learning algorithms and identify the algorithm that can produce the best cost-and-accuracy tradeoffs.<br>CONCLUSION: From the previous iteration Take 1, The baseline performance of the eight algorithms achieved an average accuracy of 72.69%. Three algorithms (Logistic Regression, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 75.00%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 75.67%, which was slightly better than the accuracy from the training data.<br>From the cost vs accuracy comparison, both the Logistic Regression and AdaBoost achieved identical accuracy while keeping the costs of incorrect predictions low. Either algorithm should be considered for further modeling or production use.<br>Dataset Used: German Credit Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/german-credit/home<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 17 Sep 2018 12:34:19 +0000","blog_url":"https://dainesanalytics.blog/2018/09/17/binary-class-classification-model-for-german-credit-risks-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.<br>For this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 72.69%. Three algorithms (Logistic Regression, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 75.00%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 75.67%, which was slightly better than the accuracy from the training data.<br>From the model-building activities, the Stochastic Gradient Boosting ensemble algorithm yielded the top-notch training and validation results. It is the recommended algorithm to use from the accuracy perspective.<br>Dataset Used: German Credit Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/german-credit/home<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date_text":"Fri, 14 Sep 2018 12:03:15 +0000","blog_url":"https://dainesanalytics.blog/2018/09/14/updated-machine-learning-templates-for-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.<br>Version 4 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.<br>The new templates also consolidated and moved the parameters used script-wide to a new section of its own (1.c), rather having them spread out all over the script.<br>You will find the Python templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date_text":"Wed, 12 Sep 2018 12:00:35 +0000","blog_url":"https://dainesanalytics.blog/2018/09/12/updated-machine-learning-templates-for-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, [https://machinelearningmastery.com/machine-learning-project-template-in-r/] I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.<br>Version 4 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.<br>The new templates also consolidated and moved the parameters used script-wide to a new section of its own (1.c), rather having them spread out all over the script.<br>You will find the R templates from the Machine Learning Project Templates page.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 3 Using Python","author_name":"David Lowe","blog_date_text":"Mon, 10 Sep 2018 12:33:36 +0000","blog_url":"https://dainesanalytics.blog/2018/09/10/binary-class-classification-model-for-seismic-bumps-take-3-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>In iterations Take1 and Take2, we had three algorithms with high accuracy and ROC results but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.<br>CONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset. With an imbalanced dataset we have on-hand, we needed to look for another metric or another approach to evaluate the models.<br>From the previous Take2 iteration, the baseline performance of the eight algorithms achieved an average ROC score of 66.16%. Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 77.98%, but with a dismal precision and recall scores.<br>From the current iteration, the baseline performance of the ten algorithms achieved an average accuracy score of 75.83%. Three algorithms (Random Forest, Extra Trees, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the best accuracy result of 96.38%, but with still-very-low precision and recall scores.<br>The ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. Also, the SMOTE technique helped to make the model evaluation more realistic with the imbalanced dataset we have. For this project however, the modeling choice was inconclusive due to the data imbalance issue that still requires a resolution.<br>Dataset Used: Seismic Bumps Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/seismic-bumps<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 2 Using Python","author_name":"David Lowe","blog_date_text":"Fri, 07 Sep 2018 12:13:37 +0000","blog_url":"https://dainesanalytics.blog/2018/09/07/binary-class-classification-model-for-seismic-bumps-take-2-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>In iteration Take1, we had three algorithms with high accuracy results but with dismal precision and recall scores. For this iteration, we will examine the viability of using the ROC scores to rank and choose the models.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset. With an imbalanced dataset we have on-hand, we needed to look for another metric or another approach to evaluate the models.<br>From the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 66.16%. Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 77.98%, but with a dismal precision and recall scores.<br>The ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. However, with an imbalanced dataset we have on-hand, we still need to look for another approach to further validate our modeling effort.<br>Dataset Used: Seismic Bumps Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/seismic-bumps<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 1 Using Python","author_name":"David Lowe","blog_date_text":"Wed, 05 Sep 2018 12:33:38 +0000","blog_url":"https://dainesanalytics.blog/2018/09/05/binary-class-classification-model-for-seismic-bumps-take-1-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Seismic Bumps Data Set is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset.<br>With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.<br>Dataset Used: Seismic Bumps Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/seismic-bumps<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 3 Using R","author_name":"David Lowe","blog_date_text":"Mon, 03 Sep 2018 12:13:43 +0000","blog_url":"https://dainesanalytics.blog/2018/09/03/binary-class-classification-model-for-seismic-bumps-take-3-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>In iterations Take1 and Take2, we had three algorithms with high accuracy and ROC results but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.<br>CONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0. With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.<br>From the previous Take2 iteration, the baseline performance of the eight algorithms achieved an average ROC score of 71.99%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 78.59%, but with a dismal sensitivity score of 0.88%.<br>From the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 87.33%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the best ROC result of 92.68%, but with a much-better sensitivity score of 70.87%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with a ROC of 84.57%, which was slightly below the ROC score of the training data.<br>The ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. Also, the SMOTE technique helped to make the model evaluation more realistic with the imbalanced dataset we have. For this project, the Random Forest appeared to be the most suitable algorithm for the dataset.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 2 Using R","author_name":"David Lowe","blog_date_text":"Fri, 31 Aug 2018 12:36:27 +0000","blog_url":"https://dainesanalytics.blog/2018/08/31/binary-class-classification-model-for-seismic-bumps-take-2-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>In iteration Take1, we had three algorithms with high accuracy results but with dismal Kappa scores. For this iteration, we will examine the viability of using the ROC scores to rank and choose the models.<br>CONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0. With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.<br>From the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 71.99%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 78.59, but with a dismal sensitivity score of 0.88%.<br>The ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. However, with an imbalanced dataset that we have on-hand, we still need to look for another approach to further validate our modeling effort.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 1 Using R","author_name":"David Lowe","blog_date_text":"Wed, 29 Aug 2018 12:16:23 +0000","blog_url":"https://dainesanalytics.blog/2018/08/29/binary-class-classification-model-for-seismic-bumps-take-1-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Seismic Bumps Data Set is a binary-class classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0.<br>With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.<br>Dataset Used: Seismic Bumps Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/seismic-bumps<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Letter Recognition Using Python","author_name":"David Lowe","blog_date_text":"Mon, 27 Aug 2018 12:39:40 +0000","blog_url":"https://dainesanalytics.blog/2018/08/27/multi-class-classification-model-for-letter-recognition-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Letter Recognition Data Set is a multi-class classification situation where we are trying to predict one of the several possible outcomes.<br>INTRODUCTION: The objective is to identify each of many black-and-white rectangular-pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 80.98%. Three algorithms (k-Nearest Neighbors, Support Vector Machine, and Extra Trees) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 97.37%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 97.46%, which was even slightly better the accuracy of the training data.<br>For this project, the Support Vector Machine algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Letter Recognition<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition<br>One potential source of performance benchmarks: https://www.kaggle.com/c/ci-letter-recognition<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Letter Recognition Using R","author_name":"David Lowe","blog_date_text":"Fri, 24 Aug 2018 12:44:26 +0000","blog_url":"https://dainesanalytics.blog/2018/08/24/multi-class-classification-model-for-letter-recognition-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Letter Recognition DataSet is a multi-class classification situation where we are trying to predict one of the several possible outcomes.<br>INTRODUCTION: The objective is to identify each of many black-and-white rectangular-pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.<br>CONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 79.30%. Three algorithms (Bagged CART, Random Forest, and k-Nearest Neighbors) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 96.32%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 96.45%, which was even slightly better the accuracy of the training data.<br>For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Letter Recognition<br>Dataset ML Model: Multi-class classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition<br>One potential source of performance benchmarks: https://www.kaggle.com/c/ci-letter-recognition<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 22 Aug 2018 12:45:20 +0000","blog_url":"https://dainesanalytics.blog/2018/08/22/binary-classification-model-for-truck-aps-failure-detection-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of results by imputing the blank cells with the value of -1. We will compare the results from Take 1 and Take 2, where we imputed the blank cells with the zero and the mean value.<br>CONCLUSION: From the Take1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.<br>From the Take2 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8348%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.<br>From the current iteration (Take3), the baseline performance of the ten algorithms achieved an average accuracy of 98.8003%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2312%, which was slightly below the accuracy of the training data.<br>For this iteration, imputing the missing cells with the -1 value improved the average performance of all models only slightly. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 20 Aug 2018 12:51:30 +0000","blog_url":"https://dainesanalytics.blog/2018/08/20/binary-classification-model-for-truck-aps-failure-detection-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The datasets’ positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of results by imputing the blank cells with the mean value. We will compare the results from Take 1, where we imputed the blank cells with the value zero.<br>CONCLUSION: From the Take1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.<br>From the current iteration (Take2), the baseline performance of the ten algorithms achieved an average accuracy of 98.8348%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.<br>For this iteration, imputing the missing cells with the mean value improved the average performance of all models slightly, but not so much for the Random Forest algorithm. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 1","author_name":"David Lowe","blog_date_text":"Fri, 17 Aug 2018 12:37:11 +0000","blog_url":"https://dainesanalytics.blog/2018/08/17/binary-classification-model-for-truck-aps-failure-detection-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset’s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.<br>This dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of baseline results by imputing the blank cells with the value zero.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.<br>For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: APS Failure at Scania Trucks Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Provisioning AWS Linux AMI-based EC2 for R Server","author_name":"David Lowe","blog_date_text":"Wed, 15 Aug 2018 12:22:47 +0000","blog_url":"https://dainesanalytics.blog/2018/08/15/provisioning-aws-linux-ami-based-ec2-for-r-server/","blog_text":"Below are my notes for provisioning an R server using AWS’ Linux 2 AMI. These notes captured my installation process during the week of 23 July 2018.<br>My goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do to get past those obstacles.<br>You can find the installation and configuration notes here on the website.<br>Abbreviations<br>AWS: Amazon Web Services<br>VPC: Virtual Private Cloud<br>EC2: Elastic Compute Cloud<br>IAM: Identity and Access Management<br>AMI: Amazon Machine Image<br>DLAMI-DG: Deep Learning AMI Developer Guide<br>Requirements<br>Needed to find workable configurations for modeling machine learning problems by exploring the use of AWS Ec2 instances. The source code language was in R and contained in an RMD script format.<br>The baseline performance was defined by running the Python script on a Dell Latitude E7450 with Intel i7-5600U CPU at 2.60GHz, 16GB RAM, and Windows 10. While this can be a decent configuration for ML modeling, occasionally we may need a system with larger memory capacity or more CPUs for the tasks at hand.<br>Background and Prerequisite Information<br>The following tools and assumptions were present prior to the provisioning of the cloud instance.<br>AWS Console with the necessary rights and configuration elements to launch an instance. I had configured a VPC subnet, an IAM role, a security group, and a key pair for setting up the instance.<br>AWS Deep Learning AMI Developer Guide, released June 6, 2018<br>Web browsers<br>PuTTY<br>AWS Configuration Notes<br>AMI: I performed the following steps using Amazon Linux 2 AMI with an m5.large general-purpose instance.<br>VPC: This exercise requires only a subnet that is accessible via the Internet.<br>Security Group: I configured the security group to allow only TCP ports 22 from any IP address because I had planned to use an SSH tunnel to access the R server.<br>IAM Role: I assign all my AWS instances to an IAM role by default. For this exercise, an IAM role is not critical.<br>Key Pair: I attached the instance to an existing key pair. The key pair is necessary to access the instance via the SSH protocol.<br> <br>Provision an instance with the Amazon Deep Learning AMI<br>Step 1) Create and launch the instance. I used an m5.large instance as the starting point.<br>Step 2) Install R base package (R3.4 as of this writing).<br>$ sudo amazon-linux-extra install R3.4<br>Step 3) Install R Server with the following commands. Check www.rstudio.org for the latest release of the server.<br>$ wget https://download2.rstudio.org/rstudio-server-rhel-1.1.456-x86_64.rpm<br>$ sudo yum install rstudio-server-rhel-1.1.456-x86_64.rpm<br>Step 4) Configure the client workstation to connect to the R server. I configured my Windows workstation to connect to the R server using an SSH tunnel. The DLAMI-DG document has a write-up on how to do this for Windows, Linux, and MacOS clients (pages 15-20).<br>See the PuTTY screenshot below for configuring an SSH tunnel.<br><br>Step 5) Install Git.<br>$ sudo yum install -y git<br>$ git clone https://github.com/daines-analytics/tabular-data-projects.git<br>Step 6) Add an user to access the R server.<br>$ sudo useradd rstudio<br>$ echo rstudio:rstudio | sudo chpasswd<br>Step 6) Start a browser on the workstation running the SSH tunnel and point to the URL http://localhost:8787. A login screen should appear.<br><br>Step 7) Go to the Terminal tab and run the “git clone” commands to copy my R scripts from GitHub to the cloud server. Locate the R script and run it.<br><br>There you have it! A working R server on an AWS cloud instance that you can access via a secured protocol. Now install your favorite packages and let the scripts run.<br>When compared to a client workstation, the right types of cloud instance can help our modeling effort. For anyone who will be attempting a similar installation, I hope these instructions can help in some way. My next step is to automate the instance creation with a CloudFormation script further. I will write down what I run into and share my findings later.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Provisioning AWS Deep Learning AMI-based EC2 for Jupyter","author_name":"David Lowe","blog_date_text":"Mon, 13 Aug 2018 12:13:15 +0000","blog_url":"https://dainesanalytics.blog/2018/08/13/provisioning-aws-deep-learning-ami-based-ec2-for-jupyter/","blog_text":"Below are my notes for provisioning a Jupyter server using AWS’ Deep Learning AMI. These notes captured my installation process during the week of 23 July 2018.<br>My goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do to get past those obstacles.<br>You can find the installation and configuration notes here on the website.<br>Abbreviations<br>AWS: Amazon Web Services<br>VPC: Virtual Private Cloud<br>EC2: Elastic Compute Cloud<br>IAM: Identity and Access Management<br>AMI: Amazon Machine Image<br>DLAMI-DG: Deep Learning AMI Developer Guide<br>Requirements:<br>Needed to find workable configurations for modeling machine learning problems by exploring the use of AWS Ec2 instances. The source code language was in Python and contained in a Jupyter notebook.<br>The baseline performance was defined by running the Python script on a Dell Latitude E7450 with Intel i7-5600U CPU at 2.60GHz, 16GB RAM, and Windows 10. While this can be a decent configuration for ML modeling, occasionally we may need a system with larger memory capacity or more CPUs for the tasks at hand.<br>The performance of the end-to-end script processing time on cloud instance should be comparable or even better than the baseline workstation.<br>Background and Prerequisite Information<br>The following tools and assumptions were present prior to the provisioning of the cloud instance.<br>AWS Console with the necessary rights and configuration elements to launch an instance. I had configured a VPC subnet, an IAM role, a security group, and a key pair for setting up the instance.<br>AWS Deep Learning AMI Developer Guide, released June 6, 2018<br>Web browsers<br>PuTTY<br>AWS Configuration Notes<br>AMI: I performed the following steps using both the Ubuntu-based and the Amazon Linux-based AMIs with an m5.large general-purpose instance. The AMIs were designed to take advantage of instances with GPUs. I found no issue running either AMI without the GPU; however, some pre-supplied tutorial examples probably need to be tweaked before they would work on a general-purpose instance.<br>VPC: This exercise requires only a subnet that is accessible via the Internet.<br>Security Group: I configured the security group to allow only TCP ports 22 from any IP address because I had planned to use an SSH tunnel to access the Jupyter server.<br>IAM Role: I assign all my AWS instances to an IAM role by default. For this exercise, an IAM role is not critical.<br>Key Pair: I attached the instance to an existing key pair. The key pair is necessary to access the instance via the SSH protocol.<br>Provision an instance with the Amazon Deep Learning AMI<br>Step 1) Create and launch the instance. I used an m5.large instance as the starting point.<br>Step 2) Configure the client workstation to connect to the Jupyter server. I configured my Windows workstation to connect to the Jupyter server using an SSH tunnel. The DLAMI-DG document has a write-up on how to do this for Windows, Linux, and MacOS clients (pages 15-20).<br>See the PuTTY screenshot below for configuring an SSH tunnel.<br><br>Step 3) Run the “git” commands to copy my Python scripts from GitHub to the cloud server.<br>$ git clone https://github.com/daines-analytics/sandbox-projects.git<br>Step 4) Activate the Python 3 environment by running the command:<br>$ source activate python3<br>Step 5) Because my Python script required numpy, pandas, scipy, scikit-learn, and matplotlib packages, I needed to install some additional packages.<br>On the Ubuntu AMI, I ran the command “conda install <package>” to check or to install them.<br>On the Amazon Linux AMI, I ran the command “pip install <package>” to check or to install them.<br>Step 6) Start the Jupyter server by running the command:<br>$ jupyter notebook<br><br>Step 7) Make a note of the Jupyter server URL and use that on the workstation browser running the SSH tunnel.<br><br>Step 8) Locate the Python script and run it (my own Git folder circled below).<br><br>Step 9) Compare the run-time script lengths. Not rigidly scientific but probably good enough.<br>Windows Workstation: 1 hour 50 minutes<br>Ubuntu/Deep Learning AMI: 1 hour 22 minutes<br>Amazon Linux/Deep Learning AMI: 1 hour 24 minutes<br>There you have it! A working Jupyter server on an AWS cloud instance that you can access via a secured protocol.<br>When compared to a client workstation, the right types of cloud instance can help our modeling effort. For anyone who will be attempting a similar installation, I hope these instructions can help in some way. My next step is to automate the instance creation with a CloudFormation script further. I will write down what I run into and share my findings later.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using R Take 3","author_name":"David Lowe","blog_date_text":"Fri, 10 Aug 2018 12:51:06 +0000","blog_url":"https://dainesanalytics.blog/2018/08/10/binary-classification-model-for-census-income-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has a categorical attribute, native-country, that contains over 40 different values. We will examine the models by removing the native-country attribute and see how the removed attribute might have an impact on the modeling. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1 and Take 2.<br>CONCLUSION: From iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on-par with the accuracy of the training data.<br>From the previous iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.93%, which was slightly better than the accuracy of the training data.<br>From this iteration (Take 3), the baseline performance of the ten algorithms achieved an average accuracy of 84.37%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.00%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2’s 24 hours and 33 minutes down to Take 3’s 14 hours and 2 minutes. That was a time improvement of 42%.<br>For this project, dropping the native-country attribute had no impact to the overall accuracy of the training model but contributed to a noticeable improvement of the model training time. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 08 Aug 2018 12:28:20 +0000","blog_url":"https://dainesanalytics.blog/2018/08/08/binary-classification-model-for-census-income-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has many cells with missing values, so we will examine the models by imputing the missing cells with a default value. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1.<br>CONCLUSION: From the previous iteration (Take 1), the baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on par with the accuracy of the training data.<br>From this iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.93%, which was slightly better than the accuracy of the training data.<br>For this project, imputing the missing values appeared to have contributed to a slight improvement of the overall accuracy of the training model. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 06 Aug 2018 12:23:54 +0000","blog_url":"https://dainesanalytics.blog/2018/08/06/binary-classification-model-for-census-income-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has many cells with missing values, so we will examine the models by deleting the rows with missing cells. This iteration of the project will produce a set of baseline results that we can use to compare with other data cleaning methods.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Four ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on par with the accuracy of the training data.<br>For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using Python Take 4","author_name":"David Lowe","blog_date_text":"Fri, 03 Aug 2018 12:34:18 +0000","blog_url":"https://dainesanalytics.blog/2018/08/03/binary-classification-model-for-census-income-using-python-take-4/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has a continuous numeric attribute, fnlwgt. The term estimate refers to population totals derived from CPS by creating “weighted tallies” of any specified socio-economic characteristics of the population. For this iteration, we will examine the models by removing the fnlwgt attribute and see how much of impact will the removed attribute have on the modeling. This iteration of the project will produce a set of results with which we will use to compare with the results from the first three iterations of the project.<br>CONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.<br>From iteration Take 2, the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.<br>From iteration Take 3, the baseline performance of the ten algorithms achieved an average accuracy of 81.95%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.29%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.50%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2’s 5 hours and 12 minutes down to Take 3’s 3 hours and 34 minutes. That was a time improvement of 34%.<br>For this iteration (Take 4), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.47%, which was slightly better than the accuracy of the training data. Moreover, the length of the script run-time decreased from Take 3’s 3 hours and 34 minutes down to Take 4’s 3 hours and 25 minutes. The time improvement was very small.<br>For this project, dropping the native-country and fnlwgt attributes improved both the training time and accuracy of the models. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 01 Aug 2018 12:44:55 +0000","blog_url":"https://dainesanalytics.blog/2018/08/01/binary-classification-model-for-census-income-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has a categorical attribute, native-country, that contains over 40 different values. We will examine the models by removing the native-country attribute and see how the removed attribute might have an impact on the modeling. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1 and Take 2.<br>CONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.<br>From iteration Take 2, the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.<br>From this iteration (Take 3), the baseline performance of the ten algorithms achieved an average accuracy of 81.95%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.29%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.50%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2’s 5 hours and 12 minutes down to Take 3’s 3 hours and 34 minutes. That was a time improvement of 34%.<br>For this project, dropping the native-country attribute had no impact to the overall accuracy of the training model but contributed to a noticeable improvement of the model training time. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 30 Jul 2018 12:49:23 +0000","blog_url":"https://dainesanalytics.blog/2018/07/30/binary-classification-model-for-census-income-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has many cells with missing values, so we will examine the models by imputing the missing cells with a default value. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1.<br>CONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.<br>From this iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.<br>For this project, imputing the missing values appeared to have contributed to a slight improvement of the overall accuracy of the training model. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Census Income Using Python Take 1","author_name":"David Lowe","blog_date_text":"Fri, 27 Jul 2018 12:26:08 +0000","blog_url":"https://dainesanalytics.blog/2018/07/27/binary-classification-model-for-census-income-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.<br>INTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.<br>This dataset has many cells with missing values, so we will examine the models by deleting the rows with missing cells. This iteration of the project will produce a set of baseline results that we can use to compare with other data cleaning methods.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.<br>For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Census Income Data Set<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Census+Income<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/adult-census-income<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Wine Quality Using R","author_name":"David Lowe","blog_date_text":"Wed, 25 Jul 2018 12:46:05 +0000","blog_url":"https://dainesanalytics.blog/2018/07/25/multi-class-classification-model-for-wine-quality-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset is a multi-class classification situation where we are trying to predict one of the three possible outcomes (cheap, average, and good).<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>From the previous iteration, we approached the dataset as a regression problem and tried to predict the wine quality (a continuous numeric variable) with the least amount of mean squared error. While regression is one approach for assessing the wine quality, expressing quality in pure numbers and fractions are difficult for people to grasp fully.<br>For this iteration of the project, we will approach this dataset as a multi-class problem and attempt to classify the wine quality into one of the three rating categories: 1-Good (quality 7 or above), 2-Average (quality of 5-6), and 3-Cheap (quality 4 or below).<br>CONCLUSION: The baseline performance of the seven algorithms achieved an average accuracy of 80.08%. Three ensemble algorithms (Bagged Decision Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 85.03%. With the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 83.98%, which was slightly worse than the accuracy of the training data.<br>For this project, predicting whether a bottle of wine would be good, average, or cheap appears to be more intuitive than to predict simply a numerical quality score. The Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Wine Quality Using R","author_name":"David Lowe","blog_date_text":"Mon, 23 Jul 2018 12:39:26 +0000","blog_url":"https://dainesanalytics.blog/2018/07/23/regression-model-for-wine-quality-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset can be approached as a regression situation where we are trying to predict the rating of the wine.<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>CONCLUSION: The baseline performance of the seven algorithms achieved an average RMSE of 0.7119. Three algorithms (Support Vector Machine, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average RMSE of 0.6088. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 0.6416, which was slightly worse than the RMSE of the training data. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Wine Quality Using Python","author_name":"David Lowe","blog_date_text":"Fri, 20 Jul 2018 12:08:59 +0000","blog_url":"https://dainesanalytics.blog/2018/07/20/multi-class-classification-model-for-wine-quality-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>SUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset is a multi-class classification situation where we are trying to predict one of the three possible outcomes (cheap, average, and good).<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>From the previous iteration, we approached the dataset as a regression problem and tried to predict the wine quality (a continuous numeric variable) with the least amount of mean squared error. While regression is one approach for assessing the wine quality, expressing quality in pure numbers and fractions are difficult for people to grasp fully.<br>For this iteration of the project, we will approach this dataset as a multi-class problem and attempt to classify the wine quality into one of the three rating categories: 1-Good (quality 7 or above), 2-Average (quality of 5-6), and 3-Cheap (quality 4 or below).<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 78.10%. Three ensemble algorithms (Bagged Decision Trees, Random Forest, and Extra Trees) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average accuracy of 84.78%. With the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an accuracy of 86.00%, which was even better than the accuracy of the training data.<br>For this project, predicting whether a bottle of wine would be good, average, or cheap appears to be more intuitive than to predict simply a numerical quality score. The Extra Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Wine Quality Using Python Take 3","author_name":"David Lowe","blog_date_text":"Wed, 18 Jul 2018 12:35:13 +0000","blog_url":"https://dainesanalytics.blog/2018/07/18/regression-model-for-wine-quality-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>From the first iteration (Take 1) with the red wine dataset, the baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data.<br>From the second iteration (Take 2) with the white wine dataset, the baseline performance of the 11 algorithms achieved an average RMSE of 0.6111. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3869. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3574, which was even better than the RMSE of the training data.<br>For this iteration of the project, we will perform the modeling using the datasets from both wine types. We will observe the results and learn whether the combined dataset would improve the overall prediction.<br>CONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.5882. Three ensemble algorithms (Random Forest, Extra Trees, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3663. Using the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3490, which was even better than the RMSE of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>In summary, combining the red and white wine data does not appear to have a noticeable difference on the prediction effectiveness of the wine quality.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Wine Quality Using Python Take 2","author_name":"David Lowe","blog_date_text":"Mon, 16 Jul 2018 12:56:54 +0000","blog_url":"https://dainesanalytics.blog/2018/07/16/regression-model-for-wine-quality-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>From the previous iteration, the baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data.<br>For this iteration of the project, we will perform the modeling using only the data for the white wine. For the subsequent iterations, we will analyze the combined data from both types of wine.<br>CONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.6111. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3869. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3574, which was even better than the RMSE of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>In summary, modeling red wine prediction appears to be slightly more accurate than modeling the prediction for white wines.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Wine Quality Using Python Take 1","author_name":"David Lowe","blog_date_text":"Fri, 13 Jul 2018 09:45:55 +0000","blog_url":"https://dainesanalytics.blog/2018/07/13/regression-model-for-wine-quality-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Wine Quality Data Set<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/wine+quality<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009<br>INTRODUCTION: The two datasets are related to red and white variants of the Portuguese “Vinho Verde” wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.<br>For this iteration of the project, we will perform the modeling using only the data from the red wine. For the subsequent iterations, we will analyze the white wine data and the combined data from both types of wine.<br>CONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. Using the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Credit Card Default Using R Take 2","author_name":"David Lowe","blog_date_text":"Wed, 11 Jul 2018 09:14:35 +0000","blog_url":"https://dainesanalytics.blog/2018/07/11/binary-classification-model-for-credit-card-default-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Default of Credit Card Clients Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset<br>INTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.<br>Previously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 81.05%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 82.18%. Using the optimized tuning parameter available, Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 81.94%, which was just slightly lower than the accuracy of the training data.<br>For the Take No.2 iteration, we will perform the binning operation for the credit limit and age attributes and observe the effects on the models.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.07%. Three algorithms (Decision Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, AdaBoost turned in the top result using the training data. It achieved an average accuracy of 82.22%. Using the optimized tuning parameter available, the AdaBoost algorithm processed the validation dataset with an accuracy of 82.06%, which was just slightly lower than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>For this round of modeling, converting the credit limit and age attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.<br>The HTML formatted report can be found here on Github.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Credit Card Default Using R Take 1","author_name":"David Lowe","blog_date_text":"Mon, 09 Jul 2018 09:04:30 +0000","blog_url":"https://dainesanalytics.blog/2018/07/09/binary-classification-model-for-credit-card-default-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Default of Credit Card Clients Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset<br>INTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.05%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 82.18%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 81.94%, which was just slightly lower than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on Github.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 3","author_name":"David Lowe","blog_date_text":"Fri, 06 Jul 2018 09:40:02 +0000","blog_url":"https://dainesanalytics.blog/2018/07/06/binary-classification-model-for-credit-card-default-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Default of Credit Card Clients Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset<br>INTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.<br>Previously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data.<br>For the Take No.2 iteration, we converted the Sex/Gender, Education, and Marital Status attributes into categorical variables and observed the effects on the models. After the conversion, the baseline performance of the ten algorithms achieved an average accuracy of 74.39%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.83%, which was slightly better than the accuracy of the training data.<br>For the Take No.3 iteration, we will perform the binning operation for the credit limit and age attributes and observe the effects on the models.<br>CONCLUSION: After the conversion, the baseline performance of the ten algorithms achieved an average accuracy of 74.34%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.75%, which was slightly better than the accuracy of the training data.<br>For this round of modeling, converting the credit limit and age attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 2","author_name":"David Lowe","blog_date_text":"Wed, 04 Jul 2018 09:57:08 +0000","blog_url":"https://dainesanalytics.blog/2018/07/04/binary-classification-model-for-credit-card-default-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Default of Credit Card Clients Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset<br>INTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.<br>Previously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data.<br>For the Take No.2 iteration, we will convert the Age, Education, and Marital Status attributes into categorical variables and observe the change’s effects on the modeling.<br>CONCLUSION: After converting the Age, Education, and Marital Status attributes into categorical variables, the baseline performance of the ten algorithms achieved an average accuracy of 74.39%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.83%, which was slightly better than the accuracy of the training data.<br>For this round of modeling, converting the Age, Education, and Marital Status attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 1","author_name":"David Lowe","blog_date_text":"Mon, 02 Jul 2018 09:52:51 +0000","blog_url":"https://dainesanalytics.blog/2018/07/02/binary-classification-model-for-credit-card-default-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Default of Credit Card Clients Data Set<br>Dataset ML Model: Binary classification with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients<br>One potential source of performance benchmark: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset<br>INTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.<br>CONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Templates for Python and R","author_name":"David Lowe","blog_date_text":"Fri, 29 Jun 2018 09:40:53 +0000","blog_url":"https://dainesanalytics.blog/2018/06/29/updated-machine-learning-templates-for-python-and-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.<br>Version 3 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.<br>You will find the templates on the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on the website.<br>Regression: Python template/report or R template/report<br>Binary Classification: Python template/report or R template/report<br>Multi-Class Classification: Python template/report or R template/report<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Classification Model for Faulty Steel Plates Using R","author_name":"David Lowe","blog_date_text":"Tue, 26 Jun 2018 09:30:08 +0000","blog_url":"https://dainesanalytics.blog/2018/06/26/multi-class-classification-model-for-faulty-steel-plates-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Faulty Steel Plates<br>Dataset ML Model: Multi-Class classification with numerical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/steel+plates+faults<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/faulty-steel-plates<br>INTRODUCTION: This dataset comes from research by Semeion, Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus “other”). The Input vector was made up of 27 indicators that approximately the geometric shape of the defect and its outline. According to the research paper, Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.<br>CONCLUSION: The baseline performance of the seven algorithms achieved an average accuracy of 69.69%. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 77.78%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.20%, which was slightly below the accuracy of the training data. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Classification Model for Faulty Steel Plates Using Python","author_name":"David Lowe","blog_date_text":"Fri, 22 Jun 2018 09:45:29 +0000","blog_url":"https://dainesanalytics.blog/2018/06/22/classification-model-for-faulty-steel-plates-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Faulty Steel Plates<br>Dataset ML Model: Multi-Class classification with numerical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/steel+plates+faults<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/faulty-steel-plates<br>INTRODUCTION: This dataset comes from research by Semeion, Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus “other”). The Input vector was made up of 27 indicators that approximately the geometric shape of the defect and its outline. According to the research paper, Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.<br>CONCLUSION: The baseline performance of the 10 algorithms achieved an average accuracy of 60.92%. Three algorithms (Bagged Decision Trees, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the top result achieved using the training data was from Stochastic Gradient Boosting. It achieved an average accuracy of 78.05%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting processed the validation dataset with an accuracy of 80.10%, which was slightly better than with the training data alone. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Edible Mushrooms Using Python","author_name":"David Lowe","blog_date_text":"Tue, 19 Jun 2018 09:48:40 +0000","blog_url":"https://dainesanalytics.blog/2018/06/19/binary-classification-model-for-edible-mushrooms-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Mushroom Data Set<br>Dataset ML Model: Binary classification with categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Mushroom<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/mushroom-classification<br>INTRODUCTION: This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible or definitely poisonous. The Guide, The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, clearly states that there is no simple rule for determining the edibility of a mushroom.<br>CONCLUSION: It was interesting to observe that just about all algorithms (except Naive Bayes and Support Vector Machine) scored an accuracy of 100% on the training data using a training/validation split of 70%/30%. Furthermore, all eight algorithms also scored a 100% accuracy rate using the validation dataset.<br>I reduced the training and validation to only a 50%-50% split, so the algorithms had less training data to work with. The same eight algorithms turned in a 100% accuracy on the larger validation dataset. After reducing the training and validation to a 30%-70% split, the Stochastic Gradient Boosting model dropped out of the race for predictive perfection. After a 20%-80% of training and validation split, Random Forest was the only model that was able to maintain a perfect prediction score for both the training and validation datasets.<br>For future studies, we can examine and see whether the machine learning algorithms can be trained with fewer features but still maintain the high prediction accuracy. For now, the Random Forest algorithm appeared to be the best-performing model for determining whether mushroom species are edible.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Edible Mushrooms Using R","author_name":"David Lowe","blog_date_text":"Fri, 15 Jun 2018 09:30:42 +0000","blog_url":"https://dainesanalytics.blog/2018/06/15/binary-classification-model-for-edible-mushrooms-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Mushroom Data Set<br>Dataset ML Model: Binary classification with categorical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Mushroom<br>One potential source of performance benchmarks: https://www.kaggle.com/uciml/mushroom-classification<br>INTRODUCTION: This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible or definitely poisonous. The Guide, The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, clearly states that there is no simple rule for determining the edibility of a mushroom.<br>CONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 98.65%, which was very encouraging. Four algorithms (Logistic Regression, Random Forest, AdaBoost, and Stochastic Gradient Boosting) yielded the top accuracy result of 100% using the training dataset alone. The training dataset contained 65% of the records from the original dataset (or 5,282 records), whereas the validation dataset had the remainder 35% or 2,842 records.<br>After applying the validation dataset to the four top training algorithms, all four algorithms continued to perform and achieved the accuracy of 100% with the validation data. Considering the Logistic Regression models required the least amount of training time, the recommendation is to consider using the Logistic Regression model for all future mushroom predictions.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using Python – Take 3","author_name":"David Lowe","blog_date_text":"Tue, 12 Jun 2018 09:14:36 +0000","blog_url":"https://dainesanalytics.blog/2018/06/12/regression-model-for-bike-sharing-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model.<br>For the Take No.3 of the project, we will leverage the hourly data, instead of the daily data from Take No.1. We will examine the algorithm performance and see how the hourly dataset performs against the daily data for modeling algorithms.<br>CONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 118 (vs. RMSE of 1483 from the daily dataset. Three algorithms (k-Nearest Neighbors, Random Forest, and Extra Trees) achieved the lowest RMSE values during the initial modeling round. After a series of tuning trials with these three algorithms, Extra Trees produced the best RMSE value of 69 (vs. 1233 using the daily data).<br>Extra Trees also processed the validation dataset with an RMSE value of 68 (vs. 1293 using the daily data), which was better than the average training result. For this project, the Extra-Trees ensemble algorithm yielded top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Furthermore, the use of hourly data generally yielded significantly better RMSE values for all algorithms vs. daily data. It is, therefore, a recommended approach to leverage the predictive models by using the hourly data whenever possible.<br>The HTML-formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using Python – Take 2","author_name":"David Lowe","blog_date_text":"Fri, 08 Jun 2018 09:58:08 +0000","blog_url":"https://dainesanalytics.blog/2018/06/08/regression-model-for-bike-sharing-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration (Take No.2) of the project, we attempt to use the data available, apply the one-hot-encoding transformation on each categorical attribute, and apply the Stochastic Gradient Boosting algorithm to examine the modeling effectiveness. Again, the goal of this iteration is to examine various data transformation options and find a sufficiently accurate (low error) combination for future prediction tasks.<br>This iteration of the project will test the following six modeling scenarios:<br>Scenario No.1: Perform one-hot-encoding on the categorical variable “mnth” and observe the change in regression accuracy.<br>Scenario No.2: Perform one-hot-encoding on the categorical variable “holiday” and observe the change in regression accuracy.<br>Scenario No.3: Perform one-hot-encoding on the categorical variable “weekday” and observe the change in regression accuracy.<br>Scenario No.4: Perform one-hot-encoding on the categorical variable “workingday” and observe the change in regression accuracy.<br>Scenario No.5: Perform one-hot-encoding on the categorical variable “weathersit” and observe the change in regression accuracy.<br>Scenario No.6: Perform one-hot-encoding on the all categorical variables and observe the change in regression accuracy.<br>For all scenarios, steps from sections No.3 and No.4 will be repeated for each scenario.<br>CONCLUSION: The baseline performance of the Stochastic Gradient Boosting stands at an RMSE value of 1255 using the training data. The various scenarios achieved an average RMSE value of between 1243 and 1261. For this iteration of the project, the one-hot-encoding transformation apparently did not improve the model performance with noticeable differences.<br>The HTML-formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using Python – Take 1","author_name":"David Lowe","blog_date_text":"Tue, 05 Jun 2018 09:47:00 +0000","blog_url":"https://dainesanalytics.blog/2018/06/05/regression-model-for-bike-sharing-using-python-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For available performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model. Again, the goal of this iteration is to find a sufficiently accurate (best Root Mean Squared Error or RMSE) algorithm for the future prediction tasks.<br>CONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 1,483. Three algorithms (AdaBoost, Random Forest, and Stochastic Gradient Boosting) achieved the better NMSE values during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting produced the lowest RMSE value of 1,233 using the training data.<br>Stochastic Gradient Boosting also processed the validation dataset with an RMSE value of 1,293, which was slightly worse than the best training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using R – Take 3","author_name":"David Lowe","blog_date_text":"Fri, 01 Jun 2018 12:25:47 +0000","blog_url":"https://dainesanalytics.blog/2018/06/01/regression-model-for-bike-sharing-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model.<br>For the Take No.3 of the project, we will leverage the hourly data, instead of the daily data from Take No.1. We will examine the algorithm performance and see how the hourly dataset performs against the daily data for modeling algorithms.<br>CONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 105 (vs. RMSE of 1322 from the daily dataset. Three algorithms (k-Nearest Neighbors, Random Forest, and Stochastic Gradient Boosting) achieved the lower RMSE and higher R-square values during the initial modeling round. After a series of tuning trials with these three algorithms, Random Forest produced the lowest RMSE value of 67 (vs. 1213 using the daily data) and the highest R-square value at 0.8648 (vs. 0.6093 using the daily data).<br>Random Forest also processed the validation dataset with an RMSE value of 64 (vs. 1177 using the daily data) and an R-square value of 0.8778 (vs. 0.6329 using the daily data), which was better than the average training result. For this project, the Random Forest ensemble algorithm yielded top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>Furthermore, the use of hourly data (vs. daily data) generally yielded significantly higher R-square values for all algorithms. It would be a recommended approach to leverage the predictive models by using the hourly data whenever possible.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using R – Take 2","author_name":"David Lowe","blog_date_text":"Tue, 29 May 2018 12:24:42 +0000","blog_url":"https://dainesanalytics.blog/2018/05/29/regression-model-for-bike-sharing-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For available performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration (Take No.2) of the project, we attempt to use the data available, transform as necessary, and apply the Stochastic Gradient Boosting algorithm to examine the modeling effectiveness. Again, the goal of this iteration is to examine various data transformation options and find a sufficiently accurate (low error) combination for future prediction tasks.<br>This iteration of the project will test the following four modeling scenarios:<br>Scenario No.1: Remove the attribute “atemp” since it was highly correlated with the attribute “temp.”<br>Scenario No.2: Perform one-hot-encoding on the variable “season.”<br>Scenario No.3: Perform one-hot-encoding on the variable “mnth.”<br>For scenarios 2-3, steps from section No.3 and No.4 will be repeated for each scenario.<br>CONCLUSION: The baseline performance of the Stochastic Gradient Boosting stands at an RMSE value of 1240 and an R-square value of 0.5943 using the training data. Scenario No.1 did slightly better with an RMSE value of 1233 and an R-square value of 0.5991. As the result, we will leverage scenario No.1 to training the final model and observe how it will do with the validation dataset.<br>The final Stochastic Gradient Boosting model processed the validation dataset with an RMSE value of 1180 and an R-square value of 0.6320, which was slightly worse than the Take No.1 result of 1177 for RMSE and 0.6329 for R-square. For this iteration of the project, data transformation did not improve the model performance with a noticeable outcome.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Model for Bike Sharing Using R – Take 1","author_name":"David Lowe","blog_date_text":"Fri, 25 May 2018 12:32:51 +0000","blog_url":"https://dainesanalytics.blog/2018/05/25/regression-model-for-bike-sharing-using-r-take-1/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Dataset Used: Bike Sharing Dataset<br>Dataset ML Model: Regression with numerical attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset<br>For available performance benchmarks, please consult: https://www.kaggle.com/contactprad/bike-share-daily-data<br>INTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model. Again, the goal of this iteration is to find a sufficiently accurate (low error) algorithm for the future prediction tasks.<br>CONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 1322. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the lower RMSE and higher R-square values during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting produced the lowest RMSE value of 1213 and the highest R-square value at 0.6093 using the training data.<br>Stochastic Gradient Boosting also processed the validation dataset with an RMSE value of 1177 and an R-square value of 0.6329, which was better than the average training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Template Using R","author_name":"David Lowe","blog_date_text":"Tue, 22 May 2018 12:39:08 +0000","blog_url":"https://dainesanalytics.blog/2018/05/22/updated-machine-learning-template-using-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.<br>Version 2 of the templates contain time stamps at various locations within the script. I have been using the timing benchmarks to test my modeling and tuning efficiency. You will find the templates on the Machine Learning Project Templates page.<br>You can also check out the templates and sample HTML-formatted reports here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Updated Machine Learning Template Using Python","author_name":"David Lowe","blog_date_text":"Fri, 18 May 2018 12:29:11 +0000","blog_url":"https://dainesanalytics.blog/2018/05/18/updated-machine-learning-template-using-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.<br>Version 2 of the templates contain time stamps at various locations within the script. I have been using the timing benchmarks to test my modeling and tuning efficiency. You will find the templates on the Machine Learning Project Templates page.<br>You can also check out the templates and sample HTML-formatted reports here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Bank Marketing Using Python, Take 3","author_name":"David Lowe","blog_date_text":"Tue, 15 May 2018 12:27:46 +0000","blog_url":"https://dainesanalytics.blog/2018/05/15/binary-classification-model-for-bank-marketing-using-python-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Dataset<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 41,000 observations with 19 input variables and 1 output variable. There are no missing values within the dataset. This dataset is based on “Bank Marketing” UCI dataset and is enriched by the addition of five new social and economic features/attributes. This dataset is almost identical to the one without the five new attributes.<br>CONCLUSION: The take No.3 version of this banking dataset aims to test the addition of five additional social-economical attributes to the dataset and the effect. You can see the results from the take No.2 here on the website.<br>The baseline performance of the ten algorithms achieved an average accuracy of 88.32% (vs. 87.68% from the take No.2 version). Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved the top accuracy/Kappa result using the training data. It produced an average accuracy of 90.06% (vs. 89.49% from the take No.2 version) using the training data.<br>Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 90.25%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The addition of the social-economical attributes did not seem to have a substantial effect on the overall accuracy of the prediction models.<br>The HTML formatted report can be found here on the website.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Bank Marketing Using R, Take 3","author_name":"David Lowe","blog_date_text":"Fri, 11 May 2018 12:31:30 +0000","blog_url":"https://dainesanalytics.blog/2018/05/11/binary-classification-model-for-bank-marketing-using-r-take-3/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Data Set<br>Data Set ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 41,000 observations with 19 input variables and 1 output variable. There are no missing values within the dataset. This dataset is based on “Bank Marketing” UCI dataset and is enriched by the addition of five new social and economic features/attributes. This dataset is almost identical to the one without the five new attributes.<br>CONCLUSION: The take No.3 version of this banking dataset aims to test the addition of five additional social-economical attributes to the dataset and the effect. You can see the results from the take No.2 here on the website.<br>The baseline performance of the seven algorithms achieved an average accuracy of 89.70% (vs. 89.22% from the take No.2 version). Three algorithms (Logistic Regression, Bagged CART, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved the top accuracy/Kappa result using the training data. It produced an average accuracy of 90.11% (vs. 89.46% from the take No.2 version) using the training data.<br>Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 90.00%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The addition of the social-economical attributes did not seem to have a substantial effect on the overall accuracy of the prediction models.<br>The HTML formatted report can be found here on the website.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Bank Marketing Using R, Take 2","author_name":"David Lowe","blog_date_text":"Tue, 08 May 2018 12:11:54 +0000","blog_url":"https://dainesanalytics.blog/2018/05/08/binary-classification-model-for-bank-marketing-using-r-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Data Set<br>Data Set ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values within the dataset.<br>CONCLUSION: The take No.2 version of this banking dataset aims to test the removal of one attribute from the dataset and the effect. You can see the results from the take No.1 here on the website.<br>The data removed was the “duration” attribute. According to the dataset documentation, this attribute highly affects the output target (e.g., if duration=0 then y=“no”). However, the duration is not known before a call is performed. Also, after the end of the call, the target variable is naturally identified. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>The baseline performance of the seven algorithms achieved an average accuracy of 89.22% (vs. 89.99% from the take No.1). Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved the top accuracy/Kappa result using the training data. It produced an average accuracy of 89.46% (vs. 90.63% from the take No.1) using the training data.<br>Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 89.18%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The elimination of the “duration” attribute did not seem to have a substantial adverse effect on the overall accuracy of the prediction models.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Model for Bank Marketing Using Python, Take 2","author_name":"David Lowe","blog_date_text":"Fri, 04 May 2018 12:16:07 +0000","blog_url":"https://dainesanalytics.blog/2018/05/04/binary-classification-model-for-bank-marketing-using-python-take-2/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Dataset<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.<br>CONCLUSION: The take No.2 version of this banking dataset aims to test the removal of one attribute from the dataset and the effect. You can see the results from the take No.1 here on the website.<br>The data removed was the “duration” attribute. According to the dataset documentation, this attribute highly affects the output target (e.g., if duration=0 then y=”no”). However, the duration is not known before a call is performed. Also, after the end of the call, the target variable is naturally identified. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>The baseline performance of the ten algorithms achieved an average accuracy of 87.68% (vs. 89.13% from the take No.1). Three algorithms (Linear Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting (SGB) achieve the top result using the training data. It produced an average accuracy of 89.49% (vs. 91.00% from the take No.1) using the training data.<br>SGB also processed the validation dataset with an accuracy of 89.21% (vs. 90.58% from the take No.1). For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The elimination of the “duration” attribute did not seem to have a substantial adverse effect on the overall accuracy of the prediction models.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Installing Jupyter and R on Fedora","author_name":"David Lowe","blog_date_text":"Tue, 01 May 2018 12:19:44 +0000","blog_url":"https://dainesanalytics.blog/2018/05/01/installing-jupyter-and-r-on-fedora/","blog_text":"This is a replication and extension to Dr. Jason Brownlee’s post. I am making these notes, so I can repeat the installation process at a later time.<br>Step 1) Provision a Fedora instance using your favorite VM Manager. I happened to use VMware, but Dr. Brownlee’s VirtualBox worked just fine. I recommend fully patch the Fedora installation before proceeding to the Python and R steps.<br>Step 2) Verify the Python 3 installation and install the packages described by Dr. Brownlee’s post.<br><br><br>Step 3) Install Jupyter using pip:<br>$ sudo python3 -m pip install –upgrade pip<br>$ sudo python3 -m pip install jupyter<br>Review the web page http://jupyter.org/install for more information.<br><br><br><br><br>Step 4) Start up the Jupyter notebook server: $ jupyter notebook<br><br><br>Step 5) The default browser should kick in and display the Jupyter notebook web page.<br><br><br>Step 6) Install R using the command line with: $ sudo dnf install R<br><br><br>Step 7) Browse RStudio web page and find the Linux client appropriate for your installation. In my case, it was the installer for “Fedora 19+/RedHat 7+/openSUSE 13.1+ (64-bit).”<br><br><br>Step 8) Click on “Install” and then “Launch.” The familiar RStudio interface should appear.<br><br><br><br><br><br><br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Machine Learning Template Using Python","author_name":"David Lowe","blog_date_text":"Sun, 29 Apr 2018 12:04:01 +0000","blog_url":"https://dainesanalytics.blog/2018/04/29/regression-machine-learning-template-using-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support regression ML problems using Python.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Bank Marketing Using Python","author_name":"David Lowe","blog_date_text":"Fri, 27 Apr 2018 12:25:55 +0000","blog_url":"https://dainesanalytics.blog/2018/04/27/simple-classification-model-for-bank-marketing-using-python/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Dataset<br>Dataset ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.<br>CONCLUSION: The baseline performance of the 11 algorithms achieved an average accuracy of 89.13%. Three algorithms (Stochastic Gradient Boosting, Random Forest, and AdaBoost) achieved the top accuracy and Kappa scores. The top result achieved using the training data was from Stochastic Gradient Boosting. It achieved an average accuracy of 91.00% after a series of tuning trials, and its accuracy in processing the validation dataset was 90.58%. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Machine Learning Template Using Python","author_name":"David Lowe","blog_date_text":"Tue, 24 Apr 2018 12:23:27 +0000","blog_url":"https://dainesanalytics.blog/2018/04/24/multi-class-machine-learning-template-using-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support multi-class ML problems using Python.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Machine Learning Template Using Python","author_name":"David Lowe","blog_date_text":"Sun, 22 Apr 2018 12:21:00 +0000","blog_url":"https://dainesanalytics.blog/2018/04/22/binary-classification-machine-learning-template-using-python/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support binary classification ML problems using Python.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Bank Marketing Using R","author_name":"David Lowe","blog_date_text":"Fri, 20 Apr 2018 12:44:51 +0000","blog_url":"https://dainesanalytics.blog/2018/04/20/simple-classification-model-for-bank-marketing-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/)<br>Dataset Used: Bank Marketing Data Set<br>Data Set ML Model: Binary classification with numerical and categorical attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/bank+marketing<br>One source of potential performance benchmarks: https://www.kaggle.com/rouseguy/bankbalanced<br>INTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.<br>CONCLUSION: The baseline performance of eight algorithms achieved an average accuracy of 89.99%. Three algorithms (Random Forest, Stochastic Gradient Boosting, and Bagged CART) achieved the top accuracy and Kappa scores. The top result achieved using the training data was from Random Forest. It achieved an average accuracy of 90.65% after a series of tuning trials, and its accuracy in processing the validation dataset was 90.91%. For this project, the Random Forest ensemble algorithms yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Regression Machine Learning Template Using R","author_name":"David Lowe","blog_date_text":"Tue, 17 Apr 2018 12:34:55 +0000","blog_url":"https://dainesanalytics.blog/2018/04/17/regression-machine-learning-template-using-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions for creating a machine learning template, I have pulled together a project template that can be used to support regression ML problems using R.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Multi-Class Machine Learning Template Using R","author_name":"David Lowe","blog_date_text":"Sun, 15 Apr 2018 12:32:35 +0000","blog_url":"https://dainesanalytics.blog/2018/04/15/multi-class-machine-learning-template-using-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support multi-class ML problems using R.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Binary Classification Machine Learning Template Using R","author_name":"David Lowe","blog_date_text":"Fri, 13 Apr 2018 12:40:30 +0000","blog_url":"https://dainesanalytics.blog/2018/04/13/binary-classification-machine-learning-template-using-r/","blog_text":"As I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.<br>Thanks to Dr. Jason Brownlee’s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support binary classification ML problems using R.<br>The purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.<br>You can also check out the sample HTML-formatted report here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Machine Learning Algorithms Catalog","author_name":"David Lowe","blog_date_text":"Wed, 11 Apr 2018 12:08:38 +0000","blog_url":"https://dainesanalytics.blog/2018/04/11/machine-learning-algorithms-catalog/","blog_text":"As I work on practicing and solving machine learning problems, I often forget which algorithms I should consider applying.<br>Thanks to Dr. Jason Brownlee’s suggestions on learning machine learning algorithms, I have pulled together a table that summarizes some of the popular algorithms with key information about them.<br>Here are the explanations of the column attributes.<br>Regression: Is this algorithm suitable for solving regression problems, yes or no.<br>Two-Class: Is this algorithm suitable for solving binary classification problems, yes or no.<br>Multi-Class: Is this algorithm suitable for solving general classification problems, yes or no.<br>Clustering: Is this algorithm suitable for solving clustering or unsupervised problems, yes or no.<br>Class (Weka): The intended class variables designed to be solved by the algorithm. I obtained the information from Weka [http://www.cs.waikato.ac.nz/ml/weka].<br>Attributes (Weka): The intended attributes or features that the algorithm can use. I obtained the information from Weka.<br>Data Prep Tips (MLM): Some key things to know handling the data for the algorithm. I credit the information to Dr. Jason Brownlee and his Machine Learning Mastery website.<br>Learning Style: Supervised or Unsupervised<br>Algorithm Class: Linear, Nonlinear, Ensemble for supervised algorithms or Clustering for unsupervised algorithms<br>Weka Library: The name of the algorithm implemented in Weka.<br>R Caret Library: The name of the algorithm implemented in the Caret package for R [http://caret.r-forge.r-project.org].<br>Python Library: The name of the algorithm implemented in the scikit-learn library for Python [http://scikit-learn.org/].<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Diabetes Prediction Using R","author_name":"David Lowe","blog_date_text":"Fri, 06 Apr 2018 12:36:14 +0000","blog_url":"https://dainesanalytics.blog/2018/04/06/simple-classification-model-for-diabetes-prediction-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Pima Indians Diabetes Database<br>Data Set ML Model: Classification with numerical attributes<br>Dataset Reference: https://www.kaggle.com/uciml/pima-indians-diabetes-database<br>For more information on performance benchmarks, please consult: https://www.kaggle.com/uciml/pima-indians-diabetes-database<br>INTRODUCTION: The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values.<br>CONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 75.85%. The top accuracy result achieved via Logistic Regression was 77.73% after a series of tuning trials. The ensemble algorithms, in this case, did not yield a better result than the non-ensemble algorithms to justify the additional processing required.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Glass Type Using R","author_name":"David Lowe","blog_date_text":"Fri, 30 Mar 2018 12:13:13 +0000","blog_url":"https://dainesanalytics.blog/2018/03/30/simple-classification-model-for-glass-type-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Glass Identification Data Set<br>Data Set ML Model: Classification with real number attributes<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Glass+Identification<br>For more information on this case study project and performance benchmarks, please consult: https://www.kaggle.com/uciml/glass<br>The glass identification dataset involves predicting the six types of glass, defined by their oxide content (i.e., Na, Fe, K, .and so forth). The criminological investigation was the motivation for the study of classification of types of glass. At the scene of the crime, the glass left can be used as evidence, if it is correctly identified!<br>CONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 71.45%. The top accuracy result achieved via RandomForest was 80.11% after a series of tuning trials. The ensemble algorithm, in this case, yielded a better result than the non-ensemble algorithms to justify the additional processing and tuning.<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Regression Model for Predicting Abalone Age Using R","author_name":"David Lowe","blog_date_text":"Sat, 24 Mar 2018 02:30:32 +0000","blog_url":"https://dainesanalytics.blog/2018/03/23/simple-regression-model-for-predicting-abalone-age-using-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Abalone Data Set<br>Data Set ML Model: Regression with Categorical, Integer, Real attributes<br>Dataset Reference: http://archive.ics.uci.edu/ml/datasets/Abalone<br>The Abalone Dataset involves predicting the age of abalone given objective measures of individuals. Although it was presented as a multi-class classification problem, this exercise will frame it using regression. The baseline performance of predicting the mean value is an RMSE of approximately 3.2 rings.<br>CONCLUSION: The baseline performance of predicting the most prevalent class achieved an RMSE of approximately 2.28 rings. The top RMSE result achieved via SVM was 2.13 rings after a series of tuning. The ensemble algorithm did not yield a better result than SVM to justify the additional processing and tuning necessary.<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Provision a LAMP Server in the AWS GovCloud Region","author_name":"David Lowe","blog_date_text":"Tue, 20 Mar 2018 12:10:18 +0000","blog_url":"https://dainesanalytics.blog/2018/03/20/provision-a-lamp-server-in-the-aws-govcloud-region/","blog_text":"This is my notes for provisioning a LAMP server in the Amazon Web Services (AWS) GovCloud region using Amazon’s Linux AMI. I had to set up a LAMP server and secured it with HTTPS. These notes outlined my installation process during the week of 12 March 2018.<br>My goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do get past those obstacles.<br>You can find the installation and configuration notes below.<br>Abbreviations<br>VPC-UG: Amazon Virtual Private Cloud User Guide, released 29 November 2017.<br>EC2-UG: Amazon Elastic Compute Cloud User Guide for Linux Instances. There are a number of revisions for this user guide. For this project, I used the version that was released on 9 February 2018. This version talks about setting up the LAMP stack and SSL/TLS with the original Amazon Linux AMI. The more recent user guides cover the newer Amazon Linux 2 AMI.<br>ROUTE53-DG: Amazon Route 53 Developer Guide, released 5 December 2017.<br>Requirements:<br>Need to provision a Linux server with the LAMP stack in AWS GovCloud region. The server will host PHP-based web applications. All HTTP traffic will be re-directed and forced to use HTTPS.<br>Background and Prerequisite Information<br>Needed to sign-up for an AWS GovCloud account. The GovCloud account is going to be a separate account from your regular AWS account with a different account ID. See the web page, https://docs.aws.amazon.com/govcloud-us/latest/UserGuide/getting-started-sign-up.html, for signing up.<br>Tools Used<br>Web browsers<br>AWS Console<br>PuTTY<br>WinSCP<br>Configure virtual private cloud (VPC)<br>When setting up the GovCloud account, AWS created a default VPC for that account. That default VPC had three subnets configured with an Internet gateway attached to each one of them.<br>For the one and only public-Internet-accessible server I needed, the default VPC would have worked. I did not use the default VPC. Instead, I rolled my own VPC with one public subnet and one private subnet.<br>If you need more info on Amazon VPC, check out their VPC-UG or many documented instructions available on the net. For training purposes, I personally use the AWS course with Linux Academy. [https://linuxacademy.com/]<br>Provision an instance with the Amazon Linux AMI and Elastic IP<br>Provision a Linux instance with Elastic IP: Because the Amazon Linux 2 AMI is not yet available on GovCloud, I had to use the version 1 of the Linux AMI.<br>Set up the DNS entry: I had an inactive domain, so I used Route 53 to configure the DNS zone and host entries. Please note that for a server hosted in GovCloud, the Route 53 settings will need to be configured from the non-GovCloud console. Another word, you will not find Route 53 in the GovCloud console. I used the instructions outline in ROUTE53-DG pages 213-216.<br>Install the LAMP Stack: The instructions on pages 41-46 held up well for my installation. I configured Apache for HTTPS in the following section.<br>Enable SSL/TLS: Refer to pages 58-72 in EC2-UG. Since I did not have a certificate to use, I opted to obtain a cert with EFF through the Let’s Encrypt program. The certbot instructions described on pages 69-72 worked only partially. Certbot aborted in the middle of the installation and complained about not able to find a virtual host. Repeated attempts of certbot -auto option did not work for me. I finally resorted to this blog post [https://nouveauframework.org/blog/installing-letsencrypts-free-ssl-amazon-linux/] as it helped me generate the three cert files I needed. I updated the ssl.conf manually and got the HTTPS going. At this point, the HTTP was not being redirected to HTTPS. Another similar inquiry on Stack Exchange pointed me to run the “certbot -apache” command. By doing that, I was able to force the HTTP to HTTPS redirect without needing to go hunt down the config file I needed to update. I also configured certbot in crontab to check for auto-renew daily.<br>Install phpMyAdmin: Needed to do a few things with PHP config because phpMyAdmin was complaining about the BlowFish secret and configuration storage. This blog post got me through the configuration tasks [https://www.digitalocean.com/community/questions/phpmyadmin-or-alternative-for-php7-nginx-mysql-5-7-ubuntu-16-04].<br>Post-AppGini App Deployment<br>After deploying the AppGini app, two more things surfaced.<br>The GD support in PHP was requested. It was a matter of installing the correct GD version by using the commands “php -version” and “sudo yum install php70-gd” for the PHP 7.0 environment I assembled.<br>I also needed to reset the ownership and file permissions for the “images” folder inside my AppGini application. AppGini was complaining about not able to write into it. I followed the instructions from page 44 of EC2-UG and resolved that.<br>This is what I wrote and could think of so far. If you will be attempting a similar installation, I hope these can help in some way. My next project is doing a similar install using Amazon Linux 2 AMI in a non-GovCloud environment. I will be checking out the Linux AMI version 2 instructions in the user guide. Will write down what I run into and share my findings later.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Ensemble Classification Model for the Sonar Dataset with R","author_name":"David Lowe","blog_date_text":"Fri, 16 Mar 2018 12:33:28 +0000","blog_url":"https://dainesanalytics.blog/2018/03/16/ensemble-classification-model-for-the-sonar-dataset-with-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set<br>ML Model: Classification, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29<br>The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.<br>CONCLUSION: The baseline performance of predicting the most prevalent class achieved an accuracy of approximately 76.0%. Top results achieved via SVM was approximately 85.06% after a series of tuning. The RandomForest ensemble algorithm, also after tuning, yielded an accuracy of 85.09%. The very slight improvement between RF and SVM was too small to justify the additional processing and tuning required by the ensemble algorithm.<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for the Sonar Dataset with R","author_name":"David Lowe","blog_date_text":"Tue, 13 Mar 2018 12:28:56 +0000","blog_url":"https://dainesanalytics.blog/2018/03/13/simple-classification-model-for-the-sonar-dataset-with-r/","blog_text":"Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set<br>ML Model: Classification, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29<br>The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.<br>The baseline performance of predicting the most prevalent class is a classification accuracy of approximately 76.0%. Top results achieve a classification accuracy of approximately 84.7%.<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Text Messages with Python","author_name":"David Lowe","blog_date_text":"Sun, 11 Mar 2018 05:10:38 +0000","blog_url":"https://dainesanalytics.blog/2018/03/10/simple-classification-model-for-text-messages-with-python/","blog_text":"Methodology Credit: Re-produced and adapted from a tutorial made available by Evgeny Volkov, SMS Spam Detection with Various Classifiers.<br>Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Data Set Description: https://www.kaggle.com/uciml/sms-spam-collection-dataset<br>Original Reference: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/<br>Modeling Approach: binary classification<br>The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according to being ham (legitimate) or spam.<br>We will be spot-checking a suite of linear and nonlinear machine learning algorithms and comparing the estimated accuracy of algorithms. For this project, we will evaluate 9 different algorithms:<br>Linear Algorithms: Logistic Regression (LR)<br>Nonlinear Algorithms: Decision Tree (DTC), Support Vector Machine (SVC), Multinomial Native Bayes (MNB) and k-Nearest Neighbors (KNC)<br>Ensemble Algorithms: Random Forest (RFC), AdaBoost (ABC), Bagging (BC), and ExtraTree (ETC)<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for Text Messages with R","author_name":"David Lowe","blog_date_text":"Wed, 07 Mar 2018 23:43:15 +0000","blog_url":"https://dainesanalytics.blog/2018/03/07/simple-classification-model-for-text-messages/","blog_text":"Methodology Credit: Re-produced and adapted from a tutorial made available by Anish Singh Walia, Text Message Classification.<br>Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Data Set Description: https://www.kaggle.com/uciml/sms-spam-collection-dataset<br>Original Reference: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/<br>Modeling Approach: binary classification<br>The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according to being ham (legitimate) or spam.<br>Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.<br>Any predictive modeling machine learning project can be broken down into about 6 common tasks:<br>Define Problem<br>Summarize Data (Use the word cloud visualization technique for this project)<br>Prepare Data (Not required for this project)<br>Evaluate Algorithms (Use Naive Bayes classifier and measure accuracy)<br>Improve Accuracy or Results<br>Finalize Model and Present Results<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification Model for the Sonar Dataset with Python","author_name":"David Lowe","blog_date_text":"Wed, 28 Feb 2018 13:43:15 +0000","blog_url":"https://dainesanalytics.blog/2018/02/28/simple-classification-model-for-the-sonar-dataset/","blog_text":"Template Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/standard-machine-learning-datasets/.<br>Dataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set<br>ML Model: Classification, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29<br>The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.<br>The baseline performance of predicting the most prevalent class is a classification accuracy of approximately 53%. Top results achieve a classification accuracy of approximately 88%.<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Regression Ensemble Model for Boston Housing with Python","author_name":"David Lowe","blog_date_text":"Sat, 24 Feb 2018 13:02:36 +0000","blog_url":"https://dainesanalytics.blog/2018/02/24/simple-regression-ensemble-model-for-boston-housing-with-python/","blog_text":"Credit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/regression-machine-learning-tutorial-weka/.<br>Dataset Used: Housing Values in Suburbs of Boston<br>ML Model: Regression, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Housing<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore feature selection options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>For this “Take-2” version of the project, we added the ensemble models to the exploration.<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Regression Baseline Model for Boston Housing Price with Python","author_name":"David Lowe","blog_date_text":"Wed, 21 Feb 2018 13:01:01 +0000","blog_url":"https://dainesanalytics.blog/2018/02/21/simple-regression-baseline-model-for-boston-housing-price-with-python/","blog_text":"Credit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/regression-machine-learning-tutorial-weka/.<br>Dataset Used: Housing Values in Suburbs of Boston<br>ML Model: Regression, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Housing<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore feature selection options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification of Titanic Dataset, Take 3","author_name":"David Lowe","blog_date_text":"Sat, 17 Feb 2018 13:53:00 +0000","blog_url":"https://dainesanalytics.blog/2018/02/17/simple-classification-of-titanic-dataset-take-3/","blog_text":"Methodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.<br>Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/).<br>Data Set Description: https://www.kaggle.com/c/titanic/data<br>Benchmark References: https://www.kaggle.com/c/titanic/data<br>For the take #3 version of the project, we will add the fourth and fifth iterations of experimenting with several machine learning algorithms. We will see whether the machine learning algorithms can improve our predictions.<br>Label all passengers dead or the attribute $Survived = 0 (The worst-case scenario)<br>Label all female passengers survived or the attribute $Survived = 1<br>Label all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0<br>Leverage machine learning algorithms to generate predictions<br>Tune the best-performing algorithm by experimenting various parameters<br>Leverage ensembles of algorithm to generate predictions<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification of Titanic Dataset, Take 2","author_name":"David Lowe","blog_date_text":"Wed, 14 Feb 2018 13:47:46 +0000","blog_url":"https://dainesanalytics.blog/2018/02/14/simple-classification-of-titanic-dataset-take-2/","blog_text":"Methodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.<br>Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/).<br>Data Set Description: https://www.kaggle.com/c/titanic/data<br>Benchmark References: https://www.kaggle.com/c/titanic/data<br>For the take #2 version of the project, we will add the fourth and fifth iterations of experimenting with several machine learning algorithms. We will see whether the machine learning algorithms can improve our predictions.<br>Label all passengers dead or the attribute $Survived = 0 (The worst-case scenario)<br>Label all female passengers survived or the attribute $Survived = 1<br>Label all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0<br>Leverage machine learning algorithms to generate predictions<br>Tune the best-performing algorithm by experimenting various parameters<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Classification of Titanic Dataset, Take 1","author_name":"David Lowe","blog_date_text":"Sat, 10 Feb 2018 13:29:28 +0000","blog_url":"https://dainesanalytics.blog/2018/02/10/simple-classification-of-titanic-dataset-take-1/","blog_text":"Methodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.<br>Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http://machinelearningmastery.com/).<br>Data Set Description: https://www.kaggle.com/c/titanic/data<br>Benchmark References: https://www.kaggle.com/c/titanic/data<br>For the take #1 version of the project, I have done a few iterations of generating predictions and submitting them to Kaggle to gather the accuracy metric. The iterations are:<br>Label all passengers dead or the attribute $Survived = 0 (The worst-case scenario)<br>Label all female passengers survived or the attribute $Survived = 1<br>Label all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Binary Classification for Breast Cancer with Ensemble Models","author_name":"David Lowe","blog_date_text":"Mon, 05 Feb 2018 04:23:08 +0000","blog_url":"https://dainesanalytics.blog/2018/02/04/simple-binary-classification-for-breast-cancer-with-ensemble-models/","blog_text":"Template Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Data Set Description: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)<br>Benchmark References: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data<br>Modeling Approach: binary classification, converting categorical to numerical attributes<br>Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.<br>We will compare several different algorithms and determine which one would yield the best results. The project aims to touch on the following areas:<br>Document a classification predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>For this “Take-2” version of the project, we added the ensemble models to the exploration.<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Binary Classification for Breast Cancer Dataset","author_name":"David Lowe","blog_date_text":"Sun, 28 Jan 2018 13:59:37 +0000","blog_url":"https://dainesanalytics.blog/2018/01/28/simple-binary-classification-for-breast-cancer-dataset/","blog_text":"Template Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>Data Set Description: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)<br>Benchmark References: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data<br>Modeling Approach: binary classification, converting categorical to numerical attributes<br>Working through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.<br>We will compare several different algorithms and determine which one would yield the best results. The project aims to touch on the following areas:<br>Document a classification predictive modeling problem end-to-end.<br>Explore data transformation options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Regression Model for Boston Housing with Ensemble Models","author_name":"David Lowe","blog_date_text":"Sun, 21 Jan 2018 13:53:27 +0000","blog_url":"https://dainesanalytics.blog/2018/01/21/simple-regression-model-for-boston-housing-with-ensemble-models/","blog_text":"Credit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/regression-machine-learning-tutorial-weka/.<br>Dataset Used: Housing Values in Suburbs of Boston<br>ML Model: Regression, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Housing<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore feature selection options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>For this “Take-2” version of the project, we added the ensemble models to the exploration.<br>Explore using and tuning ensemble methods for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."},{"blog_title":"Simple Regression Model for Boston Housing Price","author_name":"David Lowe","blog_date_text":"Mon, 15 Jan 2018 00:21:00 +0000","blog_url":"https://dainesanalytics.blog/2018/01/14/simple-regression-model-for-boston-housing-price/","blog_text":"Credit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.<br>For more information on this case study project, please consult Dr. Brownlee’s blog post at https://machinelearningmastery.com/regression-machine-learning-tutorial-weka/.<br>Dataset Used: Housing Values in Suburbs of Boston<br>ML Model: Regression, numeric inputs<br>Dataset Reference: https://archive.ics.uci.edu/ml/datasets/Housing<br>The purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:<br>Document a regression predictive modeling problem end-to-end.<br>Explore feature selection options for improving model performance<br>Explore algorithm tuning techniques for improving model performance<br>The HTML formatted report can be found here on GitHub.<br>Share this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading..."}]
