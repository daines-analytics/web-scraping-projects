[{"blog_title":"Data Quality Assurance for Enterprise Web Scraping","blog_url":"https://blog.scrapinghub.com/data-quality-assurance-for-enterprise-web-scraping","blog_date":"September 27, 2018","blog_author":"Ian Kerins","blog_summary":"When it comes to web scraping, one key element is often overlooked until it becomes a big problem. \nThat is data quality. \nGetting consistent high quality data when scraping the web is critical to the success of any web scraping project, particularly when scraping the web at scale or extracting mission critical data where accuracy is paramount. \nData quality can be the difference between a...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"What I Learned as a Google Summer of Code student at Scrapinghub","blog_url":"https://blog.scrapinghub.com/what-i-learned-as-a-google-summer-of-code-student-at-scrapinghub","blog_date":"September 12, 2018","blog_author":"Lam Chau Tung Nguyen","blog_summary":"Google Summer of Code (GSoC) was such a great experience for students like me. I learned so much about open source communities as well as contributing to their complex projects. I also learned a great deal from my mentors, Konstantin and Cathal, about programming and software engineering practices. In my opinion, the most valuable lesson I got from GSoC was what it was like to be a Software...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"GDPR Compliance For Web Scrapers: The Step-By-Step Guide","blog_url":"https://blog.scrapinghub.com/web-scraping-gdpr-compliance-guide","blog_date":"July 25, 2018","blog_author":"Ian Kerins","blog_summary":"Unless you’ve been living under a rock for the past few months you know that the EU’s General Data Protection Regulation (GDPR) is upon us. \nIt is the most comprehensive data protection law ever been introduced, fundamentally changing the way companies can use the personal data of their customers and prospects. \nThere are countless articles and guides about how GDPR will affect your company’s...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"For E-Commerce Data Scientists: Lessons Learned Scraping 100 Billion Products Pages","blog_url":"https://blog.scrapinghub.com/web-scraping-at-scale-lessons-learned-scraping-100-billion-products-pages","blog_date":"July 02, 2018","blog_author":"Ian Kerins","blog_summary":"Web scraping can look deceptively easy these days. There are numerous open-source libraries/frameworks, visual scraping tools and data extraction tools that make it very easy to scrape data from a website. However, when you want to scrape websites at scale things start to get very tricky, very fast.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"A Sneak Peek Inside What Hedge Funds Think of Alternative Financial Data","blog_url":"https://blog.scrapinghub.com/2018/06/19/a-sneak-peek-inside-what-hedge-funds-think-of-alternative-financial-data","blog_date":"June 19, 2018","blog_author":"Ian Kerins","blog_summary":"Unbeknownst to many, there is a data revolution happening in finance. \n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Want to Predict Fitbit’s Quarterly Revenue? Eagle Alpha Did It Using Web Scraped Product Data","blog_url":"https://blog.scrapinghub.com/2018/06/07/fitbit-quarterly-revenue-web-scraped-product-data","blog_date":"June 07, 2018","blog_author":"Ian Kerins","blog_summary":"Throughout the history of the financial markets information has been power. The trader with access to the most accurate information can quickly gain an edge over the market.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How Data Compliance Companies Are Turning To Web Crawlers To Take Advantage of the GDPR Business Opportunity","blog_url":"https://blog.scrapinghub.com/2018/05/30/gdpr-compliance-tools-web-scraping-crawlers","blog_date":"May 30, 2018","blog_author":"Ian Kerins","blog_summary":"Over the last couple weeks, GDPR has brought data protection center stage. What was once a fringe concern for most businesses overnight became a burning problem that needed to be solved immediately.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Looking Back at 2017","blog_url":"https://blog.scrapinghub.com/2017/12/31/looking-back-at-2017","blog_date":"December 31, 2017","blog_author":"Scott Garcia","blog_summary":"It’s been another standout year for Scrapinghub and the scraping community at large. Together we crawled 79.1 billion pages (nearly double 2016), with over 103 billion scraped records; what a year!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"A Faster, Updated Scrapinghub","blog_url":"https://blog.scrapinghub.com/2017/11/05/a-faster-updated-scrapinghub","blog_date":"November 05, 2017","blog_author":"Scott Garcia","blog_summary":"We’re very excited to announce a new look for Scrapinghub!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scraping the Steam Game Store with Scrapy","blog_url":"https://blog.scrapinghub.com/2017/07/07/scraping-the-steam-game-store-with-scrapy","blog_date":"July 07, 2017","blog_author":"Andre Perunicic","blog_summary":"This is a guest post from the folks over at Intoli, one of the awesome companies providing Scrapy commercial support and longtime Scrapy fans.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Do Androids Dream of Electric Sheep?","blog_url":"https://blog.scrapinghub.com/2017/06/19/do-androids-dream-of-electric-sheep","blog_date":"June 19, 2017","blog_author":"Mikhail Korobov","blog_summary":"It got very easy to do Machine Learning: you install a ML library like scikit-learn or xgboost, choose an estimator, feed it some training data, and get a model which can be used for predictions.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Deploy your Scrapy Spiders from GitHub","blog_url":"https://blog.scrapinghub.com/2017/04/19/deploy-your-scrapy-spiders-from-github","blog_date":"April 19, 2017","blog_author":"Valdir Stumm Jr","blog_summary":"Up until now, your deployment process using Scrapy Cloud has probably been something like this: code and test your spiders locally, commit and push your changes to a GitHub repository, and finally deploy them to Scrapy Cloud using shub deploy. However, having the development and the deployment processes in isolated steps might bring you some issues, such as unversioned and outdated code running...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Looking Back at 2016","blog_url":"https://blog.scrapinghub.com/2017/01/01/looking-back-at-2016","blog_date":"January 01, 2017","blog_author":"Cecilia Haynes","blog_summary":"We started 2016 with an eye on blowing 2015 out of the water. Mission accomplished.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Increase Sales with Online Reputation Management","blog_url":"https://blog.scrapinghub.com/2016/12/15/how-to-increase-sales-with-online-reputation-management","blog_date":"December 15, 2016","blog_author":"Cecilia Haynes","blog_summary":"One negative review can cost your business up to 22% of its prospects. This was one of the sobering findings in a study highlighted on Moz last year. With over half of shoppers rating reviews as important in their buying decision, no company large or small can afford to ignore stats like these - let alone the reviews themselves. In what follows I'll let you in on how web scraping can help you...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Build your own Price Monitoring Tool","blog_url":"https://blog.scrapinghub.com/2016/11/24/how-to-build-your-own-price-monitoring-tool","blog_date":"November 24, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Computers are great at repetitive tasks. They don't get distracted, bored, or tired. Automation is how you should be approaching tedious tasks that are absolutely essential to becoming a successful business or when carrying out mundane responsibilities. Price monitoring, for example, is a practice that every company should be doing, and is a task that readily lends itself to automation.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How You Can Use Web Data to Accelerate Your Startup","blog_url":"https://blog.scrapinghub.com/2016/11/10/how-you-can-use-web-data-to-accelerate-your-startup","blog_date":"November 10, 2016","blog_author":"Cecilia Haynes","blog_summary":"In just the US alone, there were 27 million individuals running or starting a new business in 2015. With this fiercely competitive startup scene, business owners need to take advantage of every resource available, especially given a high probability of failure. Enter web data. Web data is abundant and those who harness it can do everything from keeping an eye on competitors to ensuring customer...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"An Introduction to XPath: How to Get Started","blog_url":"https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples","blog_date":"October 27, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"XPath is a powerful language that is often used for scraping the web. It allows you to select nodes or compute values from an XML or HTML document and is actually one of the languages that you can use to extract web data using Scrapy. The other is CSS and while CSS selectors are a popular choice, XPath can actually allow you to do more.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Why Promoting Open Data Increases Economic Opportunities","blog_url":"https://blog.scrapinghub.com/2016/10/19/why-promoting-open-data-increases-economic-opportunities","blog_date":"October 19, 2016","blog_author":"Cecilia Haynes","blog_summary":"During the 2016 Collision Conference held in New Orleans, our Content Strategist Cecilia Haynes interviewed conference speaker Dr. Tyrone Grandison. At the time of the interview, he was the Deputy Chief Data Officer at the U.S. Department of Commerce. Tyrone is currently the Chief Information Officer for the Institute for Health Metrics and Evaluation.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Interview: How Up Hail uses Scrapy to Increase Transparency","blog_url":"https://blog.scrapinghub.com/2016/10/06/interview-how-up-hail-uses-scrapy-to-increase-transparency","blog_date":"October 06, 2016","blog_author":"Cecilia Haynes","blog_summary":"During the 2016 Collision Conference held in New Orleans, Scrapinghub Content Strategist Cecilia Haynes had the opportunity to interview the brains and the brawn behind Up Hail, the rideshare comparison app.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Run Python Scripts in Scrapy Cloud","blog_url":"https://blog.scrapinghub.com/2016/09/28/how-to-run-python-scripts-in-scrapy-cloud","blog_date":"September 28, 2016","blog_author":"Elias Dorneles","blog_summary":"You can deploy, run, and maintain control over your Scrapy spiders in Scrapy Cloud, our production environment. Keeping control means you need to be able to know what’s going on with your spiders and to find out early if they are in trouble.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Embracing the Future of Work: How To Communicate Remotely","blog_url":"https://blog.scrapinghub.com/2016/09/22/embracing-the-future-of-work-how-to-communicate-remotely","blog_date":"September 22, 2016","blog_author":"Cecilia Haynes","blog_summary":"What does “the Future of Work” mean to you? To us, it describes how we approach life at Scrapinghub. We don't work in a traditional office (we're 100% distributed) and we allow folks the freedom to make their own schedules (you know when you work best). By finding ways to break away from the traditional 9-to-5 mode, we ended up creating a framework for the Future of Work.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Deploy Custom Docker Images for Your Web Crawlers","blog_url":"https://blog.scrapinghub.com/2016/09/08/how-to-deploy-custom-docker-images-for-your-web-crawlers","blog_date":"September 08, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"[UPDATE]: Please see this article for an up-to-date version.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Improved Frontera: Web Crawling at Scale with Python 3 Support","blog_url":"https://blog.scrapinghub.com/2016/09/01/improved-frontera-web-crawling-at-scale-with-python-3-support","blog_date":"September 01, 2016","blog_author":"Alexander Sibiryakov","blog_summary":"Python is our go-to language of choice and Python 2 is losing traction. In order to survive, older programs need to be Python 3 compatible.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Crawl the Web Politely with Scrapy","blog_url":"https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy","blog_date":"August 25, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do NOT harm the website. We’re supporters of the democratization of web data, but not at the expense of the website’s owners.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Scrapy Cloud with Python 3 Support","blog_url":"https://blog.scrapinghub.com/2016/08/17/introducing-scrapy-cloud-with-python-3-support","blog_date":"August 17, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"It’s the end of an era. Python 2 is on its way out with only a few security and bug fixes forthcoming from now until its official retirement in 2020. Given this withdrawal of support and the fact that Python 3 has snazzier features, we are thrilled to announce that Scrapy Cloud now officially supports Python 3.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"What the Suicide Squad Tells Us About Web Data","blog_url":"https://blog.scrapinghub.com/2016/08/11/what-the-suicide-squad-tells-us-about-web-data","blog_date":"August 11, 2016","blog_author":"Cecilia Haynes","blog_summary":"Web data is a bit like the Matrix. It’s all around us, but not everyone knows how to use it meaningfully. So here’s a brief overview of the many ways that web data can benefit you as a researcher, marketer, entrepreneur, or even multinational business owner.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"This Month in Open Source at Scrapinghub August 2016","blog_url":"https://blog.scrapinghub.com/2016/08/04/this-month-in-open-source-at-scrapinghub-august-2016","blog_date":"August 04, 2016","blog_author":"Paul Tremberth","blog_summary":"Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Meet Parsel: the Selector Library behind Scrapy","blog_url":"https://blog.scrapinghub.com/2016/07/28/meet-parsel-the-selector-library-behind-scrapy","blog_date":"July 28, 2016","blog_author":"Elias Dorneles","blog_summary":"We eat our own spider food since Scrapy is our go-to workhorse on a daily basis. However, there are certain situations where Scrapy can be overkill and that’s when we use Parsel. Parsel is a Python library for extracting data from XML/HTML text using CSS or XPath selectors. It powers the scraping API of the Scrapy framework.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Incremental Crawls with Scrapy and DeltaFetch","blog_url":"https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016","blog_date":"July 20, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we’ve run into every obstacle you can imagine so don’t worry, you’re in great hands. Feel free to reach out to us on Twitter or Facebook with any suggestions for future topics.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Improving Access to Peruvian Congress Bills with Scrapy","blog_url":"https://blog.scrapinghub.com/2016/07/13/improving-access-to-peruvian-congress-bills-with-scrapy","blog_date":"July 13, 2016","blog_author":"Carlos Peña","blog_summary":"Many governments worldwide have laws enforcing them to publish their expenses, contracts, decisions, and so forth, on the web. This is so the general public can monitor what their representatives are doing on their behalf.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapely: The Brains Behind Portia Spiders","blog_url":"https://blog.scrapinghub.com/2016/07/07/scrapely-the-brains-behind-portia-spiders","blog_date":"July 07, 2016","blog_author":"Ruairi Fahy","blog_summary":"Unlike Portia labiata, the hunting spider that feeds on other spiders, our Portia feeds on data. Considered the Einstein in the spider world, we modeled our own creation after the intelligence and visual abilities of its arachnid namesake.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Portia2Code: Portia Projects into Scrapy Spiders","blog_url":"https://blog.scrapinghub.com/2016/06/29/introducing-portia2code-portia-projects-into-scrapy-spiders","blog_date":"June 29, 2016","blog_author":"Ruairi Fahy","blog_summary":"We’re thrilled to announce the release of our latest tool, Portia2Code!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scraping Infinite Scrolling Pages","blog_url":"https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016","blog_date":"June 22, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we’ve run into every obstacle you can imagine so don’t worry, you’re in great hands. Feel free to reach out to us on Twitter or Facebook with any suggestions for future topics.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"This Month in Open Source at Scrapinghub June 2016","blog_url":"https://blog.scrapinghub.com/2016/06/15/this-month-in-open-source-at-scrapinghub-june-2016","blog_date":"June 15, 2016","blog_author":"Paul Tremberth","blog_summary":"Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing the Datasets Catalog","blog_url":"https://blog.scrapinghub.com/2016/06/09/introducing-the-datasets-catalog","blog_date":"June 09, 2016","blog_author":"Cecilia Haynes","blog_summary":"Folks using Portia and Scrapy are engaged in a variety of fascinating web crawling projects, so we wanted to provide you with a way to share your data extraction prowess with the world.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing the Crawlera Dashboard","blog_url":"https://blog.scrapinghub.com/2016/06/01/introducing-the-crawlera-dashboard","blog_date":"June 01, 2016","blog_author":"Cecilia Haynes","blog_summary":"We’ve been rolling out a lot of updates, upgrades, and new features lately, and we’re continuing this trend by announcing the very first Crawlera Dashboard!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Data Extraction with Scrapy and Python 3","blog_url":"https://blog.scrapinghub.com/2016/05/25/data-extraction-with-scrapy-and-python-3","blog_date":"May 25, 2016","blog_author":"Paul Tremberth","blog_summary":"Scrapy 1.1 Release with Official Python 3 Support \nFasten your seat belts, ladies and gentlemen: Scrapy 1.1 with Python 3 support is officially out! After a couple months of hard work and four release candidates, this is the first official Scrapy release to support Python 3.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Debug your Scrapy Spiders","blog_url":"https://blog.scrapinghub.com/2016/05/18/scrapy-tips-from-the-pros-may-2016-edition","blog_date":"May 18, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to Scrapy Tips from the Pros! Every month we release a few tricks and hacks to help speed up your web scraping and data extraction activities. As the lead Scrapy maintainers, we have run into every obstacle you can imagine so don’t worry, you’re in great hands. Feel free to reach out to us on Twitter or Facebook with suggestions for future topics.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy + MonkeyLearn: Textual Analysis of Web Data","blog_url":"https://blog.scrapinghub.com/2016/05/11/monkeylearn-addon-retail-classifier-tutorial","blog_date":"May 11, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"We recently announced our integration with MonkeyLearn, bringing machine learning to Scrapy Cloud. MonkeyLearn offers numerous text analysis services via its API. Since there are so many uses to this platform addon, we’re launching a series of tutorials to help get you started.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Scrapy Cloud 2.0","blog_url":"https://blog.scrapinghub.com/2016/05/04/introducing-scrapy-cloud-2-0","blog_date":"May 04, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Scrapy Cloud has been with Scrapinghub since the beginning, but we decided some spring cleaning was in order. To that end, we’re proud to announce Scrapy Cloud 2.0!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"A (not so) Short Story on Getting Decent Internet Access","blog_url":"https://blog.scrapinghub.com/2016/04/27/a-not-so-short-story-on-getting-decent-internet-access","blog_date":"April 27, 2016","blog_author":"Agustin","blog_summary":"This is a tale of trial, tribulation, and triumph. It is the story of how I overcame obstacles including an inconveniently placed grove of eucalyptus trees, armed with little more than a broom and a pair of borrowed binoculars, to establish a stable internet connection.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scraping Websites Based on ViewStates with Scrapy","blog_url":"https://blog.scrapinghub.com/2016/04/20/scrapy-tips-from-the-pros-april-2016-edition","blog_date":"April 20, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to the April Edition of Scrapy Tips from the Pros. Each month we’ll release a few tricks and hacks that we’ve developed to help make your Scrapy workflow go more smoothly.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Machine Learning with Web Scraping: New MonkeyLearn Addon","blog_url":"https://blog.scrapinghub.com/2016/04/14/grok-your-data-with-the-new-monkeylearn-addon","blog_date":"April 14, 2016","blog_author":"Cecilia Haynes","blog_summary":"Say Hello to the MonkeyLearn Addon \nWe deal in data. Vast amounts of it. But while we’ve been traditionally involved in providing you with the data that you need, we are now taking it a step further by helping you analyze it as well.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Mapping Corruption in the Panama Papers with Open Data","blog_url":"https://blog.scrapinghub.com/2016/04/06/how-to-become-a-whistleblower-from-panama-papers-to-open-data","blog_date":"April 06, 2016","blog_author":"Cecilia Haynes","blog_summary":"We are at a point in the digital age where corruption is increasingly difficult to hide. Information leaks are abundant and shocking.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Web Scraping to Create Open Data","blog_url":"https://blog.scrapinghub.com/2016/03/30/web-scraping-to-create-open-data","blog_date":"March 30, 2016","blog_author":"Lluís Esquerda","blog_summary":"Open data is the idea that some data should be freely available to everyone to use and republish as they wish, without restrictions from copyright, patents or other mechanisms of control. \n\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy Tips from the Pros: March 2016 Edition","blog_url":"https://blog.scrapinghub.com/2016/03/23/scrapy-tips-from-the-pros-march-2016-edition","blog_date":"March 23, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to the March Edition of Scrapy Tips from the Pros! Each month we’ll release a few tips and hacks that we’ve developed to help make your Scrapy workflow go more smoothly.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"This Month in Open Source at Scrapinghub March 2016","blog_url":"https://blog.scrapinghub.com/2016/03/16/this-month-in-open-source-at-scrapinghub-march-2016","blog_date":"March 16, 2016","blog_author":"Paul Tremberth","blog_summary":"Welcome to This Month in Open Source at Scrapinghub! In this monthly column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Join Scrapinghub for Google Summer of Code 2016","blog_url":"https://blog.scrapinghub.com/2016/03/14/join-scrapinghub-for-google-summer-of-code-2016","blog_date":"March 14, 2016","blog_author":"Cecilia Haynes","blog_summary":"Read More"},{"blog_title":"How Web Scraping is Revealing Lobbying and Corruption in Peru","blog_url":"https://blog.scrapinghub.com/2016/03/09/how-web-scraping-is-revealing-lobbying-and-corruption-in-peru","blog_date":"March 09, 2016","blog_author":"Carlos Peña","blog_summary":"Update: With the release of the Panama Papers, a reliable means of exposing corruption and the methods of money laundering and tax evasion are now even more important. Web scraping provides an avenue to find, collate, and organize data without relying on information leaks.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Splash 2.0 Is Here with Qt 5 and Python 3","blog_url":"https://blog.scrapinghub.com/2016/02/29/splash-2-0-here-with-qt-5-and-python-3","blog_date":"February 29, 2016","blog_author":"Richard Dowinton","blog_summary":"We’re pleased to announce that Splash 2.0 is officially live after many months of hard work.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Migrate your Kimono Projects to Portia","blog_url":"https://blog.scrapinghub.com/2016/02/25/migrate-your-kimono-projects-to-portia","blog_date":"February 26, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Heads up, Kimono Labs users!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy Tips from the Pros: February 2016 Edition","blog_url":"https://blog.scrapinghub.com/2016/02/24/scrapy-tips-from-the-pros-february-2016-edition","blog_date":"February 24, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Welcome to the February Edition of Scrapy Tips from the Pros. Each month we’ll release a few tips and hacks that we’ve developed to help make your Scrapy workflow go more smoothly.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Portia: The Open Source Alternative to Kimono Labs","blog_url":"https://blog.scrapinghub.com/2016/02/17/portia-alternative-to-kimono","blog_date":"February 17, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Attention Kimono users: we've created an exporter so you can easily convert your projects from Kimono to Portia! \n \nImagine your business depended heavily on a third party tool and one day that company decided to shut down its service with only 2 weeks notice. That, unfortunately, is what happened to users of Kimono Labs yesterday. \nAnd it’s one of the many reasons why we love open source so much. \n...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Web Scraping Finds Stores Guilty of Price Inflation","blog_url":"https://blog.scrapinghub.com/2016/02/10/which-stores-are-guilty-of-price-inflation","blog_date":"February 10, 2016","blog_author":"Cecilia Haynes","blog_summary":"This is the last post ending a series of articles that have traced the prices of some of the top gifts, gadgets, and gizmos from Black Friday 2015 through to January 2016. \n\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Python 3 is Coming to Scrapy","blog_url":"https://blog.scrapinghub.com/2016/02/04/python-3-support-with-scrapy-1-1rc1","blog_date":"February 04, 2016","blog_author":"Richard Dowinton","blog_summary":"Read More"},{"blog_title":"Happy Anniversary: Scrapinghub Turns 5","blog_url":"https://blog.scrapinghub.com/2016/01/27/happy-anniversary-scrapinghub-turns-5","blog_date":"January 27, 2016","blog_author":"Cecilia Haynes","blog_summary":"Birthdays are a big deal at Scrapinghub. We always make sure to celebrate each team member on their special day, recognizing achievements and sending well-wishes for the year to come. Well, on December 15, 2015, we celebrated one of the most momentous birthdays of the year: our own. We are officially 5 years old and what an amazing 5 years it has been.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy Tips from the Pros: Part 1","blog_url":"https://blog.scrapinghub.com/2016/01/19/scrapy-tips-from-the-pros-part-1","blog_date":"January 19, 2016","blog_author":"Valdir Stumm Jr","blog_summary":"Scrapy is at the heart of Scrapinghub. We use this framework extensively and have accumulated a wide range of shortcuts to get around common problems. We’re launching a series to share these Scrapy tips with you so that you can get the most out of your daily workflow. Each post will feature two to three tips, so stay tuned.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Vizlegal: Rise of Machine-Readable Laws and Court Judgments","blog_url":"https://blog.scrapinghub.com/2016/01/13/vizlegal-rise-of-machine-readable-laws-and-court-judgments","blog_date":"January 13, 2016","blog_author":"vizlegal","blog_summary":"Guest Post by Gavin Sheridan, founder/CEO of Vizlegal.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Christmas Eve vs New Year's Eve: Last Minute Price Inflation?","blog_url":"https://blog.scrapinghub.com/2016/01/06/christmas-eve-vs-new-years-eve-last-minute-price-inflation","blog_date":"January 06, 2016","blog_author":"Cecilia Haynes","blog_summary":"Check out Part 1 comparing Black Friday and Cyber Monday and Part 2 comparing Black Friday, Cyber Monday, and Green Monday\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Looking Back at 2015","blog_url":"https://blog.scrapinghub.com/2015/12/31/looking-back-at-2015","blog_date":"December 31, 2015","blog_author":"Cecilia Haynes","blog_summary":"2015 has been a standout year for Scrapinghub with new developments in expanded products, noteworthy clients, and cherished partnerships. We were curious about whether we could continue our growth from 2014 and are pleased to report that our curiosity has been successfully laid to rest. So without further ado, let’s dive straight into what has made 2015 a hard act to follow.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Winter Sales Showdown: Black Friday vs Cyber Monday vs Green Monday","blog_url":"https://blog.scrapinghub.com/2015/12/21/winter-sales-showdown-black-friday-vs-cyber-monday-vs-green-monday","blog_date":"December 21, 2015","blog_author":"Cecilia Haynes","blog_summary":"Check out Part 1 comparing Black Friday and Cyber Monday\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Chats With RINAR Solutions","blog_url":"https://blog.scrapinghub.com/2015/12/16/chats-with-rinar-solutions","blog_date":"December 16, 2015","blog_author":"Cecilia Haynes","blog_summary":"Meet Tomás Rinke. He is the CTO and Co-Founder of RINAR Solutions, a startup that provides data consulting services to inform decision making. He is an avid Scrapy user and a Scrapinghub development partner. As an off-shoot of RINAR Solutions, he developed DataJudicial, an app that provides information on the legal sector.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Black Friday, Cyber Monday: Are They Worth It?","blog_url":"https://blog.scrapinghub.com/2015/12/03/black-friday-cyber-monday-are-they-worth-it","blog_date":"December 03, 2015","blog_author":"Cecilia Haynes","blog_summary":"This post kicks off a series of articles that will trace the prices of some of the top gifts, gadgets, and gizmos from Black Friday through to January 2016.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Tips for Creating a Cohesive Company Culture Remotely","blog_url":"https://blog.scrapinghub.com/2015/11/16/tips-for-creating-a-cohesive-company-culture-remotely","blog_date":"November 16, 2015","blog_author":"Cecilia Haynes","blog_summary":"We are growing. Rapidly. This past year, we have gone from 80 to 114 Scrapinghubbers. Companies expanding so rapidly can risk losing what made them them in the first place. We don’t want that to happen, especially since we have the added challenge of being an entirely distributed company with remote coworkers scattered around the world in 36 countries.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Parse Natural Language Dates with Dateparser","blog_url":"https://blog.scrapinghub.com/2015/11/09/parse-natural-language-dates-with-dateparser","blog_date":"November 09, 2015","blog_author":"Richard Dowinton","blog_summary":"We recently released Dateparser 0.3.1 with support for Belarusian and Indonesian, as well as the Jalali calendar used in Iran and Afghanistan. With this in mind, we’re taking the opportunity to introduce and demonstrate the features of Dateparser.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Aduana: Link Analysis to Crawl the Web at Scale","blog_url":"https://blog.scrapinghub.com/2015/09/29/aduana-link-analysis-to-crawl-the-web-at-scale","blog_date":"September 29, 2015","blog_author":"Pedro López-Adeva","blog_summary":"Crawling vast numbers of websites for specific types of information is impractical. Unless, that is, you prioritize what you crawl. Aduana is an experimental tool that we developed to help you do that. It’s a special backend for Frontera, our tool to expedite massive crawls in parallel (primer here).\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy on the Road to Python 3 Support","blog_url":"https://blog.scrapinghub.com/2015/08/19/scrapy-on-the-road-to-python-3-support","blog_date":"August 19, 2015","blog_author":"Giovanni Piazza","blog_summary":"Scrapy is one of the few popular Python packages (almost 10k github stars) that's not yet compatible with Python 3. The team and community around it are working to make it compatible as soon as possible. Here's an overview of what has been happening so far.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Javascript support for Portia","blog_url":"https://blog.scrapinghub.com/2015/08/19/introducing-javascript-support-for-portia","blog_date":"August 19, 2015","blog_author":"Ruairi Fahy","blog_summary":"Today we released the latest version of Portia bringing with it the ability to crawl pages that require JavaScript. To celebrate this release we are making Splash available as a free trial to all Portia users so you can try it out with your projects.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Distributed Frontera: Web Crawling at Scale","blog_url":"https://blog.scrapinghub.com/2015/08/05/distributed-frontera-web-crawling-at-large-scale","blog_date":"August 06, 2015","blog_author":"Alexander Sibiryakov","blog_summary":"Welcome to Distributed Frontera \nThis past year, we have been working on a distributed version of our crawl frontier framework, Frontera. This work was partially funded by DARPA and is included in the DARPA Open Catalog.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"The Road to Loading JavaScript in Portia","blog_url":"https://blog.scrapinghub.com/2015/08/03/the-road-to-loading-javascript-in-portia","blog_date":"August 03, 2015","blog_author":"Ruairi Fahy","blog_summary":"Support for JavaScript has been a much requested feature ever since Portia’s first release 2 years ago. The wait is nearly over and we are happy to inform you that we will be launching these changes in the very near future. If you’re feeling adventurous you can try it out on the develop branch at Github. This post aims to highlight the path we took to achieving JavaScript support in Portia.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"EuroPython 2015","blog_url":"https://blog.scrapinghub.com/2015/07/21/europython-2015","blog_date":"July 21, 2015","blog_author":"Richard Dowinton","blog_summary":"EuroPython 2015 is happening this week and we’re having the largest company meetup so far as a part of it, with more than 30 members from our fully remote-working team attending. The event which is held in Bilbao started on Monday and is providing great quality talks, sessions and plenty of tasty Spanish dishes.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"StartupChats Remote Working Q&A","blog_url":"https://blog.scrapinghub.com/2015/07/17/startupchats-remote-working","blog_date":"July 17, 2015","blog_author":"Richard Dowinton","blog_summary":"Earlier this week, Scrapinghub was invited along with several other fully-distributed companies to participate in a remote working Q&A hosted by Startups Canada.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"PyCon Philippines 2015","blog_url":"https://blog.scrapinghub.com/2015/07/15/pycon-philippines-2015","blog_date":"July 15, 2015","blog_author":"Richard Dowinton","blog_summary":"Earlier this month we attended PyCon Philippines as a gold sponsor, presenting on the 2nd day. This was particularly exciting as it was the first time the whole Philippines team was together in one place and it was nice meeting each other in person!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Google Summer of Code 2015","blog_url":"https://blog.scrapinghub.com/2015/06/25/google-summer-of-code-2015","blog_date":"June 25, 2015","blog_author":"Pablo Hoffman","blog_summary":"We are very excited to be participating again this year on Google Summer of Code. After a successful experience last year where Julia Medina (now a proud Scrapinghubber!) worked on Scrapy API cleanup and per-spider settings, we are back again this year with 3 ideas approved:\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Link Analysis Algorithms Explained","blog_url":"https://blog.scrapinghub.com/2015/06/19/link-analysis-algorithms-explained","blog_date":"June 19, 2015","blog_author":"Richard Dowinton","blog_summary":"When scraping content from the web, you often crawl websites which you have no prior knowledge of. Link analysis algorithms are incredibly useful in these scenarios to guide the crawler to relevant pages.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"EuroPython, here we go!","blog_url":"https://blog.scrapinghub.com/2015/06/12/europython-here-we-go","blog_date":"June 12, 2015","blog_author":"Giovanni Piazza","blog_summary":"We are very excited about EuroPython 2015!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Aduana: Link Analysis with Frontera","blog_url":"https://blog.scrapinghub.com/2015/06/08/aduana-link-analysis-with-frontera-2","blog_date":"June 08, 2015","blog_author":"Richard Dowinton","blog_summary":"It's not uncommon to need to crawl a large number of unfamiliar websites when gathering content. Page ranking algorithms are incredibly useful in these scenarios as it can be tricky to determine which pages are relevant to the content you're looking for.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Using git to manage vacations in a large distributed team","blog_url":"https://blog.scrapinghub.com/2015/06/08/git-for-managing-vacations","blog_date":"June 08, 2015","blog_author":"Pablo Hoffman","blog_summary":"Here at Scrapinghub we are a remote team of 100+ engineers distributed among 30+ countries. As part of their standard contract, Scrapinghubbers get 20 vacation days per year and local country holidays off, and yet we spent almost zero time managing this. How do we do it?. The answer is “git” and here we explain how.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Gender Inequality Across Programming Languages","blog_url":"https://blog.scrapinghub.com/2015/05/27/gender-inequality-across-programming-languages","blog_date":"May 27, 2015","blog_author":"Richard Dowinton","blog_summary":"Gender inequality is a hot topic in the tech industry. Over the last several years we’ve gathered business profiles for our clients, and we realised this data would prove useful in identifying trends in how gender and employment relate to one another.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Traveling Tips for Remote Workers","blog_url":"https://blog.scrapinghub.com/2015/05/12/traveling-tips-for-remote-workers","blog_date":"May 12, 2015","blog_author":"Giovanni Piazza","blog_summary":"Being free to work from wherever you feel like, no boundaries holding you to a specific place or country. This is one of the greatest advantages of working remotely, and it's leading many people to travel around the globe while completing their work. Today Claudio Salazar, a Scrapinghubber from Chile, is here to share his experiences and tips for these who seek working on the road.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"A Career in Remote Working","blog_url":"https://blog.scrapinghub.com/2015/04/28/a-career-in-remote-working","blog_date":"April 28, 2015","blog_author":"Richard Dowinton","blog_summary":"This year I have reached a major milestone in my life, which is getting my bachelor's degree in mathematics. When I made the decision to go back to college, it was solely because my experience working at Scrapinghub, I figured out that having a math background would be a great foundation for getting into ML-related stuff. \n\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Frontera: The Brain Behind the Crawls","blog_url":"https://blog.scrapinghub.com/2015/04/22/frontera-the-brain-behind-the-crawls","blog_date":"April 22, 2015","blog_author":"Richard Dowinton","blog_summary":"At Scrapinghub we're always building and running large crawls–last year we had 11 billion requests made on Scrapy Cloud alone. Crawling millions of pages from the internet requires more sophistication than getting a few contacts of a list, as we need to make sure that we get reliable data, up to date lists of item pages and are able to optimise our crawl as much as possible.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrape Data Visually with Portia and Scrapy Cloud","blog_url":"https://blog.scrapinghub.com/2015/04/07/scrape-data-visually-with-portia-and-scrapy-cloud","blog_date":"April 07, 2015","blog_author":"Richard Dowinton","blog_summary":"It’s been several months since we first integrated Portia into our Scrapy Cloud platform, and last week we officially began to phase out Autoscraping in favor of Portia.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapinghub: A Remote Working Success Story","blog_url":"https://blog.scrapinghub.com/2015/03/17/scrapinghub-remote-working-success-story","blog_date":"March 17, 2015","blog_author":"Joanne O'Flynn","blog_summary":"When Scrapinghub came into the world in 2010, one thing we wanted was for it to be a company which could be powered by a global workforce, each individual working remotely from anywhere in the world. Reduced commuting time and as a consequence increased family time were the primary reasons for this. In Uruguay, Pablo was commuting long distances to do work which realistically could have be done...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Why we moved to Slack","blog_url":"https://blog.scrapinghub.com/2015/03/16/why-we-moved-to-slack","blog_date":"March 16, 2015","blog_author":"Pablo Hoffman","blog_summary":"We are veterans in the chat group arena. We have been using one form of another since we started Scrapinghub in 2010 and I've been personally using corporate group chats since 2004. We started Scrapinghub using our own hosted version of ejabberd, then moved to HipChat in 2013 and we just finished moving to Slack. Thanks to Slack's migration tools, the process went pretty smoothly. In this post...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"The History of Scrapinghub","blog_url":"https://blog.scrapinghub.com/2015/03/16/history-of-scrapinghub","blog_date":"March 16, 2015","blog_author":"Joanne O'Flynn","blog_summary":"Joanne O’Flynn meets with Pablo Hoffman and Shane Evans to find out what inspired them to set up web crawling company Scrapinghub.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Skinfer: A Tool for Inferring JSON Schemas","blog_url":"https://blog.scrapinghub.com/2015/03/04/skinfer-a-tool-for-inferring-json-schemas","blog_date":"March 05, 2015","blog_author":"Elias Dorneles","blog_summary":"Imagine that you have a lot of samples for a certain kind of data in JSON format. Maybe you want to have a better feel of it, know which fields appear in all records, which appear only in some and what are their types. In other words, you want to know the schema for the data that you have.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Handling JavaScript in Scrapy with Splash","blog_url":"https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash","blog_date":"March 02, 2015","blog_author":"Richard Dowinton","blog_summary":"A common roadblock when developing spiders is dealing with sites that use a heavy amount of JavaScript. Many modern websites run entirely on JavaScript, and require scripts to be run in order for the page to render properly. In many cases, pages also present modals and other dialogues that need to be interacted with to show the full page. In this post we’re going to show you how you can use...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapinghub Crawls the Deep Web","blog_url":"https://blog.scrapinghub.com/2015/02/24/memex","blog_date":"February 24, 2015","blog_author":"Shane Evans","blog_summary":"\"The easiest way to think about Memex is: How can I make the unseen seen?\" \n\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"New Changes to Our Scrapy Cloud Platform","blog_url":"https://blog.scrapinghub.com/2015/01/22/new-changes-to-our-scrapy-cloud-platform","blog_date":"January 23, 2015","blog_author":"Richard Dowinton","blog_summary":"We are proud to announce some exciting changes we've introduced this week. These changes bring a much more pleasant user experience, and several new features including the addition of Portia to our platform!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing ScrapyRT: An API for Scrapy spiders","blog_url":"https://blog.scrapinghub.com/2015/01/22/introducing-scrapyrt-an-api-for-scrapy-spiders","blog_date":"January 22, 2015","blog_author":"Richard Dowinton","blog_summary":"We’re proud to announce our new open source project, ScrapyRT! ScrapyRT, short for Scrapy Real Time, allows you to extract data from a single web page via an API using your existing Scrapy spiders.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Looking Back at 2014","blog_url":"https://blog.scrapinghub.com/2014/12/31/looking-back-at-2014","blog_date":"December 31, 2014","blog_author":"Breno Colom","blog_summary":"One year ago we were looking back at the great 2013 we had and realized we would have quite a big challenge in front of us in order to have as much growth as we had during last year. So here are some highlights of the things we’ve been up to during this year, let's see how well we did!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"XPath Tips from the Web Scraping Trenches","blog_url":"https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches","blog_date":"July 17, 2014","blog_author":"Elias Dorneles","blog_summary":"In the context of web scraping, XPath is a nice tool to have in your belt, as it allows you to write specifications of document locations more flexibly than CSS selectors. In case you're looking for a tutorial, here is a XPath tutorial with nice examples.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Data Reviews","blog_url":"https://blog.scrapinghub.com/2014/06/26/introducing-data-reviews","blog_date":"June 27, 2014","blog_author":"Breno Colom","blog_summary":"One of the things that takes more time when building a spider is reviewing the scraped data and making sure it conforms to the requirements and expectations of your client or team. This process is so time consuming that, in many cases, it ends up taking more time than writing the spider code itself, depending on how well the requirements are written. To make this process more efficient we have...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Extracting schema.org Microdata Using Scrapy Selectors and XPath","blog_url":"https://blog.scrapinghub.com/2014/06/18/extracting-schema-org-microdata-using-scrapy-selectors-and-xpath","blog_date":"June 18, 2014","blog_author":"Paul Tremberth","blog_summary":"EDIT: 2015-10-30\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Announcing Portia, the Open Source Visual Web Scraper!","blog_url":"https://blog.scrapinghub.com/2014/04/01/announcing-portia","blog_date":"April 01, 2014","blog_author":"Shane Evans","blog_summary":"We’re proud to announce the developer release of Portia, our new open source visual scraping tool based on Scrapy. Check out this video:\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Optimizing Memory Usage of Scikit-Learn Models Using Succinct Tries","blog_url":"https://blog.scrapinghub.com/2014/03/26/optimizing-memory-usage-of-scikit-learn-models-using-succinct-tries","blog_date":"March 26, 2014","blog_author":"Mikhail Korobov","blog_summary":"We use the scikit-learn library for various machine-learning tasks at Scrapinghub. For example, for text classification we'd typically build a statistical model using sklearn's Pipeline, FeatureUnion, some classifier (e.g. LinearSVC) + feature extraction and preprocessing classes. The model is usually trained on a developers machine, then serialized (using pickle/joblib) and uploaded to a server...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Open Source at Scrapinghub","blog_url":"https://blog.scrapinghub.com/2014/01/18/open-source-at-scrapinghub","blog_date":"January 18, 2014","blog_author":"Pablo Hoffman","blog_summary":"Here at Scrapinghub we love open source. We love using and contributing to it. Over these years we have open sourced a few projects, that we keep using over and over, in the hope that it will make others lives easier. Writing reusable code is harder than it sounds, but it enforces good practices such as documenting accurately, testing extensively and worrying about backwards support. In the end...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Looking Back at 2013","blog_url":"https://blog.scrapinghub.com/2013/12/31/looking-back-at-2013","blog_date":"December 31, 2013","blog_author":"Shane Evans","blog_summary":"This time last year Pablo and I were chatting about the previous year and what to expect in 2013. I noticed that our team had almost doubled in size in the previous year and we wondered could that possibly continue in 2013?\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Marcos Campal Is a ScrapingHubber!","blog_url":"https://blog.scrapinghub.com/2013/10/01/marcos-campal-is-a-scrapinghubber","blog_date":"October 01, 2013","blog_author":"Pablo Hoffman","blog_summary":"We’re excited to welcome Marcos Campal to the Scrapinghub engineering team.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Dash","blog_url":"https://blog.scrapinghub.com/2013/07/26/introducing-dash","blog_date":"July 27, 2013","blog_author":"Shane Evans","blog_summary":"We're excited to introduce Dash, a major update to our scraping platform.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Why MongoDB Is a Bad Choice for Storing Our Scraped Data","blog_url":"https://blog.scrapinghub.com/2013/05/13/mongo-bad-for-scraped-data","blog_date":"May 13, 2013","blog_author":"Shane Evans","blog_summary":"MongoDB was used early on at Scrapinghub to store scraped data because it's convenient. Scraped data is represented as (possibly nested) records which can be serialized to JSON. The schema is not known ahead of time and may change from one job to the next. We need to support browsing, querying and downloading the stored data. This was very easy to implement using MongoDB (easier than the...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing Crawlera, a Smart Page Downloader","blog_url":"https://blog.scrapinghub.com/2013/05/10/introducing-crawlera","blog_date":"May 11, 2013","blog_author":"Pablo Hoffman","blog_summary":"We are proud to introduce Crawlera, a smart web downloader designed specifically for web crawling.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Git Workflow for Scrapy Projects","blog_url":"https://blog.scrapinghub.com/2013/03/06/git-workflow-scrapy-projects","blog_date":"March 06, 2013","blog_author":"Pablo Hoffman","blog_summary":"Our customers often ask us what's the best workflow for working with Scrapy projects. A popular approach we have seen and used in the past is to split the spiders folder (typically project/spiders) into two folders: project/spiders_prod and project/spiders_dev, and use the SPIDER_MODULES setting to control which spiders are loaded on each environment. This works reasonably well, until you have to...\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"How to Fill Login Forms Automatically","blog_url":"https://blog.scrapinghub.com/2012/10/26/filling-login-forms-automatically","blog_date":"October 26, 2012","blog_author":"Pablo Hoffman","blog_summary":"We often have to write spiders that need to login to sites, in order to scrape data from them. Our customers provide us with the site, username and password, and we do the rest.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Spiders activity graphs","blog_url":"https://blog.scrapinghub.com/2012/08/24/spiders-activity-graphs","blog_date":"August 25, 2012","blog_author":"Pablo Hoffman","blog_summary":"Today we are introducing a new feature called Spider activity graphs. These allow you to visualize quickly how your spiders are working, and it's a very useful tool for busy projects to find out which spiders are not working as expected.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Finding Similar Items","blog_url":"https://blog.scrapinghub.com/2012/07/23/finding-similar-items","blog_date":"July 23, 2012","blog_author":"Shane Evans","blog_summary":"This post describes an approach to the problem of finding near duplicates among crawled items and how this was implemented at Scrapinghub.\n\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy 0.15 dropping support for Python 2.5","blog_url":"https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5","blog_date":"February 27, 2012","blog_author":"Pablo Hoffman","blog_summary":"After a year considering it, we have decided to go ahead and drop support for Python 2.5 in Scrapy.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Autoscraping casts a wider net","blog_url":"https://blog.scrapinghub.com/2012/02/27/autoscraping-casts-a-wider-net","blog_date":"February 27, 2012","blog_author":"Shane Evans","blog_summary":"We have recently started letting more users into the private beta for our Autoscraping service. We're receiving a lot of applications following the shutdown of Needlebase and we're increasing our capacity to accommodate these users.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy 0.14 released","blog_url":"https://blog.scrapinghub.com/2011/11/17/scrapy-0-14-released","blog_date":"November 18, 2011","blog_author":"Pablo Hoffman","blog_summary":"After 10 months of work, and many changes, we are pleased to announce the release of Scrapy 0.14.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Dirbot - a new example Scrapy project","blog_url":"https://blog.scrapinghub.com/2011/04/28/dirbot-a-new-example-scrapy-project","blog_date":"April 28, 2011","blog_author":"Pablo Hoffman","blog_summary":"Scrapy users have complained in the past about the lack of a pre-built example project that contains, for example, the dmoz spider described in the tutorial.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Introducing w3lib and scrapely","blog_url":"https://blog.scrapinghub.com/2011/04/19/introducing-w3lib-and-scrapely","blog_date":"April 20, 2011","blog_author":"Pablo Hoffman","blog_summary":"In an effort to make Scrapy code smaller and more reusable, we’ve been working on splitting the Scrapy codebase into two different modules:\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Scrapy 0.12 released","blog_url":"https://blog.scrapinghub.com/2011/01/02/scrapy-0-12-released","blog_date":"January 03, 2011","blog_author":"Pablo Hoffman","blog_summary":"Hello everyone, we’re pleased to announce the release of Scrapy 0.12!\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Spoofing your Scrapy bot IP using tsocks","blog_url":"https://blog.scrapinghub.com/2010/11/12/scrapy-tsocks","blog_date":"November 12, 2010","blog_author":"Pablo Hoffman","blog_summary":"It is well known that many websites show different content depending on the region where they’re accessed. For example, some retailer sites show products available only for the region (US, Europe) of the user accessing the site.\n\n                                      \n                                        \n                                            Read More"},{"blog_title":"Hello, world","blog_url":"https://blog.scrapinghub.com/2010/06/26/hello-world","blog_date":"June 26, 2010","blog_author":"Shane Evans","blog_summary":"It's finally time to start a Scrapinghub blog! In the upcoming months we expect to open our private beta to new customers, launch new services, add many new features and continue to contribute to open source projects. It's about time we had a way to to tell everyone about all the great things that are happening!\n\n                                      \n                                        \n                                            Read More"}]
