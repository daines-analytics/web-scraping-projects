{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of Quotes from Famous People Using BeautifulSoup\n",
    "### David Lowe\n",
    "### May 19, 2019\n",
    "\n",
    "SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python and leveraged the BeautifulSoup module.\n",
    "\n",
    "INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the Python script attempts to follow the page links and scrape the quote information off each page.\n",
    "\n",
    "Starting URLs: http://quotes.toscrape.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import smtplib\n",
    "import sys\n",
    "from email.message import EmailMessage\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "startTimeScript = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the email notification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Python Web Scraping Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"The web scraping process has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed the web page: http://quotes.toscrape.com/\n"
     ]
    }
   ],
   "source": [
    "# Specifying the URL of desired web page to be scrapped\n",
    "website_url = \"http://quotes.toscrape.com\"\n",
    "starting_url = website_url + \"/\"\n",
    "\n",
    "# Creating an html document from the URL\n",
    "uastring = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.80 Safari/537.36\"\n",
    "headers={'User-Agent': uastring}\n",
    "\n",
    "try:\n",
    "    s = requests.Session()\n",
    "    resp = s.get(website_url, headers=headers)\n",
    "#     print(resp.text)\n",
    "except HTTPError as e:\n",
    "    print('The server could not serve up the web page!')\n",
    "    sys.exit(\"Script Processing Aborted!!!\")\n",
    "except ConnectionError as e:\n",
    "    print('The server could not be reached!')\n",
    "    sys.exit(\"Script Processing Aborted!!!\")\n",
    "\n",
    "try:\n",
    "    webpage = BeautifulSoup(resp.text, 'lxml')\n",
    "except AttributeError as e:\n",
    "    print('Page title could not be found - Might indicate problems!')\n",
    "    sys.exit(\"Script Processing Aborted!!!\")\n",
    "else:\n",
    "    print('Successfully accessed the web page: ' + starting_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Scraping and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"The page loading and item extraction process has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a dataframe to capture the records\n",
    "df = pd.DataFrame(columns=['author_name','quote_text','quote_tags','author_link'])\n",
    "pageNum = 1\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 3 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/2/\n",
      "Waiting 4 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/3/\n",
      "Waiting 5 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/4/\n",
      "Waiting 2 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/5/\n",
      "Waiting 3 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/6/\n",
      "Waiting 2 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/7/\n",
      "Waiting 5 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/8/\n",
      "Waiting 2 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/9/\n",
      "Waiting 2 seconds to process next page...\n",
      "Successfully accessed the web page: http://quotes.toscrape.com/page/10/\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "while not done :\n",
    "    quote_listing = webpage.find_all(\"div\", class_=\"quote\")\n",
    "#     print(quote_listing)\n",
    "\n",
    "    for quote_item in quote_listing :\n",
    "        author_name = \"[Not Found]\"\n",
    "        quote_text = \"[Not Found]\"\n",
    "        quote_tags = \"\"\n",
    "        author_link = \"[Not Found]\"\n",
    "\n",
    "        author_name = quote_item.find(\"small\", class_=\"author\").string\n",
    "        quote_text = quote_item.find(\"span\", class_=\"text\").string\n",
    "        tag_listing = quote_item.find_all(\"a\", class_=\"tag\")\n",
    "        if len(tag_listing) > 0 :\n",
    "            for each_tag in tag_listing :\n",
    "                quote_tags = quote_tags + \"#\" + each_tag.string\n",
    "        author_link = website_url + quote_item.find('a').get('href')\n",
    "\n",
    "#         print(author_name, quote_text, quote_tags, author_link)\n",
    "        df.loc[i] = [author_name, quote_text, quote_tags, author_link]\n",
    "        i = i + 1\n",
    "\n",
    "    if ((pageNum % 5)==0) :\n",
    "        email_notify(\"Finished parsing page: \" + next_page_url + \" at \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "    pageNum = pageNum + 1\n",
    "\n",
    "    next_page = webpage.find(\"li\", class_=\"next\")\n",
    "    if next_page != None :\n",
    "        next_page_url = website_url + next_page.find('a').get('href')\n",
    "\n",
    "        # Adding random wait time so we do not hammer the website needlessly\n",
    "        waitTime = randint(2,5)\n",
    "        print(\"Waiting \" + str(waitTime) + \" seconds to process next page...\")\n",
    "        sleep(waitTime)\n",
    "        try:\n",
    "            resp = s.get(next_page_url, headers=headers)\n",
    "        except HTTPError as e:\n",
    "            print(\"No more page to retrieve. The processing has completed!\")\n",
    "            done = True\n",
    "        else:\n",
    "            try:\n",
    "                webpage = BeautifulSoup(resp.text, 'lxml')\n",
    "            except AttributeError as e:\n",
    "                print('Page title could not be found - Might indicate problems!')\n",
    "                sys.exit(\"Script Processing Aborted!!!\")\n",
    "            else:\n",
    "                print('Successfully accessed the web page: ' + next_page_url)\n",
    "    else :\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data and Producing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records written to file: 100\n",
      "Total time for the script: 0:00:40.338690\n"
     ]
    }
   ],
   "source": [
    "out_file = df.to_json(orient='records')\n",
    "with open('web-scraping-py-bsoup-simple-pagination.json', 'w') as f:\n",
    "    f.write(out_file)\n",
    "print('Total number of records written to file:', len(df))\n",
    "email_notify(\"The web scraping process has completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
