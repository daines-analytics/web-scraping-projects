{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of Quotes from Famous People Using BeautifulSoup\n",
    "### David Lowe\n",
    "### May 19, 2019\n",
    "\n",
    "SUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python and leveraged the BeautifulSoup module.\n",
    "\n",
    "INTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take3 iteration, the Python script attempts to scrape the displayed quote information via an infinite scrolling page.\n",
    "\n",
    "Note: For this iteration, the website returns the data in JSON format when using the API URL format. As a result, the BeautifulSoup module is not necessary for parsing the web pages for this iteration.\n",
    "\n",
    "Starting URLs: http://quotes.toscrape.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import smtplib\n",
    "import sys\n",
    "from email.message import EmailMessage\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "startTimeScript = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the email notification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Python Web Scraping Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"The web scraping process has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the URL of desired web page to be scrapped\n",
    "api_url = 'http://quotes.toscrape.com/api/quotes?page='\n",
    "pageNum = 1\n",
    "website_url = api_url + str(pageNum)\n",
    "\n",
    "# Creating an html document from the URL\n",
    "uastring = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.80 Safari/537.36\"\n",
    "headers={'User-Agent': uastring}\n",
    "\n",
    "try:\n",
    "    s = requests.Session()\n",
    "    resp = s.get(website_url, headers=headers)\n",
    "#     print(resp.text)\n",
    "except HTTPError as e:\n",
    "    print('The server could not serve up the web page!')\n",
    "    sys.exit(\"Script Processing Aborted!!!\")\n",
    "except ConnectionError as e:\n",
    "    print('The server could not be reached!')\n",
    "    sys.exit(\"Script Processing Aborted!!!\")\n",
    "\n",
    "webpage = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Scraping and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_notify(\"The page loading and item extraction process has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a dataframe to capture the records\n",
    "df = pd.DataFrame(columns=['Author_Name','Quote_Text','Quote_Tags','Author_Link'])\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=1\n",
      "Waiting 2 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=2\n",
      "Waiting 2 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=3\n",
      "Waiting 4 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=4\n",
      "Waiting 4 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=5\n",
      "Waiting 5 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=6\n",
      "Waiting 4 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=7\n",
      "Waiting 4 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=8\n",
      "Waiting 3 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=9\n",
      "Waiting 2 seconds to process next page...\n",
      "Parsing web page for quotes: http://quotes.toscrape.com/api/quotes?page=10\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "while not done :\n",
    "    print('Parsing web page for quotes:',website_url)\n",
    "    for quote_item in webpage['quotes']:\n",
    "        author_name = \"[Not Found]\"\n",
    "        quote_text = \"[Not Found]\"\n",
    "        quote_tags = \"\"\n",
    "        author_link = \"[Not Found]\"\n",
    "\n",
    "        author_name = quote_item['author']['name']\n",
    "        quote_text = quote_item['text']\n",
    "        tag_listing = quote_item['tags']\n",
    "        if len(tag_listing) > 0 :\n",
    "            for each_tag in tag_listing :\n",
    "                quote_tags = quote_tags + \"#\" + each_tag\n",
    "        author_link = \"https://www.goodreads.com\" + quote_item['author']['goodreads_link']\n",
    "#         print(author_name, quote_text, quote_tags, author_link)\n",
    "\n",
    "        df.loc[i] = [author_name, quote_text, quote_tags, author_link]\n",
    "        i = i + 1\n",
    "\n",
    "    if ((pageNum % 5)==0) :\n",
    "        email_notify(\"Finished parsing page: \" + website_url + \" at \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "\n",
    "    if webpage['has_next'] :\n",
    "        pageNum = pageNum + 1\n",
    "        website_url = api_url + str(pageNum)\n",
    "        # Adding random wait time so we do not hammer the website needlessly\n",
    "        waitTime = randint(2,5)\n",
    "        print(\"Waiting \" + str(waitTime) + \" seconds to process next page...\")\n",
    "        sleep(waitTime)\n",
    "\n",
    "        try:\n",
    "            resp = s.get(website_url, headers=headers)\n",
    "        except HTTPError as e:\n",
    "            print('The server could not serve up the web page!')\n",
    "            sys.exit(\"Script Processing Aborted!!!\")\n",
    "        except ConnectionError as e:\n",
    "            print('The server could not be reached!')\n",
    "            sys.exit(\"Script Processing Aborted!!!\")\n",
    "\n",
    "        webpage = resp.json()\n",
    "    else :\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data and Producing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records written to file: 100\n",
      "Total time for the script: 0:00:43.273316\n"
     ]
    }
   ],
   "source": [
    "out_file = df.to_json(orient='records')\n",
    "with open('web-scraping-py-bsoup-infinite-scrolling.json', 'w') as f:\n",
    "    f.write(out_file)\n",
    "print('Total number of records written to file:', len(df))\n",
    "email_notify(\"The web scraping process has completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))\n",
    "print ('Total time for the script:', (datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
