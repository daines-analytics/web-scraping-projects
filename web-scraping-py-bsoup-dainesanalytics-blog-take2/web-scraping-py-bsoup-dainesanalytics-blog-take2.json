[{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 5","author_name":"David Lowe","blog_date":"Fri, 07 Jun 2019 12:44:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/07\/binary-classification-model-for-heart-disease-study-using-r-take-5\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take3, we examined the Switzerland dataset and\ncreated an Extra Trees model to fit the data.\nIn iteration Take4, we examined the Long Beach VA dataset\nand created an Extra Trees model to fit the data.\nIn this iteration, we will combine all four datasets and\ncreate a machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 80.56%. Two algorithms (Random\nForest and Gradient Boosting) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Gradient Boosting turned in\nthe top overall result and achieved an accuracy metric of 82.84%. By using the\noptimized parameters, the Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 77.82%, which was somewhat below the prediction\naccuracy gained from the training data and possibly due to overfitting.\nCONCLUSION: For the combined dataset, the Gradient Boosting\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Gradient Boosting could be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 4","author_name":"David Lowe","blog_date":"Thu, 06 Jun 2019 12:22:34 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/06\/binary-classification-model-for-heart-disease-study-using-r-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a machine learning model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and\ncreated a machine learning model to fit the data.\nIn iteration Take3, we examined the Switzerland dataset and\ncreated a machine learning model to fit the data.\nIn this iteration, we will examine the Long Beach VA dataset\nand create a machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 73.74%. Two algorithms (Random\nForest and Gradient Boosting) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Random Forest turned in the\ntop overall result and achieved an accuracy metric of 74.71%. By using the\noptimized parameters, the Random Forest algorithm processed the testing dataset\nwith an accuracy of 79.66%, which was even better than the prediction accuracy\ngained from the training data.\nCONCLUSION: For the Long Beach VA dataset, the Random Forest\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Random Forest could be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 3","author_name":"David Lowe","blog_date":"Wed, 05 Jun 2019 12:49:03 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/05\/binary-classification-model-for-heart-disease-study-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and created a machine learning model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and created a machine learning model to fit the data.\nIn this iteration, we will examine the Switzerland dataset\nand create a machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 90.93%. Two algorithms (Decision\nTrees and Gradient Boosting) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Gradient Boosting turned in\nthe top overall result and achieved an accuracy metric of 93.40%. By using the\noptimized parameters, the Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 94.40%, which was even better than the prediction\naccuracy gained from the training data.\nCONCLUSION: For the Switzerland dataset, the Gradient\nBoosting algorithm achieved the best overall results using the training and\ntesting datasets. For this dataset, Gradient Boosting could be considered for\nfurther modeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 2","author_name":"David Lowe","blog_date":"Tue, 04 Jun 2019 12:33:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/04\/binary-classification-model-for-heart-disease-study-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and created a machine learning model to fit the data.\nIn this iteration, we will examine the Hungarian dataset and\ncreate a machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 79.33%. Two algorithms (Logistic\nRegression and Gradient Boosting) achieved the top accuracy metrics after the\nfirst round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 82.13%. By\nusing the optimized parameters, the Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 82.76%, which was even better than the\nprediction accuracy gained from the training data.\nCONCLUSION: For the Cleveland dataset, the Gradient Boosting\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Gradient Boosting could be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 03 Jun 2019 12:10:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/03\/binary-classification-model-for-heart-disease-study-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn this iteration, we will examine the Cleveland dataset and\ncreate a machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 84.78%. Two algorithms (Random\nForest and Gradient Boosting) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Gradient Boosting turned in\nthe top overall result and achieved an accuracy metric of 87.95%. By using the\noptimized parameters, the Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 82.22%, which was slightly below the prediction\naccuracy gained from the training data.\nCONCLUSION: For the Cleveland dataset, the Gradient Boosting\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Gradient Boosting could be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of SAS Global Forum 2019 Proceedings Using BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 02 Jun 2019 12:30:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/06\/02\/web-scraping-of-sas-global-forum-2019-proceedings-using-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by\nextracting specific pieces of information from a website. The web scraping\npython code leverages the BeautifulSoup module.\nINTRODUCTION: On occasions we need to download a batch of documents listed\non a single web page without clicking on the download link one at a time. This\nweb scraping script will automatically traverse through the entire web page and\ncollect all links to the PDF documents. The script will also download the PDF\ndocuments as part of the scraping process.\nStarting URLs: https:\/\/www.sas.com\/en_us\/events\/sas-global-forum\/program\/proceedings.html\nThe source code and HTML output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Coronary Artery Disease Using R Take 1","author_name":"David Lowe","blog_date":"Fri, 31 May 2019 12:09:35 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/31\/binary-classification-model-for-coronary-artery-disease-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Z-Alizadeh Sani CAD dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The researchers collected the data file for\ncoronary artery disease (CAD) diagnosis. Each patient could be in two possible\ncategories CAD or Normal. A patient is categorized as CAD, if his\/her diameter\nnarrowing is greater than or equal to 50%, and otherwise as Normal. The\nZ-Alizadeh Sani dataset contains the records of 303 patients, each of which has\n59 features. The features can belong to one of four groups: demographic,\nsymptom and examination, ECG, and laboratory and echo features. In this\nextension, the researchers add three features for the LAD, LCX, and RCA arteries.\nCAD becomes true when at least one of these three arteries is stenotic. To properly\nuse this dataset for CAD classification only one of LAD, LCX, RCA or Cath\n(Result of angiography) can be present in the dataset. This dataset not only\ncan be used for CAD detection, but also stenosis diagnosis of each LAD, LCX and\nRCA arteries.\nIn this iteration, we plan to establish the baseline\nprediction accuracy for further takes of modeling.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 83.07%. Two algorithms (Random\nForest and Gradient Boosting) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Gradient Boosting turned in\nthe top overall result and achieved an accuracy metric of 89.19%. By using the\noptimized parameters, the Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 77.78%, which was significantly below the\nprediction accuracy gained from the training data and possibly due to\nover-fitting.\nCONCLUSION: For this iteration, the Gradient Boosting\nalgorithm achieved the best overall training and validation results. For this\ndataset, the Gradient Boosting algorithm could be considered for further\nmodeling.\nDataset Used: Z-Alizadeh Sani Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/extention+of+Z-Alizadeh+sani+dataset\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Using Docker to Build Data Science Environments with RStudio","author_name":"David Lowe","blog_date":"Wed, 29 May 2019 12:13:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/29\/using-docker-to-build-data-science-environments-with-rstudio\/","blog_text":"\n\nI have been using Docker to create environments for data\nscience work. With Docker, I was able to painlessly create the environments\nwith a degree of accuracy and consistency. After getting exposed to using\nDocker for environment creation, it is hard to imagine doing it any other ways.\nFor more information, I encourage you to check out the Rocker images and this blog post about using RStudio and Docker.\nStep 1: Create VM and update OS as necessary\nI created virtual machines on VMware using CentOS 7 and make\nit accessible through bridged networking. I also used CentOS\u2019 minimum\ninstallation as it just needs the basic components to run Docker. We will need\nto access the VM via SSH with the port\n8787 opened for the RStudio server instance.\nStep 2: Access the VM via SSH (through a non-root user call docker_admin) and install Git with the sudo command.\nStep 3: Install Docker for the non-root docker_admin user. Verify the installation with the command \u201cdocker image ls.\u201d\nMore information on installing Docker CE can be found at here and here.\nIt boils down to:\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\nsudo yum-config-manager --add-repo https:\/\/download.docker.com\/linux\/centos\/docker-ce.repo\nsudo yum -y install docker-ce\nsudo systemctl start docker && sudo systemctl enable docker\nsudo usermod -aG docker docker_admin\nStep 4: For my environments, I need to clone some R template scripts. This step is not mandatory if you do not require it.\ngit clone https:\/\/github.com\/daines-analytics\/template-latest.git examples\nFor my environments, I also need to make some environment\nvariables accessible to the scripts. Again, this step may not be mandatory for\nyour installation.\nscp .Renviron cloud_user@<IP_Address>:\/home\/cloud_user\nStep 5: Create\nthe Dockerfile or use the one from the template directory\nFROM rocker\/verse\nLABEL com.dainesanalytics.rstudio.version=v1.0\nRUN Rscript -e \"install.packages(c('knitr', 'tidyverse', 'caret', 'corrplot', 'mailR', 'DMwR', 'ROCR', 'Hmisc', 'randomForest', 'e1071', 'elasticnet', 'gbm', 'xgboost'))\"\nCOPY --chown=rstudio:rstudio .Renviron \/home\/rstudio\nCOPY --chown=rstudio:rstudio examples\/ \/home\/rstudio\nMy\nenvironments require many of the machine learning packages, but these\npackages may not be mandatory for your installation.\nStep 6: Build the Docker image with the command:\ndocker image build -t rstudio\/nonroot:v1 .\nStep 7: Run the Docker container with the command:\ndocker container run --rm -e PASSWORD=rserver -p 8787:8787 --name\nrstudio-server rstudio\/nonroot:v1\nThe password can be any string, and the RStudio Server just\nrequires one.\nStep 8: After we are done with the container and\/or the virtual machine, we can shut down the container with the command:\ndocker container stop [container ID]\nThe templates (R and Docker) can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Using Docker to Build Data Science Environments with Anaconda","author_name":"David Lowe","blog_date":"Mon, 27 May 2019 12:56:05 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/27\/using-docker-to-build-data-science-environments-with-anaconda\/","blog_text":"\n\nI have been using Docker to create environments for data\nscience work. With Docker, I was able to painlessly create the environments\nwith a degree of accuracy and consistency. After getting exposed to using\nDocker for environment creation, it is hard to imagine doing it any other ways.\nFor more information, I encourage you to check out the Anaconda images and this blog post about using Anaconda and Docker.\nStep 1: Create VM and update OS as necessary\nI created virtual machines on VMware using CentOS 7 and make it accessible through bridged networking. I also used CentOS\u2019 minimum installation as it just needs the basic components to run Docker. We will need to access the VM via SSH with the port (8080 in my case) opened for the Jupyter notebook instance.\nStep 2: Access the VM via SSH (through a non-root user call docker_admin) and install Git with the sudo command.\nStep 3: Install Docker for the non-root docker_admin user. Verify the installation with the command \u201cdocker image ls.\u201d\nMore information on installing Docker CE can be found at here and here.\nIt boils down to:\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\nsudo yum-config-manager --add-repo \\\n  https:\/\/download.docker.com\/linux\/centos\/docker-ce.repo\nsudo yum -y install docker-ce\nsudo systemctl start docker && sudo systemctl enable docker\nsudo usermod -aG docker docker_admin\nStep 4: For my environments, I need to clone some Python template scripts. This step is not mandatory if you do not require it.\ngit clone https:\/\/github.com\/daines-analytics\/template-latest.git examples\nFor my environments, I also need to make some environment variables accessible to the scripts. Again, this step may not be mandatory for your installation.\nscp docker_env.txt cloud_user@<IP_Address>:\/home\/cloud_user\nStep 5: Create the Dockerfile or use the one from the template directory\nFROM continuumio\/anaconda3\nLABEL com.dainesanalytics.anaconda.version=v1.0\nEXPOSE 8080\nRUN conda install -c conda-forge -y --freeze-installed imbalanced-learn xgboost\nRUN useradd -ms \/bin\/bash dev_user\nUSER dev_user\nWORKDIR \/home\/dev_user\nCOPY --chown=dev_user:dev_user examples\/ \/home\/dev_user\nCMD \/opt\/conda\/bin\/jupyter notebook --ip=0.0.0.0 --port=8080 --no-browser --notebook-dir=\/home\/dev_user\nStep 6: Build the Docker image with the command:\ndocker image build -t anaconda3\/nonroot:v1 .\nStep 7: Run the Docker container with the command:\ndocker container run --rm --env-file docker_env.txt -p 8080:8080\n--name jupyter-server anaconda3\/nonroot:v1\nStep 8: After we are done with the container and\/or the virtual machine, we can shut down the container with the command:\ndocker container stop [container ID]\nThe templates (Python and Docker) can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Daines Analytics Blog Using BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 26 May 2019 12:09:35 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/26\/web-scraping-of-daines-analytics-blog-using-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by\ngathering specific pieces of information from a website. The web scraping code\nwas written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog.\nThe purpose of this exercise is to practice web scraping by gathering the blog\nentries from Daines Analytics\u2019 RSS feed. This iteration of the script\nautomatically traverses the RSS feed to capture all blog entries.\nStarting URLs: https:\/\/dainesanalytics.blog\/feed or\nhttps:\/\/dainesanalytics.blog\/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 5","author_name":"David Lowe","blog_date":"Fri, 24 May 2019 12:39:16 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/24\/binary-classification-model-for-heart-disease-study-using-python-take-5\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take3, we examined the Switzerland dataset and\ncreated an Extra Trees model to fit the data.\nIn iteration Take4, we examined the Long Beach VA dataset\nand created an Extra Trees model to fit the data.\nIn this iteration, we will combine all four datasets and\nlook for a suitable machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 76.33%. Two algorithms (Logistic\nRegression and Gradient Boosting) achieved the top accuracy metrics after the\nfirst round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 80.43%. By\nusing the optimized parameters, the Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 80.79%, which was slightly better than the\nprediction accuracy gained from the training data.\nCONCLUSION: For the combined dataset, the Gradient Boosting\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Gradient Boosting should be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 4","author_name":"David Lowe","blog_date":"Thu, 23 May 2019 12:27:54 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/23\/binary-classification-model-for-heart-disease-study-using-python-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take3, we examined the Switzerland dataset and\ncreated an Extra Trees model to fit the data.\nIn this iteration, we will examine the Long Beach VA dataset\nand look for a suitable machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 73.45%. Two algorithms (Logistic\nRegression and Extra Trees) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Extra Trees turned in the\ntop overall result and achieved an accuracy metric of 77.85%. By using the\noptimized parameters, the Extra Trees algorithm processed the testing dataset\nwith an accuracy of 66.66%, which was significantly below the prediction\naccuracy gained from the training data.\nCONCLUSION: For the Long Beach VA dataset, the Extra Trees\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Extra Trees should be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 22 May 2019 12:00:05 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/22\/binary-classification-model-for-heart-disease-study-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a Logistic Regression model to fit the data.\nIn iteration Take2, we examined the Hungarian dataset and\ncreated a Logistic Regression model to fit the data.\nIn this iteration, we will examine the Switzerland dataset\nand look for a suitable machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 88.10%. Two algorithms (Extra Trees\nand Gradient Boosting) achieved the top accuracy metrics after the first round\nof modeling. After a series of tuning trials, Extra Trees turned in the top\noverall result and achieved an accuracy metric of 93.02%. By using the\noptimized parameters, the Extra Trees algorithm processed the testing dataset\nwith an accuracy of 91.89%, which was slightly below the prediction accuracy\ngained from the training data.\nCONCLUSION: For the Switzerland dataset, the Extra Trees\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Extra Trees should be considered for further\nmodeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 2","author_name":"David Lowe","blog_date":"Tue, 21 May 2019 12:08:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/21\/binary-classification-model-for-heart-disease-study-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Heart Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nIn iteration Take1, we examined the Cleveland dataset and\ncreated a Logistic Regression model to fit the data.\nIn this iteration, we will examine the Hungarian dataset and\nlook for a suitable machine learning model to fit the data.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 80.04%. Two algorithms (Logistic\nRegression and Extra Trees) achieved the top accuracy metrics after the first\nround of modeling. After a series of tuning trials, Logistic Regression turned\nin the top overall result and achieved an accuracy metric of 83.90%. By using\nthe optimized parameters, the Logistic Regression algorithm processed the\ntesting dataset with an accuracy of 80.68%, which was slightly below the\nprediction accuracy gained from the training data.\nCONCLUSION: For the Hungarian dataset, the Logistic\nRegression algorithm achieved the best overall results using the training and\ntesting datasets. For this dataset, Logistic Regression should be considered\nfor further modeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Heart Disease Study Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 20 May 2019 12:25:23 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/20\/binary-classification-model-for-heart-disease-study-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Heart Disease dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The original database contains 76 attributes,\nbut all published experiments refer to using a subset of 14 of them. In\nparticular, the Cleveland database is the only one that has been used by\nmachine learning researchers to this date. The \u201cnum\u201d field refers to\nthe presence of heart disease in the patient. It is integer valued from 0 (no\npresence) to 4. Experiments with the Cleveland database have concentrated on\nsimply attempting to distinguish presence (values 1,2,3,4) from absence (value\n0).\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 74.69%. Two algorithms (Logistic\nRegression and Stochastic Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, Logistic\nRegression turned in the top overall result and achieved an accuracy metric of\n82.07%. By using the optimized parameters, the Logistic Regression algorithm\nprocessed the testing dataset with an accuracy of 90.10%, which was even better\nthan the prediction accuracy gained from the training data.\nCONCLUSION: For this iteration, the Logistic Regression\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Logistic Regression should be considered for\nfurther modeling or production use.\nDataset Used: Heart Disease Data Set\nDataset ML Model: Binary classification with numerical and\ncategorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\nOne potential source of performance benchmark:\nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping Templates for Python with BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 19 May 2019 12:56:16 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/19\/web-scraping-templates-for-python-with-beautifulsoup\/","blog_text":"\n\nAs I work on practicing and solving web scraping problems, I\nfind myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support web scraping tasks using Python.\nThe Python scripts leverage the BeautifulSoup module. You can find the web scraping templates from the Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Diabetes Readmission Prediction Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 17 May 2019 12:40:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/17\/binary-classification-model-for-diabetes-readmission-prediction-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes reflected\nin HbA1c determination may improve patient outcomes and lower cost of inpatient\ncare.\nIn iteration Take1, we established the baseline prediction\naccuracy for further takes of modeling. To limit the processing time and memory\nrequirements, we also limited the attributes used for this project by not\nincluding those attributes that do not appear on the final model of the\nresearch paper.\nIn iteration Take2, we further tested the machine learning\nmodels by rearranging some of the features to be more consistent with the\nresearch paper (Table 4). We had hoped to improve the overall accuracy and\napplicability of the model by having features with a fewer number of\ncategories.\nIn this iteration, we will test the machine learning models\nby reconfiguring the target variable to have only two categories, thus making\nthis a binary classification exercise. We hope to improve the overall accuracy\napplicability of the model by predicting with just the \u201cyes\u201d and \u201cno\u201d outcomes.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 61.82%. Two algorithms (Logistic\nRegression and Gradient Boosting) achieved the top accuracy metrics after the\nfirst round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 63.05%. By\nusing the optimized parameters, the Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 62.96%, which was slightly below the\nprediction accuracy using the training data.\nCONCLUSION: Restructuring the target variable to binary\noutcomes yielded accuracy improvement and a reduction in the processing time.\nFor this iteration, the Gradient Boosting algorithm achieved the overall\nresults using the training and testing datasets. For this dataset, Gradient\nBoosting should be considered for further modeling or production use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks:\nhttp:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 15 May 2019 12:26:38 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/15\/multi-class-classification-model-for-diabetes-readmission-prediction-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes reflected\nin HbA1c determination may improve patient outcomes and lower cost of inpatient\ncare.\nIn iteration Take1, we established the baseline prediction\naccuracy for further takes of modeling. To limit the processing time and memory\nrequirements, we also limited the attributes used for this project by not\nincluding those attributes that do not appear on the final model of the\nresearch paper.\nIn this iteration, we further test the machine learning\nmodels by rearranging some of the features to be more consistent with the\nresearch papers (Table 4). We hope to improve the overall accuracy and\napplicability of the model by having features with a fewer number of\ncategories.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 56.32%. Two algorithms (Linear\nDiscriminant Analysis and Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, Gradient\nBoosting turned in the top overall result and achieved an accuracy metric of\n58.07%. By using the optimized parameters, the Gradient Boosting algorithm\nprocessed the testing dataset with an accuracy of 57.04%, which was slightly\nbelow the prediction accuracy from the training data.\nCONCLUSION: Restructuring the categorical variables did not\nyield accuracy or processing time improvement. For this iteration, the Gradient\nBoosting algorithm achieved the top-tier training and validation results. For\nthis dataset, Gradient Boosting should be considered for further modeling or\nproduction use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks:\nhttp:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 13 May 2019 12:46:04 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/13\/multi-class-classification-model-for-diabetes-readmission-prediction-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes reflected\nin HbA1c determination may improve patient outcomes and lower cost of inpatient\ncare.\nIn this iteration, we will establish the baseline prediction\naccuracy for further takes of modeling. To limit the processing time and memory\nrequirements, we also will limit the attributes used for this project by not\nincluding those attributes that do not appear on the final model of the\nresearch paper.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 57.15%. Two algorithms (Linear\nDiscriminant Analysis and eXtreme Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\neXtreme Gradient Boosting turned in the top overall result and achieved an\naccuracy metric of 58.74%. By using the optimized parameters, the eXtreme\nGradient Boosting algorithm processed the testing dataset with an accuracy of\n58.85%, which was even better than the prediction accuracy from the training\ndata.\nCONCLUSION: For this iteration, the eXtreme Gradient\nBoosting algorithm achieved the best overall results. For this dataset, eXtreme\nGradient Boosting should be considered for further modeling or production use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks: http:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 4","author_name":"David Lowe","blog_date":"Sun, 12 May 2019 12:32:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/12\/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-4\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by gathering specific pieces of information from a website. The web\nscraping code was written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: A demo website, created by Scrapinghub, lists\nquotes from famous people. It has many endpoints showing the quotes in\ndifferent ways, and each endpoint presents a different scraping challenge for\npracticing web scraping. For this Take4 iteration, the Python script attempts\nto execute the login form and scrape the Goodreads links off each quote. The\nGoodreads links appear only after a successful authentication.\nStarting URLs: http:\/\/quotes.toscrape.com\/login\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Diabetes Readmission Prediction Using Python Take 3","author_name":"David Lowe","blog_date":"Fri, 10 May 2019 12:29:10 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/10\/binary-classification-model-for-diabetes-readmission-prediction-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes reflected\nin HbA1c determination may improve patient outcomes and lower cost of inpatient\ncare.\nIn iteration Take1, we established the baseline prediction\naccuracy for further takes of modeling. To limit the processing time and memory\nrequirements, we also limited the attributes used for this project by not\nincluding those attributes that do not appear on the final model of the\nresearch paper.\nIn iteration Take2, we further tested the machine learning\nmodels by rearranging some of the features to be more consistent with the\nresearch papers (Table 4). We had hoped to improve the overall accuracy and\napplicability of the model by having features with a fewer number of\ncategories.\nIn this iteration, we will test the machine learning models\nby reconfiguring the target variable to have only two categories, thus making\nthis a binary classification exercise. We hope to improve the overall accuracy\nand applicability of the model by predicting with just the \u201cyes\u201d and\n\u201cno\u201d outcomes.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 58.93%. Two algorithms (Logistic\nRegression and Gradient Boosting) achieved the top accuracy metrics after the\nfirst round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 63.14%. By\nusing the optimized parameters, the Bagged Decision Trees algorithm processed\nthe testing dataset with an accuracy of 62.85%, which was just slightly below\nthe prediction accuracy from the training data.\nCONCLUSION: Restructuring the target variable to binary\noutcomes yielded accuracy improvement and a reduction in the processing time.\nFor this iteration, the Gradient Boosting algorithm achieved the best overall\nresults using the training and testing datasets. For this dataset, Gradient\nBoosting should be considered for further modeling or production use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks:\nhttp:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using Python Take 2","author_name":"David Lowe","blog_date":"Wed, 08 May 2019 12:24:25 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/08\/multi-class-classification-model-for-diabetes-readmission-prediction-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes reflected\nin HbA1c determination may improve patient outcomes and lower cost of inpatient\ncare.\nIn iteration Take1, we established the baseline prediction\naccuracy for further takes of modeling. To limit the processing time and memory\nrequirements, we also limited the attributes used for this project by not\nincluding those attributes that do not appear on the final model of the\nresearch paper.\nIn this iteration, we further test the machine learning\nmodels by rearranging some of the features to be more consistent with the\nresearch papers (Table 4). We hope to improve the overall accuracy and\napplicability of the model by having features with a fewer number of\ncategories.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 53.04%. Two algorithms (Linear\nDiscriminant Analysis and Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, Gradient\nBoosting turned in the top overall result and achieved an accuracy metric of\n58.11%. By using the optimized parameters, the Bagged Decision Trees algorithm\nprocessed the testing dataset with an accuracy of 57.73%, which was just\nslightly below the prediction accuracy from the training data.\nCONCLUSION: Restructuring the categorical variables did not\nyield accuracy improvement but certainly had a positive impact on the\nprocessing time. For this iteration, the Gradient Boosting algorithm achieved\nthe best overall results using the training and testing datasets. For this\ndataset, Gradient Boosting should be considered for further modeling or\nproduction use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks:\nhttp:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Diabetes Readmission Prediction Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 06 May 2019 12:52:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/06\/multi-class-classification-model-for-diabetes-readmission-prediction-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Diabetes Readmission Prediction is a\nmulti-class classification situation where we are trying to predict one of the\nseveral possible outcomes.\nINTRODUCTION: Management of hyperglycemia in hospitalized\npatients has a significant bearing on the outcome, in terms of both morbidity\nand mortality. However, there are few national assessments of diabetes care\nduring hospitalization which could serve as a baseline for change. This\nanalysis of a large clinical database was undertaken to provide such an\nassessment and to find future directions which might lead to improvements in\npatient safety. The statistical model suggests that the relationship between\nthe probability of readmission and the HbA1c measurement depends on the primary\ndiagnosis. The data suggest further that the greater attention to diabetes\nreflected in HbA1c determination may improve patient outcomes and lower cost of\ninpatient care.\nIn this iteration, we plan to establish the baseline prediction accuracy for further takes of modeling. To limit the processing time and memory requirements, we also will limit the attributes used for this project by not including those attributes that do not appear on the final model of the research paper.\nANALYSIS: The baseline performance of the machine learning algorithms achieved an average accuracy of 53.71%. Two algorithms (Linear Discriminant Analysis and Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Gradient Boosting turned in the top overall result and achieved an accuracy metric of 58.78%. By using the optimized parameters, the Bagged Decision Trees algorithm processed the testing dataset with an accuracy of 58.41%, which was just slightly below the prediction accuracy from the training data.\nCONCLUSION: For this iteration, the Gradient Boosting\nalgorithm achieved the best overall results using the training and testing\ndatasets. For this dataset, Gradient Boosting should be considered for further\nmodeling or production use.\nDataset Used: Diabetes 130-US hospitals for years 1999-2008\nData Set\nDataset ML Model: Multi-Class classification with numerical\nand categorical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008\nOne source of potential performance benchmarks:\nhttp:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 3","author_name":"David Lowe","blog_date":"Sun, 05 May 2019 12:53:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/05\/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-3\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by gathering specific pieces of information from a website. The web\nscraping code was written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: A demo website, created by Scrapinghub, lists\nquotes from famous people. It has many endpoints showing the quotes in\ndifferent ways, and each endpoint presents a different scraping challenge for\npracticing web scraping. For this Take3 iteration, the Python script attempts\nto scrape the displayed quote information via an infinite scrolling page.\nNote: For this iteration, the website returns the data in\nJSON format when using the API URL format. As a result, the BeautifulSoup\nmodule is not necessary for parsing the web pages for this iteration.\nStarting URLs: http:\/\/quotes.toscrape.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 03 May 2019 12:44:54 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/03\/binary-classification-model-for-parkinsons-disease-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a prediction\nmodel using various machine learning algorithms and to document the end-to-end\nsteps using a template. Parkinson\u2019s Disease dataset is a binary classification\nsituation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.\nThe control group consists of 64 healthy individuals (23 men and 41 women) with\nages varying between 41 and 82. During the data collection process, the\nmicrophone is set to 44.1 KHz and following the physician\u2019s examination, the\nsustained phonation of the vowel \/a\/ was collected from each subject with three\nrepetitions.\nIn the first iteration, the script focused on evaluating\nvarious machine learning algorithms and identifying the model that produces the\nbest overall metrics. The first iteration established the performance baseline\nfor accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of attribute importance ranking by using the Gradient Boosting\nalgorithm. By selecting only the most important attributes, we decreased the\nprocessing time and maintained a similar level of prediction accuracy compared\nto the first iteration.\nIn iteration Take3, we will examine the feature selection\ntechnique of Recursive Feature Elimination (RFE) by using the Random Forest\nalgorithm. By selecting only the most relevant attributes, we hoped to decrease\nthe processing time and maintain a similar level of prediction accuracy\ncompared to the first iteration.\nANALYSIS: In the first iteration, the baseline performance\nof the machine learning algorithms achieved an average accuracy of 77.84%. Two\nalgorithms (Random Forest and Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\nRandom Forest turned in the top overall result and achieved an accuracy metric\nof 88.24%. By using the optimized parameters, the Random Forest algorithm\nprocessed the testing dataset with an accuracy of 83.63%, which was just\nslightly below the prediction accuracy using the training data.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 82.14%. Two algorithms\n(Random Forest and Gradient Boosting) achieved the top accuracy metrics after\nthe first round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 88.92%. By\nusing the optimized parameters, the Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 88.05%, which was just slightly below the\nprediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 541, from 753 down to 212. The processing time went\nfrom 2 hours 16 minutes in the first iteration down to 27 minutes in Take2,\nwhich was a decrease of 80.1%.\nIn iteration Take3, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 81.69%. Two algorithms\n(Random Forest and Gradient Boosting) achieved the top accuracy metrics after\nthe first round of modeling. After a series of tuning trials, Random Forest turned\nin the top overall result and achieved an accuracy metric of 89.11%. By using\nthe optimized parameters, the Random Forest algorithm processed the testing\ndataset with an accuracy of 84.96%, which was just slightly below the\nprediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 571, from 753 down to 182. The processing time went\nfrom 2 hours 16 minutes in the first iteration down to 1 hour 43 minutes in\nTake3, which was a decrease of 24.2%.\nCONCLUSION: For this iteration, using the RFE technique and\nthe Random Forest algorithm achieved the best overall modeling results. Using a\nfeature selection technique further reduced the processing time while achieving\nan even better prediction accuracy overall. For this dataset, either Gradient\nBoosting or Random Forest, combined with a feature selection technique, should\nbe considered for further modeling or production use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u2019s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 01 May 2019 12:44:56 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/05\/01\/binary-classification-model-for-parkinsons-disease-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a prediction\nmodel using various machine learning algorithms and to document the end-to-end\nsteps using a template. Parkinson\u2019s Disease dataset is a binary classification\nsituation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.\nThe control group consists of 64 healthy individuals (23 men and 41 women) with\nages varying between 41 and 82. During the data collection process, the\nmicrophone is set to 44.1 KHz and following the physician\u2019s examination, the\nsustained phonation of the vowel \/a\/ was collected from each subject with three\nrepetitions.\nIn the first iteration, the script focused on evaluating\nvarious machine learning algorithms and identifying the model that produces the\nbest overall metrics. The first iteration established the performance baseline\nfor accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of attribute importance ranking by using the Gradient Boosting\nalgorithm. By selecting only the most important attributes, we decreased the\nprocessing time and maintained a similar level of prediction accuracy compared\nto the first iteration.\nIn iteration Take3, we will examine the feature selection\ntechnique of Recursive Feature Elimination (RFE) by using the Random Forest\nalgorithm. By selecting only the most relevant attributes, we hoped to decrease\nthe processing time and maintain a similar level of prediction accuracy\ncompared to the first iteration.\nANALYSIS: In the first iteration, the baseline performance\nof the machine learning algorithms achieved an average accuracy of 81.58%. Two\nalgorithms (Extra Trees and Stochastic Gradient Boosting) achieved the top\naccuracy metrics after the first round of modeling. After a series of tuning\ntrials, Extra Trees turned in the top overall result and achieved an accuracy\nmetric of 88.09%. By using the optimized parameters, the Extra Trees algorithm\nprocessed the testing dataset with an accuracy of 87.22%, which was just\nslightly below the prediction accuracy using the training data.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 82.97%. Two algorithms\n(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\nExtra Trees turned in the top overall result and achieved an accuracy metric of\n90.17%. By using the optimized parameters, the Extra Trees algorithm processed\nthe testing dataset with an accuracy of 89.42%, which was just slightly below\nthe prediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 585, from 753 down to 168. The processing time went\nfrom 18 minutes 16 seconds in the first iteration down to 11 minutes 33 seconds\nin Take2, which was a decrease of 36.7%.\nIn iteration Take3, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 80.96%. Two algorithms\n(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\nExtra Trees turned in the top overall result and achieved an accuracy metric of\n88.27%. By using the optimized parameters, the Extra Trees algorithm processed\nthe testing dataset with an accuracy of 88.10%, which was just slightly below\nthe prediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 503, from 753 down to 250. The processing time went\nfrom 18 minutes 16 seconds in the first iteration down to 14 minutes 37 seconds\nin Take3, which was a decrease of 19.9%.\nCONCLUSION: For this iteration, using the RFE technique and\nthe Extra Trees algorithm achieved the best overall modeling results. Using\nfeature selection technique further reduced the processing time while achieving\nan even better prediction accuracy overall. For this dataset, Extra Trees,\ncombined with a feature selection technique, should be considered for further\nmodeling or production use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u2019s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using R Take 2","author_name":"David Lowe","blog_date":"Mon, 29 Apr 2019 12:40:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/29\/binary-classification-model-for-parkinsons-disease-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a prediction\nmodel using various machine learning algorithms and to document the end-to-end\nsteps using a template. Parkinson\u2019s Disease dataset is a binary classification\nsituation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.\nThe control group consists of 64 healthy individuals (23 men and 41 women) with\nages varying between 41 and 82. During the data collection process, the\nmicrophone is set to 44.1 KHz and following the physician\u2019s examination, the\nsustained phonation of the vowel \/a\/ was collected from each subject with three\nrepetitions.\nIn the first iteration, the script focused on evaluating\nvarious machine learning algorithms and identifying the model that produces the\nbest overall metrics. The first iteration established the performance baseline\nfor accuracy and processing time.\nIn iteration Take2, we will examine the feature selection\ntechnique of attribute importance ranking by using the Gradient Boosting\nalgorithm. By selecting only the most important attributes, we hoped to\ndecrease the processing time and maintain a similar level of prediction\naccuracy compared to the first iteration.\nANALYSIS: In the first iteration, the baseline performance\nof the machine learning algorithms achieved an average accuracy of 77.84%. Two\nalgorithms (Random Forest and Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\nRandom Forest turned in the top overall result and achieved an accuracy metric\nof 88.24%. By using the optimized parameters, the Random Forest algorithm\nprocessed the testing dataset with an accuracy of 83.63%, which was just\nslightly below the prediction accuracy using the training data.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 82.14%. Two algorithms\n(Random Forest and Gradient Boosting) achieved the top accuracy metrics after\nthe first round of modeling. After a series of tuning trials, Gradient Boosting\nturned in the top overall result and achieved an accuracy metric of 88.92%. By\nusing the optimized parameters, the Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 88.05%, which was just slightly below the\nprediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 541, from 753 down to 212. The processing time went\nfrom 2 hours 16 minutes in the first iteration down to 27 minutes in Take2,\nwhich was a decrease of 80.1%.\nCONCLUSION: For this iteration, using the Attribute\nImportance Ranking technique and the Gradient Boosting algorithm achieved the\nbest overall modeling results. Using feature selection technique further\nreduced the processing time while achieving an even better prediction accuracy\noverall. For this dataset, Gradient Boosting should be considered for further\nmodeling or production use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u2019s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of AWS Documentation using BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 28 Apr 2019 12:58:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/28\/web-scraping-of-aws-documentation-using-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by extracting specific information from a website. Using the extracted\ninformation, the script further completes other tasks (downloading files in\nthis case). The web scraping python code leverages the BeautifulSoup module.\nINTRODUCTION: On occasions, there is a need to download a\nbatch of documents off web pages without clicking on the download links one at\na time. This web scraping script will automatically traverse through the\nnecessary web pages and collect all links with the PDF document format. The\nscript will also download the PDF documents as part of the scraping process.\nFor this script to work, it requires the use of Selenium\nbrowser automation software and one of its WebDrivers (Firefox in this case).\nStarting URLs: https:\/\/docs.aws.amazon.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using Python","author_name":"David Lowe","blog_date":"Fri, 26 Apr 2019 12:45:29 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/26\/binary-classification-model-for-parkinsons-disease-using-python-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction\nmodel using various machine learning algorithms and to document the end-to-end\nsteps using a template. Parkinson\u2019s Disease dataset is a binary classification\nsituation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul University.\nThe control group consists of 64 healthy individuals (23 men and 41 women) with\nages varying between 41 and 82. During the data collection process, the\nmicrophone is set to 44.1 KHz and following the physician\u2019s examination, the\nsustained phonation of the vowel \/a\/ was collected from each subject with three\nrepetitions.\nIn the first iteration, the script focused on evaluating\nvarious machine learning algorithms and identifying the model that produces the\nbest overall metrics. The first iteration established the performance baseline\nfor accuracy and processing time.\nIn iteration Take2, we will examine the feature selection\ntechnique of attribute importance ranking by using the Gradient Boosting\nalgorithm. By selecting only the most important attributes, we hoped to\ndecrease the processing time and maintain a similar level of prediction\naccuracy compared to the first iteration.\nANALYSIS: In the first iteration, the baseline performance\nof the machine learning algorithms achieved an average accuracy of 81.58%. Two\nalgorithms (Extra Trees and Stochastic Gradient Boosting) achieved the top\naccuracy metrics after the first round of modeling. After a series of tuning\ntrials, Extra Trees turned in the top overall result and achieved an accuracy\nmetric of 88.09%. By using the optimized parameters, the Extra Trees algorithm\nprocessed the testing dataset with an accuracy of 87.22%, which was just\nslightly below the prediction accuracy using the training data.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 82.97%. Two algorithms\n(Extra Trees and Stochastic Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\nExtra Trees turned in the top overall result and achieved an accuracy metric of\n90.17%. By using the optimized parameters, the Extra Trees algorithm processed\nthe testing dataset with an accuracy of 89.42%, which was just slightly below\nthe prediction accuracy using the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 585, from 753 down to 168. The processing time went\nfrom 18 minutes 16 seconds in the first iteration down to 11 minutes 33 seconds\nin Take2, which was a decrease of 36.7%.\nCONCLUSION: For this iteration, using the Attribute\nImportance Ranking technique and the Extra Trees algorithm achieved the best\noverall modeling results. Using feature selection technique further reduced the\nprocessing time while achieving an even better prediction accuracy overall. For\nthis dataset, Extra Trees should be considered for further modeling or\nproduction use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u2019s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using R","author_name":"David Lowe","blog_date":"Wed, 24 Apr 2019 12:23:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/24\/binary-classification-model-for-parkinsons-disease-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a prediction\nmodel using various machine learning algorithms and to document the end-to-end\nsteps using a template. Parkinson\u2019s Disease dataset is a binary classification\nsituation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul\nUniversity. The control group consists of 64 healthy individuals (23 men and 41\nwomen) with ages varying between 41 and 82. During the data collection process,\nthe microphone is set to 44.1 KHz and following the physician\u2019s examination,\nthe sustained phonation of the vowel was collected from each subject with three\nrepetitions.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 77.84%. Two algorithms (Random\nForest and Stochastic Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, Random\nForest turned in the top overall result and achieved an accuracy metric of\n88.24%. By using the optimized parameters, the Random Forest algorithm\nprocessed the testing dataset with an accuracy of 83.63%, which was just\nslightly below the prediction accuracy using the training data.\nCONCLUSION: For this iteration, the Random Forest algorithm\nachieved the best overall results using the training and testing datasets. For\nthis dataset, Random Forest should be considered for further modeling or\nproduction use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u2019s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Parkinson\u2019s Disease Using Python","author_name":"David Lowe","blog_date":"Tue, 23 Apr 2019 12:16:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/23\/binary-classification-model-for-parkinsons-disease-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Parkinson\u2019s Disease dataset is a binary\nclassification situation where we are trying to predict one of the two possible\noutcomes.\nINTRODUCTION: The data used in this study were gathered from\n188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at\nthe Department of Neurology in Cerrahpasa Faculty of Medicine, Istanbul\nUniversity. The control group consists of 64 healthy individuals (23 men and 41\nwomen) with ages varying between 41 and 82. During the data collection process,\nthe microphone is set to 44.1 KHz and following the physician\u2019s examination,\nthe sustained phonation of the vowel \/a\/ was collected from each subject with\nthree repetitions.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 81.58%. Two algorithms (Extra Trees\nand Stochastic Gradient Boosting) achieved the top accuracy metrics after the\nfirst round of modeling. After a series of tuning trials, Extra Trees turned in\nthe top overall result and achieved an accuracy metric of 88.09%. By using the\noptimized parameters, the Extra Trees algorithm processed the testing dataset\nwith an accuracy of 87.22%, which was just slightly below the prediction\naccuracy using the training data.\nCONCLUSION: For this iteration, the Extra Trees algorithm\nachieved the best overall results using the training and testing datasets. For\nthis dataset, Extra Trees should be considered for further modeling or\nproduction use.\nDataset Used: Parkinson\u2019s Disease Classification Data Set\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Parkinson%27s+Disease+Classification\nSakar, C.O., Serbes, G., Gunduz, A., Tunc, H.C., Nizam, H.,\nSakar, B.E., Tutuncu, M., Aydin, T., Isenkul, M.E. and Apaydin, H., 2018. A\ncomparative analysis of speech signal processing algorithms for Parkinson\u00e2\u20ac\u2122s\ndisease classification and the use of the tunable Q-factor wavelet transform.\nApplied Soft Computing, DOI: https:\/\/doi.org\/10.1016\/j.asoc.2018.10.022\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Song Year Prediction Using Python","author_name":"David Lowe","blog_date":"Mon, 22 Apr 2019 12:17:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/22\/multi-class-classification-model-for-song-year-prediction-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Song Year Prediction dataset is a multi-class\nclassification situation where we are trying to predict one of the ten possible\noutcomes.\nINTRODUCTION: This data is a subset of the Million Song\nDataset, http:\/\/labrosa.ee.columbia.edu\/millionsong\/, a collaboration between\nLabROSA (Columbia University) and The Echo Nest. The purpose of this exercise\nis to predict the release year of a song from audio features. Songs are mostly\nwestern, commercial tracks ranging from 1922 to 2011, with a peak in the year\n2000s. The data preparer recommended the train\/test split of first 463,715\nexamples for training and the last 51,630 examples for testing. This approach\navoids the \u2018producer effect\u2019 by making sure no song from a given artist ends up\nin both the train and test set.\nANALYSIS: The baseline performance of the machine learning\nalgorithms achieved an average accuracy of 57.24%. Two algorithms (Stochastic\nGradient Boosting and eXtreme Gradient Boosting) achieved the top accuracy\nmetrics after the first round of modeling. After a series of tuning trials,\neXtreme Gradient Boosting turned in the top overall result and achieved an\naccuracy metric of 63.85%. By using the optimized parameters, the eXtreme\nGradient Boosting algorithm processed the testing dataset with an accuracy of\n63.65%, which was just slightly below the training data.\nCONCLUSION: For this iteration, the eXtreme Gradient\nBoosting algorithm achieved the best overall results using the training and\ntesting datasets. For this dataset, eXtreme Gradient Boosting should be\nconsidered for further modeling or production use.\nDataset Used: YearPredictionMSD Data Set\nDataset ML Model: Classification with numerical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/YearPredictionMSD\nThierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,\nand Paul Lamere. The Million Song Dataset. In Proceedings of the 12th\nInternational Society for Music Information Retrieval Conference (ISMIR 2011),\n2011.\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/msd-audio-features\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates v9 for R","author_name":"David Lowe","blog_date":"Fri, 19 Apr 2019 12:40:06 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/19\/updated-machine-learning-templates-v9-for-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML)\nproblems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 9 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added or updated the sample code to support:\nMore streamlined email function using the operating system\u2019s environment variablesThe addition of eXtreme Gradient Boosting (XGBOOST) algorithm\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Machine Learning Templates for Google Colaboratory","author_name":"David Lowe","blog_date":"Wed, 17 Apr 2019 12:19:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/17\/machine-learning-templates-for-google-colaboratory\/","blog_text":"\n\nGoogle\u2019s Colaboratory (Colab) is a Jupyter-based research tool for machine learning education and research. The Colab Jupyter notebook environment is a standard virtual machine that requires no setup to provision and to use.\nThe Colab environment\u2019s virtual machine has access to 12 GB\nof RAM, which is sufficient to practice many machine learning problems. The intended\nuse for the virtual machine is interactive modeling and learning. The virtual\nmachine can terminate after some time, and we will need to reconnect to another\nColab virtual machine. Another word, the Colab environment is not suitable for long-running\nmachine learning jobs, but the Colab virtual machine does include access to a GPU.\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly. Thanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression and classification ML problems using Python in Colab.\nYou will find the Python templates (including the ones for Colab) from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Song Year Prediction Using Python","author_name":"David Lowe","blog_date":"Mon, 15 Apr 2019 12:41:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/15\/regression-model-for-song-year-prediction-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Song Year Prediction dataset is a\nclassic regression situation where we are trying to predict the value of a\ncontinuous variable.\nINTRODUCTION: This data is a subset of the Million Song\nDataset, http:\/\/labrosa.ee.columbia.edu\/millionsong\/, a collaboration between\nLabROSA (Columbia University) and The Echo Nest. The purpose of this exercise\nis to predict the release year of a song from audio features. Songs are mostly\nwestern, commercial tracks ranging from 1922 to 2011, with a peak in the year\n2000s. The data preparer recommended the train\/test split of first 463,715\nexamples for training and the last 51,630 examples for testing. This approach\navoids the \u2018producer effect\u2019 by making sure no song from a given artist ends up\nin both the train and test set.\nANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 10.16. Two algorithms (Stochastic Gradient Boosting and eXtreme Gradient Boosting) achieved the top RMSE metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an RMSE metric of 9.04. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with a RMSE of 9.06, which was just slightly above the training data.\nCONCLUSION: For this iteration, the eXtreme Gradient\nBoosting algorithm achieved the best overall results using the training and\ntesting datasets. For this dataset, eXtreme Gradient Boosting should be\nconsidered for further modeling or production use.\nDataset Used: YearPredictionMSD Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/YearPredictionMSD\nThierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman,\nand Paul Lamere. The Million Song Dataset. In Proceedings of the 12th\nInternational Society for Music Information Retrieval Conference (ISMIR 2011),\n2011.\nOne potential source of performance benchmarks:\nhttps:\/\/www.kaggle.com\/uciml\/msd-audio-features\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 2","author_name":"David Lowe","blog_date":"Sun, 14 Apr 2019 12:53:07 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/14\/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by gathering specific pieces of information from a website. The web\nscraping code was written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: A demo website, created by Scrapinghub, lists\nquotes from famous people. It has many endpoints showing the quotes in\ndifferent ways, and each endpoint presents a different scraping challenge for\npracticing web scraping. For this Take2 iteration, the Python script attempts\nto follow the links to the author page and scrape the author information.\nStarting URLs: http:\/\/quotes.toscrape.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of File Download Using Python and BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 14 Apr 2019 12:47:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/14\/web-scraping-of-file-download-using-python-and-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by extracting specific pieces of information from a website. The web\nscraping python code leverages the BeautifulSoup module.\nINTRODUCTION: On occasions we need to download a batch of documents from a single web page without clicking on the download link one at a time. This web scraping script will automatically traverse through the entire web page and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.\nStarting URL: https:\/\/www.knime.com\/about\/events\/knime-spring-summit-2019-berlin\nThe source code and JSON output can be found here for Python and here for R on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates v9 for Python","author_name":"David Lowe","blog_date":"Fri, 12 Apr 2019 12:33:25 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/12\/updated-machine-learning-templates-v9-for-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML)\nproblems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 9 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added or updated the sample code to support:\nMore streamlined email function using the\noperating system\u2019s environment variablesUpdated code for the Synthetic Minority\nOver-sampling Technique (SMOTE) transformationThe addition of eXtreme Gradient Boosting\n(XGBOOST) algorithm\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (XGBoost Tuning Batch #2)","author_name":"David Lowe","blog_date":"Wed, 10 Apr 2019 12:12:59 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/10\/binary-classification-model-for-customer-transaction-prediction-using-python-xgboost-tuning-batch-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\neXtreme Gradient Boosting algorithm with the synthetic over-sampling technique\n(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions\nare evaluated on the area under the ROC\ncurve between the predicted probability and the observed target.\nANALYSIS: We applied different values for the max_depth, min_child_weight,\nsubsample, and colsample_bytree parameters using fixed n_estimators (1000 or\n100). The max_depth values vary from 10, 15, 20 to 25. The min_child_weight values\nvary from 3 to 5 with different learning rates. The subsample and\ncolsample_bytree values vary from 0.6 to 1.0. The following output files are\navailable for comparison.\npy-classification-santander-kaggle-XGB-take11py-classification-santander-kaggle-XGB-take12py-classification-santander-kaggle-XGB-take13py-classification-santander-kaggle-XGB-take14py-classification-santander-kaggle-XGB-take15py-classification-santander-kaggle-XGB-take16py-classification-santander-kaggle-XGB-take17py-classification-santander-kaggle-XGB-take18py-classification-santander-kaggle-XGB-take19py-classification-santander-kaggle-XGB-take21py-classification-santander-kaggle-XGB-take22py-classification-santander-kaggle-XGB-take23py-classification-santander-kaggle-XGB-take24py-classification-santander-kaggle-XGB-take25py-classification-santander-kaggle-XGB-take26py-classification-santander-kaggle-XGB-take27py-classification-santander-kaggle-XGB-take28py-classification-santander-kaggle-XGB-take29\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (XGBoost Tuning Batch #1)","author_name":"David Lowe","blog_date":"Mon, 08 Apr 2019 12:21:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/08\/binary-classification-model-for-customer-transaction-prediction-using-python-xgboost-tuning-batch-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\neXtreme Gradient Boosting algorithm with the synthetic over-sampling technique\n(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions\nare evaluated on the area under the ROC\ncurve between the predicted probability and the observed target.\nANALYSIS: We applied different values for the max_depth, learning_rate,\nand n_estimators parameters. The max_depth values vary from 3 to 6. The learning_rate\nvalues vary from 0.1 to 0.5. The n_estimators values vary from 1000 to 4000. The\nfollowing output files are available for comparison.\npy-classification-santander-kaggle-XGB-take1py-classification-santander-kaggle-XGB-take2py-classification-santander-kaggle-XGB-take3py-classification-santander-kaggle-XGB-take4py-classification-santander-kaggle-XGB-take5py-classification-santander-kaggle-XGB-take6py-classification-santander-kaggle-XGB-take7-part1py-classification-santander-kaggle-XGB-take7-part2py-classification-santander-kaggle-XGB-take7-part3py-classification-santander-kaggle-XGB-take7-part4py-classification-santander-kaggle-XGB-take7-part5py-classification-santander-kaggle-XGB-take8-part1py-classification-santander-kaggle-XGB-take8-part2py-classification-santander-kaggle-XGB-take8-part3py-classification-santander-kaggle-XGB-take8-part4py-classification-santander-kaggle-XGB-take8-part5py-classification-santander-kaggle-XGB-take9-part1py-classification-santander-kaggle-XGB-take9-part2py-classification-santander-kaggle-XGB-take9-part3py-classification-santander-kaggle-XGB-take9-part4py-classification-santander-kaggle-XGB-take9-part5py-classification-santander-kaggle-XGB-take10-part1py-classification-santander-kaggle-XGB-take10-part2py-classification-santander-kaggle-XGB-take10-part3py-classification-santander-kaggle-XGB-take10-part4py-classification-santander-kaggle-XGB-take10-part5\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 3","author_name":"David Lowe","blog_date":"Sun, 07 Apr 2019 12:30:21 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/07\/kaggle-competition-banco-santander-customer-transaction-prediction-update-3\/","blog_text":"\n\nIf you are new to Python machine learning like me, you might find the current Kaggle competition \u201cSantander Customer Transaction Prediction\u201d interesting.\nThe competition is essentially a binary classification\nproblem with a decently large dataset (200 attributes and 200,000 rows of\ntraining data). I have not participated in Kaggle competition before and will\nuse this one to get some learning under the belt.\nI had run the training data through a list of machine\nlearning algorithms (see below) and iterate them through three stages. This\nblog post will serve as the meta post that summarizes the progress.\nThe current plan with the milestones is as follow:\nStage 1: Gather the Baseline Performance.\nLogisticRegression: completed and posted on Monday\n25 February 2019DecisionTreeClassifier: completed and posted on Wednesday\n27 February 2019KNeighborsClassifier: completed and posted on Friday\n1 March 2019BaggingClassifier: completed and posted on Sunday\n3 March 2019RandomForestClassifier: completed and posted on\nMonday 4 March 2019ExtraTreesClassifier: completed and posted on\nWednesday 6 March 2019GradientBoostingClassifier: completed and posted\non Friday 8 March 2019\nStage 2: Feature Selection using the Attribute Importance Ranking\ntechnique\nLogisticRegression: completed and posted on Monday 11 March 2019BaggingClassifier: completed and posted on Wednesday 13 March 2019RandomForestClassifier: completed and posted on Friday 15 March 2019ExtraTreesClassifier: completed and posted on Sunday 17 March 2019GradientBoostingClassifier: completed and posted on Monday 18 March 2019\nStage 3: Over-Sampling (SMOTE) and Balancing Ensembles techniques\nLogisticRegression: completed and posted on Wednesday\n20 March 2019ExtraTreesClassifier: completed and posted on Friday\n22 March 2019RandomForestClassifier: completed and posted on Monday\n25 March 2019GradientBoostingClassifier: completed and posted\non Wednesday 27 March 2019Balanced Bagging: completed and posted on Friday\n29 March 2019Balanced Boosting: completed and posted on Sunday\n31 March 2019Balanced Random Forest: completed and posted on Monday\n1 April 2019XGBoost with Full Feature: completed and posted\non Wednesday 3 April 2019XGBoost with SMOTE: completed and posted on Friday\n5 April 2019\n Stage 4: eXtreme Gradient Boosting Tuning Batches\nBatch #1: planned for Monday 8 April 2019Batch #2: planned for Monday 10 April 2019\nI have posted all Python scripts here on GitHub. The final submission deadline is 10 April 2019.\nFeel free to take a look at the scripts and experiment. Who knows, you\nmight have something you can turn in by the time April comes around. Happy\nlearning and good luck!\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (eXtreme Gradient Boosting with SMOTE)","author_name":"David Lowe","blog_date":"Fri, 05 Apr 2019 12:59:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/05\/binary-classification-model-for-customer-transaction-prediction-using-python-extreme-gradient-boosting-with-smote\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\neXtreme Gradient Boosting algorithm with the synthetic over-sampling technique\n(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions\nare evaluated on the area under the ROC curve between the predicted probability\nand the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.9129. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.9584. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.6560.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (eXtreme Gradient Boosting with Full Features)","author_name":"David Lowe","blog_date":"Wed, 03 Apr 2019 12:58:19 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/03\/binary-classification-model-for-customer-transaction-prediction-using-python-extreme-gradient-boosting-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\neXtreme Gradient Boosting (XGBoost) algorithm with the full set of features for\nthis problem. Submissions are evaluated\non the area under the ROC curve between the predicted probability and the\nobserved target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8315. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8908. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.6496.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with SMOTE)","author_name":"David Lowe","blog_date":"Mon, 01 Apr 2019 12:03:34 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/04\/01\/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-smote\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nRandom Forest algorithm with the synthetic over-sampling technique (SMOTE) to\nmitigate the effect of imbalanced data for this problem. Submissions are\nevaluated on the area under the ROC curve between the predicted probability and\nthe observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.9695. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.9972. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5096.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Gradient Boosting with SMOTE)","author_name":"David Lowe","blog_date":"Sun, 31 Mar 2019 12:00:49 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/31\/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-smote-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nGradient Boosting algorithm with the synthetic over-sampling technique (SMOTE)\nto mitigate the effect of imbalanced data for this problem. Submissions are\nevaluated on the area under the ROC curve between the predicted probability and\nthe observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.9092. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.9405. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.6289.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Bagging)","author_name":"David Lowe","blog_date":"Fri, 29 Mar 2019 12:45:38 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/29\/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-bagging\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nBalanced Bagging classifier (from the imbalanced-learn package) with inner\nbalancing samplers to mitigate the effect of imbalanced data for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.7144. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.7799. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.6659.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Boosting)","author_name":"David Lowe","blog_date":"Wed, 27 Mar 2019 12:39:48 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/27\/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-boosting\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nBalanced Boosting classifier (from the imbalanced-learn package) with inner\nbalancing samplers to mitigate the effect of imbalanced data for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average ROC-AUC score of 0.6844. After a series of tuning trials, the top result from the training data was a ROC-AUC score of 0.6860. By using the optimized parameters, the algorithm processed the test dataset with a ROC-AUC score of 0.5681.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Balanced Random Forest)","author_name":"David Lowe","blog_date":"Mon, 25 Mar 2019 12:28:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/25\/binary-classification-model-for-customer-transaction-prediction-using-python-balanced-random-forest\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nBalanced Random Forest classifier (from the imbalanced-learn package) with\ninner balancing samplers to mitigate the effect of imbalanced data for this\nproblem. Submissions are evaluated on the area under the ROC curve between the\npredicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8224. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8660. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.7761.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 2","author_name":"David Lowe","blog_date":"Sun, 24 Mar 2019 12:32:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/24\/kaggle-competition-banco-santander-customer-transaction-prediction-update-2\/","blog_text":"\n\nIf you are new to Python machine learning like me, you might find the current Kaggle competition \u201cSantander Customer Transaction Prediction\u201d interesting.\nThe competition is essentially a binary classification\nproblem with a decently large dataset (200 attributes and 200,000 rows of\ntraining data). I have not participated in Kaggle competition before and will\nuse this one to get some learning under the belt.\nI plan to run the training data through a list of machine\nlearning algorithms (see below) and iterate them through three stages. This\nblog post will serve as the meta post that summarizes the progress.\nThe current plan with the milestones is as follow:\nStage 1: Gather the Baseline Performance.\nLogisticRegression: completed and posted on Monday\n25 February 2019DecisionTreeClassifier: completed and posted on Wednesday\n27 February 2019KNeighborsClassifier: completed and posted on Friday\n1 March 2019BaggingClassifier: completed and posted on Sunday\n3 March 2019RandomForestClassifier: completed and posted on\nMonday 4 March 2019ExtraTreesClassifier: completed and posted on\nWednesday 6 March 2019GradientBoostingClassifier: completed and posted\non Friday 8 March 2019\nStage 2: Feature Selection using the Attribute Importance Ranking\ntechnique\nBaggingClassifier: completed and posted on Wednesday\n13 March 2019RandomForestClassifier: completed and posted on Friday\n15 March 2019ExtraTreesClassifier: completed and posted on Sunday\n17 March 2019GradientBoostingClassifier: completed and posted\non Monday 18 March 2019\nStage 3: Over-Sampling (SMOTE) and Balancing Ensembles techniques\nLogisticRegression: completed and posted on Wednesday\n20 March 2019ExtraTreesClassifier: completed and posted on Friday\n22 March 2019RandomForestClassifier: planned for Monday 25\nMarch 2019GradientBoostingClassifier: planned for Wednesday\n27 March 2019Balanced Bagging: planned for Friday 29 March\n2019Balanced Boosting: planned for Sunday 31 March\n2019Balanced Random Forest: planned for Monday 1 April\n2019\nI post all Python scripts here on GitHub. The final submission deadline is 10 April 2019.\nFeel free to take a look at the scripts and experiment. Who knows, you\nmight have something you can turn in by the time April comes around. Happy\nlearning and good luck!\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with SMOTE)","author_name":"David Lowe","blog_date":"Sat, 23 Mar 2019 00:52:12 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/22\/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-smote\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nExtra Trees algorithm with the synthetic over-sampling technique (SMOTE) to\nmitigate the effect of imbalanced data for this problem. Submissions are\nevaluated on the area under the ROC curve between the predicted probability and\nthe observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.9769. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.9986. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5036.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with SMOTE)","author_name":"David Lowe","blog_date":"Wed, 20 Mar 2019 12:57:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/20\/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-smote\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nLogistic Regression algorithm with the synthetic over-sampling technique\n(SMOTE) to mitigate the effect of imbalanced data for this problem. Submissions\nare evaluated on the area under the ROC curve between the predicted probability\nand the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8765. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8788. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.7776.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Gradient Boosting with Attribute Importance Ranking)","author_name":"David Lowe","blog_date":"Mon, 18 Mar 2019 12:32:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/18\/binary-classification-model-for-customer-transaction-prediction-using-python-gradient-boosting-with-attribute-importance-ranking\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nGradient Boosting algorithm with a reduced set of features (derived from using\nthe Attribute Importance Ranking technique with the GradientBoostingClassifier\nalgorithm) for this problem. Submissions are evaluated on the area under the\nROC curve between the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8322. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8619. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5798.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with Attribute Importance Ranking)","author_name":"David Lowe","blog_date":"Sun, 17 Mar 2019 12:30:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/17\/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-attribute-importance-ranking\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nRandom Forest algorithm with a reduced set of features (derived from using the\nAttribute Importance Ranking technique with the GradientBoostingClassifier\nalgorithm) for this problem. Submissions are evaluated on the area under the\nROC curve between the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.7208. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8330. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5013.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with Attribute Importance Ranking)","author_name":"David Lowe","blog_date":"Fri, 15 Mar 2019 12:42:22 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/15\/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-attribute-importance-ranking\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nExtra Trees algorithm with a reduced set of features (derived from using the\nAttribute Importance Ranking technique with the GradientBoostingClassifier\nalgorithm) for this problem. Submissions are evaluated on the area under the\nROC curve between the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.6658. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8441. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5000.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Bagged Decision Trees with Attribute Importance Ranking)","author_name":"David Lowe","blog_date":"Wed, 13 Mar 2019 12:37:50 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/13\/binary-classification-model-for-customer-transaction-prediction-using-python-bagged-decision-trees-with-attribute-importance-ranking\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nBagged Decision Trees algorithm with a reduced set of features (derived from\nusing the Attribute Importance Ranking technique with the\nGradientBoostingClassifier algorithm) for this problem. Submissions are\nevaluated on the area under the ROC curve between the predicted probability and\nthe observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.7280. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.7823. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5136.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with Attribute Importance Ranking)","author_name":"David Lowe","blog_date":"Mon, 11 Mar 2019 12:55:16 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/11\/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-attribute-importance-ranking\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nLogistic Regression algorithm with a reduced set of features (derived from\nusing the Attribute Importance Ranking technique with the\nGradientBoostingClassifier algorithm) for this problem. Submissions are evaluated on the area under the ROC curve\nbetween the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8279. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8317. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5912.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction Update 1","author_name":"David Lowe","blog_date":"Sun, 10 Mar 2019 12:14:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/10\/kaggle-competition-banco-santander-customer-transaction-prediction-update-1\/","blog_text":"\n\nIf you are new to Python machine learning like me, you might find the current Kaggle competition \u201cSantander Customer Transaction Prediction\u201d interesting.\nThe competition is essentially a binary classification\nproblem with a decently large dataset (200 attributes and 200,000 rows of\ntraining data). I have not participated in Kaggle competition before and will\nuse this one to get some learning under the belt.\nI plan to run the training data through a list of machine\nlearning algorithms (see below) and iterate them through three stages. This\nblog post will serve as the meta post that summarizes the progress.\nThe current plan with the milestones is as follow:\nStage 1: Gather the Baseline Performance.\nLogisticRegression: completed and posted on Monday\n25 February 2019DecisionTreeClassifier: completed and posted on Wednesday\n27 February 2019KNeighborsClassifier: completed and posted on Friday\n1 March 2019BaggingClassifier: completed and posted on Sunday\n3 March 2019RandomForestClassifier: completed and posted on Monday\n4 March 2019ExtraTreesClassifier: completed and posted on Wednesday\n6 March 2019GradientBoostingClassifier: completed and posted\non Friday 8 March 2019\nStage 2: Feature Selection using the Attribute Importance Ranking\ntechnique\nLogisticRegression: planned for Monday 11 March 2019BaggingClassifier: planned for Wednesday 13 March 2019RandomForestClassifier: planned for Friday 15 March 2019ExtraTreesClassifier: planned for Sunday 17 March 2019GradientBoostingClassifier: planned for Monday 18 March 2019\nStage 3: Over-Sampling and Balancing Ensembles techniques (TBD)\nI will post all Python script in a folder on GitHub. The final submission deadline is 10 April 2019.\nFeel free to take a look at the scripts and experiment. Who knows, you\nmight have something you can turn in by the time April comes around. Happy\nlearning and good luck!\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Stochastic Gradient Boosting with Full Features)","author_name":"David Lowe","blog_date":"Fri, 08 Mar 2019 13:46:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/08\/binary-classification-model-for-customer-transaction-prediction-using-python-stochastic-gradient-boosting-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nStochastic Gradient Boosting algorithm with the full set of features for this\nproblem. Submissions are evaluated on the area under the ROC curve between the\npredicted probability and the observed target.\n ANALYSIS: The baseline performance achieved an average ROC-AUC score of  0.8337. After a series of tuning trials, the top result from the  training data was a ROC-AUC score of 0.8720. By using the optimized  parameters, the algorithm processed the test dataset with a ROC-AUC  score of 0.5684. \nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Random Forest with Full Features)","author_name":"David Lowe","blog_date":"Wed, 06 Mar 2019 13:59:23 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/06\/binary-classification-model-for-customer-transaction-prediction-using-python-random-forest-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nRandom Forest algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.6984. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8377. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5000.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Extra Trees with Full Features)","author_name":"David Lowe","blog_date":"Mon, 04 Mar 2019 13:19:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/04\/binary-classification-model-for-customer-transaction-prediction-using-python-extra-trees-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nExtra Trees algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.6298. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8482. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5000.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Bagged Decision Trees with Full Features)","author_name":"David Lowe","blog_date":"Sun, 03 Mar 2019 13:45:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/03\/binary-classification-model-for-customer-transaction-prediction-using-python-bagged-decision-trees-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nBagged Decision Trees algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.7112. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.7696. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5054.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Decision Trees with Full Features)","author_name":"David Lowe","blog_date":"Fri, 01 Mar 2019 13:39:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/03\/01\/binary-classification-model-for-customer-transaction-prediction-using-python-decision-trees-with-full-features-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nk-Nearest Neighbors algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area under the ROC curve between the predicted\nprobability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.5373. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.7238. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5000.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Decision Trees with Full Features)","author_name":"David Lowe","blog_date":"Wed, 27 Feb 2019 13:39:58 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/27\/binary-classification-model-for-customer-transaction-prediction-using-python-decision-trees-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nDecision Trees algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area\nunder the ROC curve between the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.5525. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.5646. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.5604.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Customer Transaction Prediction Using Python (Logistic Regression with Full Features)","author_name":"David Lowe","blog_date":"Mon, 25 Feb 2019 13:37:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/25\/binary-classification-model-for-customer-transaction-prediction-using-python-logistic-regression-with-full-features\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Santander Bank Customer Transaction\nPrediction competition is a binary classification situation where we are trying\nto predict one of the two possible outcomes.\nINTRODUCTION: Santander Bank\u2019s data science team wants to\nidentify which customers will make a specific transaction in the future,\nirrespective of the amount of money transacted. The bank is continually\nchallenging its machine learning algorithms to make sure they can more\naccurately identify new ways to solve its most common challenges such as: Will\na customer buy this product? Can a customer pay this loan?\nFor this iteration, we will examine the effectiveness of the\nLogistic Regression algorithm with the full set of features for this problem.\nSubmissions are evaluated on the area\nunder the ROC curve between the predicted probability and the observed target.\nANALYSIS: The baseline performance achieved an average\nROC-AUC score of 0.8523. After a series of tuning trials, the top result from\nthe training data was a ROC-AUC score of 0.8593. By using the optimized\nparameters, the algorithm processed the test dataset with a ROC-AUC score of\n0.6276.\nCONCLUSION: To be determined after comparing the results\nfrom other machine learning algorithms.\nDataset Used: Santander Customer Transaction Prediction\nDataset ML Model: Binary classification with numerical\nattributes\nDataset Reference:\nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Kaggle Competition: Banco Santander Customer Transaction Prediction","author_name":"David Lowe","blog_date":"Sun, 24 Feb 2019 13:24:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/24\/kaggle-competition-banco-santander-customer-transaction-prediction\/","blog_text":"\n\nIf you are new to Python machine learning like me, you might find the current Kaggle competition \u201cSantander Customer Transaction Prediction\u201d interesting.\nThe competition is essentially a binary classification\nproblem with a decently large dataset (200 attributes and 200,000 rows of\ntraining data). I have not participated in Kaggle competition before and will\nuse this one to get some learning under the belt.\nI plan to run the training data through a list of machine\nlearning algorithms (see below) and iterate them through three stages. This blog\npost will serve as the meta post that summarizes the progress.\nThe current plan with the milestones are as follow:\nStage 1: Gather the Baseline Performance.\nLogisticRegression: targeted Monday 25 February\n2019DecisionTreeClassifier: targeted Wednesday 27 February\n2019KNeighborsClassifier: targeted Friday 1 March\n2019BaggingClassifier: targeted Monday 4 March 2019RandomForestClassifier: targeted Wednesday 6\nMarch 2019ExtraTreesClassifier: targeted Friday 8 March\n2019GradientBoostingClassifier: TBD\nStage 2: Feature Selection using the Attribute Importance Ranking technique (TBD)\nStage 2: Feature Selection using the Recursive Feature Elimination technique (TBD)\nI will post all Python script in a folder on GitHub. The final submission deadline is 10 April 2019.\nFeel free to take a look at the scripts and experiment. Who knows, you\nmight have something you can turn in by the time April comes around. Happy\nlearning and good luck!\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates v8 for R","author_name":"David Lowe","blog_date":"Fri, 22 Feb 2019 13:03:49 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/22\/updated-machine-learning-templates-v8-for-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML)\nproblems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 8 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support:\nDownloading the data file from an URLUnzipping the data file archive if necessary\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates v8 for Python","author_name":"David Lowe","blog_date":"Wed, 20 Feb 2019 13:07:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/20\/updated-machine-learning-templates-v8-for-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML)\nproblems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 8 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support:\nDownloading the data file from an URLUnzipping the data file archive if necessary\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 4","author_name":"David Lowe","blog_date":"Mon, 18 Feb 2019 13:50:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/18\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning\nalgorithms and to document the end-to-end steps using a template. The Human\nActivities and Postural Transitions dataset is a classic multi-class\nclassification situation where we are trying to predict one of the 12 possible\noutcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are\nstand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and\nlie-to-stand. All the participants were wearing a smartphone on the waist\nduring the experiment execution. The research team also video-recorded the\nactivities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and\n30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of eliminating collinear features. We performed iterative modeling at\ncollinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear\nfeatures, we decreased the processing time and maintained a comparable level of\nmodel accuracy comparing to iteration Take1.\nIn iteration Take3, we examined the feature selection\ntechnique of attribute importance ranking by using the Random Forest algorithm.\nBy selecting only the most important attributes, we hoped to decrease the\nprocessing time and maintain a similar level of accuracy compared to iteration\nTake1.\nIn the current iteration Take4, we will examine the feature\nselection technique of Recursive Feature Elimination by using the Random Forest\nalgorithm. By limiting to only the 300 most relevant attributes, we hope to\ndecrease the processing time and maintain a similar level of accuracy compared\nto iteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 89.61%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Stochastic Gradient Boosting turned in the top overall\nresult and achieved an accuracy metric of 97.70%. By using the optimized\nparameters, the Stochastic Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 92.85%, which was below the training data and\npossibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 89.48%. Two algorithms\n(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, eXtreme\nGradient Boosting turned in the top overall result and achieved an accuracy\nmetric of 98.31%. By using the optimized parameters, the eXtreme Gradient\nBoosting algorithm processed the testing dataset with an accuracy of 94.97%,\nwhich was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 278, from 561 down to 283. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in\nTake2, which was a reduction of 23.9%.\nIn iteration Take3, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 90.04%. Two algorithms\n(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, eXtreme\nGradient Boosting turned in the top overall result and achieved an accuracy\nmetric of 98.35%. By using the optimized parameters, the eXtreme Gradient\nBoosting algorithm processed the testing dataset with an accuracy of 93.86%,\nwhich was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 62, from 561 down to 499. The processing time went from\n10 hours 17 minutes in iteration Take1 up to 16 hours 59 minutes in Take3,\nwhich was an increase of 65.1%.\nIn the current iteration Take4, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 89.91%. Two\nalgorithms (Random Forest and eXtreme Gradient Boosting) achieved the top\naccuracy metrics after the first round of modeling. After a series of tuning\ntrials, eXtreme Gradient Boosting turned in the top overall result and achieved\nan accuracy metric of 98.40%. By using the optimized parameters, the eXtreme\nGradient Boosting algorithm processed the testing dataset with an accuracy of\n93.77%, which was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 311, from 561 down to 250. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 7 hours 56 minutes in\nTake4, which was a decrease of 22.8%.\nCONCLUSION: For this iteration, the Recursive Feature\nElimination technique and using the eXtreme Gradient Boosting algorithm\nachieved the best overall result. For this dataset, we should consider using\nthe eXtreme Gradient Boosting algorithm for further modeling or production use.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using BeautifulSoup Take 1","author_name":"David Lowe","blog_date":"Sun, 17 Feb 2019 13:30:51 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/17\/web-scraping-of-quotes-from-famous-people-using-beautifulsoup-take-1\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the Python script attempts to follow the page links and scrape the quote information off each page.\nStarting URLs: http:\/\/quotes.toscrape.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 4","author_name":"David Lowe","blog_date":"Fri, 15 Feb 2019 13:42:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/15\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Human Activities and Postural\nTransitions dataset is a classic multi-class classification situation where we\nare trying to predict one of the 12 possible outcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,\nlie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a\nsmartphone on the waist during the experiment execution. The research team also\nvideo-recorded the activities to label the data manually. The research team\nrandomly partitioned the obtained data into two sets, 70% for the training data\nand 30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of eliminating collinear features. We performed iterative modeling at\ncollinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear\nfeatures, we decreased the processing time and maintained a comparable level of\nmodel accuracy comparing to iteration Take1.\nIn iteration Take3, we examined the feature selection\ntechnique of attribute importance ranking by using the Random Forest algorithm.\nBy selecting only the most important attributes, we hoped to decrease the\nprocessing time and to maintain a similar level of accuracy compared to\niteration Take1.\nIn the current iteration Take4, we will examine the feature\nselection technique of Recursive Feature Elimination by using the Linear\nDiscriminant Analysis algorithm. By limiting to only the 300 most relevant\nattributes, we hope to decrease the processing time and maintain a similar\nlevel of accuracy compared to iteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 88.52%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 94.19%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 94.71%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 88.04%. Two algorithms\n(Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the\ntop accuracy metrics after the first round of modeling. After a series of\ntuning trials, Linear Discriminant Analysis turned in the top overall result\nand achieved an accuracy metric of 92.32%. By using the optimized parameters,\nthe Linear Discriminant Analysis algorithm processed the testing dataset with\nan accuracy of 93.89%, which was even better than the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 278, from 561 down to 283. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 4 hours 31 minutes in Take2,\nwhich was a reduction of 35.9%.\nIn iteration Take3, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 89.02%. Two algorithms\n(Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the\ntop accuracy metrics after the first round of modeling. After a series of\ntuning trials, Linear Discriminant Analysis turned in the top overall result\nand achieved an accuracy metric of 94.42%. By using the optimized parameters,\nthe Linear Discriminant Analysis algorithm processed the testing dataset with\nan accuracy of 94.97%, which was even better than the training data.\nFrom the model-building perspective, the number of\nattributes decreased by 107, from 561 down to 454. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 6 hours 06 minutes in Take3,\nwhich was a reduction of 13.4%.\nIn the current iteration Take4, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 87.22%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 91.45%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 91.42%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 261, from 561 down to 300. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 4 hours 57 minutes in Take4,\nwhich was a reduction of 29.7%.\nCONCLUSION: For this iteration, the Recursive Feature\nElimination technique and the Linear Discriminant Analysis algorithm achieved\nthe best overall results while reducing the processing time. For this dataset,\nwe should consider using the Linear Discriminant Analysis algorithm for further\nmodeling or production use.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 3","author_name":"David Lowe","blog_date":"Wed, 13 Feb 2019 13:18:31 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/13\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning\nalgorithms and to document the end-to-end steps using a template. The Human\nActivities and Postural Transitions dataset is a classic multi-class\nclassification situation where we are trying to predict one of the 12 possible\noutcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are\nstand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and\nlie-to-stand. All the participants were wearing a smartphone on the waist\nduring the experiment execution. The research team also video-recorded the\nactivities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and\n30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of eliminating collinear features. We performed iterative modeling at\ncollinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear\nfeatures, we decreased the processing time and maintained a comparable level of\nmodel accuracy comparing to iteration Take1.\nIn the current iteration Take3, we will examine the feature\nselection technique of attribute importance ranking by using the Random Forest\nalgorithm. By selecting only the most important attributes, we hope to decrease\nthe processing time and maintain a similar level of accuracy compared to\niteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 89.61%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Stochastic Gradient Boosting turned in the top overall\nresult and achieved an accuracy metric of 97.70%. By using the optimized\nparameters, the Stochastic Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 92.85%, which was below the training data and\npossibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 89.48%. Two algorithms\n(Random Forest and eXtreme Gradient Boosting) achieved the top accuracy metrics\nafter the first round of modeling. After a series of tuning trials, eXtreme\nGradient Boosting turned in the top overall result and achieved an accuracy\nmetric of 98.31%. By using the optimized parameters, the eXtreme Gradient\nBoosting algorithm processed the testing dataset with an accuracy of 94.97%,\nwhich was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 278, from 561 down to 283. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in\nTake2, which was a reduction of 23.9%.\nIn the current iteration Take3, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 90.04%. Two\nalgorithms (Random Forest and eXtreme Gradient Boosting) achieved the top\naccuracy metrics after the first round of modeling. After a series of tuning\ntrials, eXtreme Gradient Boosting turned in the top overall result and achieved\nan accuracy metric of 98.35%. By using the optimized parameters, the eXtreme\nGradient Boosting algorithm processed the testing dataset with an accuracy of\n93.86%, which was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 62, from 561 down to 499. The processing time went from\n10 hours 17 minutes in iteration Take1 up to 16 hours 59 minutes in Take3,\nwhich was an increase of 65.1%.\nCONCLUSION: For this iteration, the attribute importance\nranking technique and using the eXtreme Gradient Boosting algorithm achieved\nthe best overall results. For this dataset, we should consider using the\neXtreme Gradient Boosting algorithm for further modeling or production use.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 3","author_name":"David Lowe","blog_date":"Mon, 11 Feb 2019 13:12:09 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/11\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Human Activities and Postural\nTransitions dataset is a classic multi-class classification situation where we\nare trying to predict one of the 12 possible outcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,\nlie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a\nsmartphone on the waist during the experiment execution. The research team also\nvideo-recorded the activities to label the data manually. The research team\nrandomly partitioned the obtained data into two sets, 70% for the training data\nand 30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn iteration Take2, we examined the feature selection\ntechnique of eliminating collinear features. We performed iterative modeling at\ncollinear levels of 75%, 80%, 85%, 90%, and 95%. By eliminating the collinear\nfeatures, we decreased the processing time and maintained a comparable level of\nmodel accuracy comparing to iteration Take1.\nIn the current iteration Take3, we will examine the feature\nselection technique of attribute importance ranking by using the Random Forest\nalgorithm. By selecting only the most important attributes, we hope to decrease\nthe processing time and maintain a similar level of accuracy compared to\niteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 88.52%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 94.19%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 94.71%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nIn iteration Take2, the baseline performance of the machine\nlearning algorithms achieved an average accuracy of 88.04%. Two algorithms (Linear\nDiscriminant Analysis and Stochastic Gradient Boosting) achieved the top\naccuracy metrics after the first round of modeling. After a series of tuning\ntrials, Linear Discriminant Analysis turned in the top overall result and\nachieved an accuracy metric of 92.32%. By using the optimized parameters, the\nLinear Discriminant Analysis algorithm processed the testing dataset with an\naccuracy of 93.89%, which was even better than the training data.\nFrom the model-building perspective, the number of attributes\ndecreased by 278, from 561 down to 283. The processing time went from 7 hours 3\nminutes in iteration Take1 down to 4 hours 31 minutes in Take2, which was a\nreduction of 35.9%.\nIn the current iteration Take3, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 89.02%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 94.42%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 94.97%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 107, from 561 down to 454. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 6 hours 06 minutes in Take3,\nwhich was a reduction of 13.4%.\nCONCLUSION: For this iteration, the attribute importance\nranking technique and the Linear Discriminant Analysis algorithm achieved the\nbest overall results while reducing the processing time. For this dataset, we\nshould consider using the Linear Discriminant Analysis algorithm for further\nmodeling or production use.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping Templates for R and Python","author_name":"David Lowe","blog_date":"Sun, 10 Feb 2019 13:05:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/10\/web-scraping-templates-for-r-and-python\/","blog_text":"\n\nAs I work on practicing and solving web scraping problems, I\nfind myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support web scraping tasks using R or Python.\nThe R scripts leverage the rvest package, while the Python scripts leverage the Scrapy framework. You will find the web scraping templates from the Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 2","author_name":"David Lowe","blog_date":"Fri, 08 Feb 2019 13:06:31 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/08\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning\nalgorithms and to document the end-to-end steps using a template. The Human\nActivities and Postural Transitions dataset is a classic multi-class\nclassification situation where we are trying to predict one of the 12 possible\noutcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are\nstand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and\nlie-to-stand. All the participants were wearing a smartphone on the waist\nduring the experiment execution. The research team also video-recorded the\nactivities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and\n30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn the current iteration Take2, we will examine the feature\nselection technique of eliminating collinear features. We will perform\niterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By\neliminating the collinear features, we hope to decrease the processing time and\nmaintain a comparable level of model accuracy comparing to iteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 89.61%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Stochastic Gradient Boosting turned in the top overall\nresult and achieved an accuracy metric of 97.70%. By using the optimized\nparameters, the Stochastic Gradient Boosting algorithm processed the testing\ndataset with an accuracy of 92.85%, which was below the training data and\npossibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nCOL_75%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n89.75%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved\nthe top accuracy metrics after the first round of modeling. After a series of\ntuning trials, eXtreme Gradient Boosting turned in the top overall result and\nachieved an accuracy metric of 97.84%. By using the optimized parameters, the\neXtreme Gradient Boosting algorithm processed the testing dataset with an\naccuracy of 94.75%, which was below the training data and possibly due to\nover-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 408, from 561 down to 153. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 4 hours 26 minutes in\nTake2, which was a reduction of 66.6%.\nCOL_80%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n90.27%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved\nthe top accuracy metrics after the first round of modeling. After a series of\ntuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved\nan accuracy metric of 98.00%. By using the optimized parameters, the eXtreme\nGradient Boosting algorithm processed the testing dataset with an accuracy of\n94.91%, which was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 385, from 561 down to 176. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 4 hours 56 minutes in\nTake2, which was a reduction of 47.9%.\nCOL_85%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n89.31%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved\nthe top accuracy metrics after the first round of modeling. After a series of\ntuning trials, eXtreme Gradient Boosting turned in the top overall result and\nachieved an accuracy metric of 98.14%. By using the optimized parameters, the\neXtreme Gradient Boosting algorithm processed the testing dataset with an\naccuracy of 94.15%, which was below the training data and possibly due to\nover-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 362, from 561 down to 199. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 5 hours 27 minutes in\nTake2, which was a reduction of 47.0%.\nCOL_90%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n89.38%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved\nthe top accuracy metrics after the first round of modeling. After a series of\ntuning trials, eXtreme Gradient Boosting turned in the top overall result and\nachieved an accuracy metric of 98.17%. By using the optimized parameters, the\neXtreme Gradient Boosting algorithm processed the testing dataset with an\naccuracy of 94.12%, which was below the training data and possibly due to\nover-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 338, from 561 down to 223. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 6 hours 2 minutes in Take2,\nwhich was a reduction of 41.3%.\nCOL_95%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n89.48%. Two algorithms (Random Forest and eXtreme Gradient Boosting) achieved\nthe top accuracy metrics after the first round of modeling. After a series of\ntuning trials, eXtreme Gradient Boosting turned in the top overall result and\nachieved an accuracy metric of 98.31%. By using the optimized parameters, the\neXtreme Gradient Boosting algorithm processed the testing dataset with an\naccuracy of 94.97%, which was below the training data and possibly due to\nover-fitting.\nFrom the model-building perspective, the number of\nattributes decreased by 278, from 561 down to 283. The processing time went\nfrom 10 hours 17 minutes in iteration Take1 down to 7 hours 49 minutes in\nTake2, which was a reduction of 23.9%.\nCONCLUSION: For this iteration, eliminating collinear\nfeatures at the 95% level and using the eXtreme Gradient Boosting algorithm\nachieved the best overall results. For this dataset, we should consider using\nthe eXtreme Gradient Boosting algorithm for further modeling or production use.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Time Series Model for Monthly Sales of French Champagne Using Python","author_name":"David Lowe","blog_date":"Wed, 06 Feb 2019 13:05:26 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/06\/time-series-model-for-annual-water-usage-in-baltimore-using-python-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nCode Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.\nPREFACE: This is a replication of Python code from Dr.\nBrownlee\u2019s blog post on time series. I have combined all the code snippets into\none script so that I can turn the whole process into a template. The comments\nand analysis were also part of the blog post and annotated here to explain each\ncoding block.\nSUMMARY: The purpose of this project is to construct a time\nseries prediction model and document the end-to-end steps using a template. The\nMonthly Sales of French Champagne dataset is a time series situation where we\nare trying to forecast future outcomes based on the past data points.\nINTRODUCTION: The problem is to predict the number of\nmonthly sales of champagne for the Perrin Freres label (named for a region in\nFrance). The dataset provides the number of monthly sales of champagne from\nJanuary 1964 to September 1972, or just under 10 years of data. The values are\na count of millions of sales and there are 105 observations. The dataset is\ncredited to Makridakis and Wheelwright, 1989.\nANALYSIS: The baseline prediction (or persistence) for the\ndataset resulted in an RMSE of 3186.501. The manually configured model was\nsimplified to ARIMA(1,1,1) and produced an RMSE of 956.958, which is\ndramatically better than the persistence RMSE of 3186.501. After applying the\ngrid search technique to the dataset, the final RMSE of the ARIMA(0,0,1) model\nwas 939.464, which is slightly lower than the manually configured ARIMA from the\nprevious section. This difference may or may not be statistically significant.\nAt the end, we selected ARIMA(0,0,1) as the final model.\nCONCLUSION: The final RMSE for the validation period is\npredicted at 361 million sales. This is much better than the expectation of an\nerror of a little more than 924 million sales per month. At this scale on the\nplot, the 12 months of forecast sales figures look fantastic.\nDataset Used: Monthly Sales of French Champagne\nDataset ML Model: Time series forecast with numerical\nattributes\nDataset Reference:\nhttps:\/\/datamarket.com\/data\/set\/22r5\/perrin-freres-monthly-champagne-sales-millions-64-72#!ds=22r5&display=line\nOne potential source of performance benchmark:\nhttps:\/\/machinelearningmastery.com\/time-series-forecast-study-python-monthly-sales-french-champagne\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 04 Feb 2019 13:33:45 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/04\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. \nSUMMARY: The purpose of this project is to construct a\nprediction model using various machine learning algorithms and to document the\nend-to-end steps using a template. The Human Activities and Postural\nTransitions dataset is a classic multi-class classification situation where we\nare trying to predict one of the 12 possible outcomes.\nINTRODUCTION: The research team carried out experiments with\na group of 30 volunteers who performed a protocol of activities composed of six\nbasic activities. There are three static postures (standing, sitting, lying)\nand three dynamic activities (walking, walking downstairs and walking\nupstairs). The experiment also included postural transitions that occurred\nbetween the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie,\nlie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a\nsmartphone on the waist during the experiment execution. The research team also\nvideo-recorded the activities to label the data manually. The research team\nrandomly partitioned the obtained data into two sets, 70% for the training data\nand 30% for the testing.\nIn iteration Take1, the script focused on evaluating various\nmachine learning algorithms and identifying the model that produces the best\noverall metrics. Because the dataset has many attributes that were collinear\nwith other attributes, we eliminated the attributes that have a collinearity\nmeasurement of 99% or higher. Iteration Take1 established the performance\nbaseline for accuracy and processing time.\nIn the current iteration Take2, we will examine the feature\nselection technique of eliminating collinear features. We will perform\niterative modeling at collinear levels of 75%, 80%, 85%, 90%, and 95%. By\neliminating the collinear features, we hope to decrease the processing time and\nmaintain a comparable level of model accuracy comparing to iteration Take1.\nANALYSIS: In iteration Take1, the baseline performance of\nthe machine learning algorithms achieved an average accuracy of 88.52%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 94.19%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 94.71%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 108, from 561 down to 453.\nCOL_75%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n88.53%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient\nBoosting) achieved the top accuracy metrics after the first round of modeling.\nAfter a series of tuning trials, Stochastic Gradient Boosting turned in the top\noverall result and achieved an accuracy metric of 91.63%. By using the\noptimized parameters, the Stochastic Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 92.53%, which was even better than the\ntraining data.\nFrom the model-building perspective, the number of\nattributes decreased by 408, from 561 down to 153. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 2 hours 48 minutes in Take2,\nwhich was a reduction of 60.2%.\nCOL_80%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n85.96%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient\nBoosting) achieved the top accuracy metrics after the first round of modeling.\nAfter a series of tuning trials, Stochastic Gradient Boosting turned in the top\noverall result and achieved an accuracy metric of 91.86%. By using the\noptimized parameters, the Stochastic Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 93.13%, which was even better than the\ntraining data.\nFrom the model-building perspective, the number of\nattributes decreased by 385, from 561 down to 176. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 3 hours 12 minutes in Take2,\nwhich was a reduction of 54.6%.\nCOL_85%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n87.32%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient\nBoosting) achieved the top accuracy metrics after the first round of modeling.\nAfter a series of tuning trials, Stochastic Gradient Boosting turned in the top\noverall result and achieved an accuracy metric of 91.16%. By using the\noptimized parameters, the Stochastic Gradient Boosting algorithm processed the\ntesting dataset with an accuracy of 92.47%, which was even better than the\ntraining data.\nFrom the model-building perspective, the number of\nattributes decreased by 362, from 561 down to 199. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 3 hours 37 minutes in Take2,\nwhich was a reduction of 48.6%.\nCOL_90%: In the current iteration Take2, the baseline performance\nof the machine learning algorithms achieved an average accuracy of 87.22%. Two\nalgorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting)\nachieved the top accuracy metrics after the first round of modeling. After a\nseries of tuning trials, Linear Discriminant Analysis turned in the top overall\nresult and achieved an accuracy metric of 91.52%. By using the optimized\nparameters, the Linear Discriminant Analysis algorithm processed the testing\ndataset with an accuracy of 92.82%, which was even better than the training\ndata.\nFrom the model-building perspective, the number of\nattributes decreased by 338, from 561 down to 223. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 3 hours 53 minutes in Take2,\nwhich was a reduction of 44.9%.\nCOL_95%: In the current iteration Take2, the baseline\nperformance of the machine learning algorithms achieved an average accuracy of\n88.04%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient\nBoosting) achieved the top accuracy metrics after the first round of modeling.\nAfter a series of tuning trials, Linear Discriminant Analysis turned in the top\noverall result and achieved an accuracy metric of 92.32%. By using the\noptimized parameters, the Linear Discriminant Analysis algorithm processed the\ntesting dataset with an accuracy of 93.89%, which was even better than the\ntraining data.\nFrom the model-building perspective, the number of\nattributes decreased by 278, from 561 down to 283. The processing time went\nfrom 7 hours 3 minutes in iteration Take1 down to 4 hours 31 minutes in Take2,\nwhich was a reduction of 35.9%.\nCONCLUSION: For this iteration, eliminating collinear\nfeatures at the 95% level and using the Linear Discriminant Analysis algorithm\nachieved the best overall results. For this dataset, we should consider using\nthe Linear Discriminant Analysis algorithm for further modeling or production\nuse.\nDataset Used: Smartphone-Based Recognition of Human\nActivities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical\nattributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using Python and BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 03 Feb 2019 13:41:18 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/03\/web-scraping-of-merely-do-it-blog-entries-using-python-and-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by gathering specific pieces of information from a website. The web\nscraping code was written in Python and leveraged the BeautifulSoup module.\nINTRODUCTION: David Lowe hosts his blog at merelydoit.blog.\nThe purpose of this exercise is to practice web scraping by gathering the blog\nentries from Merely Do It\u2019s RSS feed. This iteration of the script\nautomatically traverses the RSS feed to capture all entries from the blog site.\nStarting URLs: https:\/\/merelydoit.blog\/feed or\nhttps:\/\/merelydoit.blog\/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using R Take 1","author_name":"David Lowe","blog_date":"Fri, 01 Feb 2019 13:13:05 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/02\/01\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.\nINTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.\nIn the current iteration Take1, the script will focus on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that are collinear with other attributes, we will eliminate the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 will establish the baseline performance for accuracy and processing time.\nANALYSIS: In the current iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 89.80%. Two algorithms (Linear Discriminant Analysis and eXtreme Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, eXtreme Gradient Boosting turned in the top overall result and achieved an accuracy metric of 98.59%. By using the optimized parameters, the eXtreme Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.67%, which was below the training data and possibly due to over-fitting.\nFrom the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.\nCONCLUSION: For this iteration, the eXtreme Gradient Boosting algorithm achieved the best overall results. For this dataset, we should consider using the eXtreme Gradient Boosting algorithm for further modeling or production use.\nDataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Time Series Model for Annual Water Usage in Baltimore Using Python","author_name":"David Lowe","blog_date":"Wed, 30 Jan 2019 13:22:39 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/30\/time-series-model-for-annual-water-usage-in-baltimore-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nCode Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.\nPREFACE: This is a replication of Python code from Dr.\nBrownlee\u2019s blog post on time series. I have combined all the code snippets into\none script so that I can turn the whole\nprocess into a template. The comments and analysis were also part of the blog\npost and annotated here to explain each coding block.\nSUMMARY: The purpose of this project is to construct a time\nseries prediction model and document the end-to-end steps using a template. The\nAnnual Water Usage in Baltimore dataset is a time series situation where we are\ntrying to forecast future outcomes based on the past data points.\nINTRODUCTION: The problem is to predict annual water usage.\nThe dataset provides the annual water usage in Baltimore from 1885 to 1963, or\n79 years of data. The dataset contains 79 observations in the units of liters\nper capita per day and is credited to\nHipel and McLeod, 1994.\nANALYSIS: The baseline prediction (or persistence) for the\ndataset resulted in an RMSE of 21.975. The manually configured model was\nsimplified to ARIMA(4,1,1) and produced an RMSE of 31.097, which was higher\nthan the persistent model. After applying the grid search technique to the\ndataset, the final RMSE of the ARIMA(2,1,0) model was 21.733. This is only a\nslightly smaller error than the persistent model, and it may or may not be\nstatistically different.\nCONCLUSION: The final RMSE for the validation period is predicted at 16 liters per capita per day. This is not too different from the expected\nerror of 21, but we would expect that it is also not too different from a\nsimple persistence model. The forecast does have the characteristics of a\npersistence forecast. This suggests that\nalthough this time series does have an obvious trend, it is still a reasonably\ndifficult problem.\nDataset Used: Annual Water Usage in Baltimore\nDataset ML Model: Time series forecast with numerical\nattributes\nDataset Reference:\nhttps:\/\/datamarket.com\/data\/set\/22sl\/baltmore-city-annual-water-use-liters-per-capita-per-day-1885-1968#!ds=22sl&display=line\nOne potential source of performance benchmark:\nhttps:\/\/machinelearningmastery.com\/time-series-forecast-study-python-annual-water-usage-baltimore\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activities and Postural Transitions Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 28 Jan 2019 13:11:07 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/28\/multi-class-classification-model-for-human-activities-and-postural-transitions-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities and Postural Transitions dataset is a classic multi-class classification situation where we are trying to predict one of the 12 possible outcomes.\nINTRODUCTION: The research team carried out experiments with a group of 30 volunteers who performed a protocol of activities composed of six basic activities. There are three static postures (standing, sitting, lying) and three dynamic activities (walking, walking downstairs and walking upstairs). The experiment also included postural transitions that occurred between the static postures. These are stand-to-sit, sit-to-stand, sit-to-lie, lie-to-sit, stand-to-lie, and lie-to-stand. All the participants were wearing a smartphone on the waist during the experiment execution. The research team also video-recorded the activities to label the data manually. The research team randomly partitioned the obtained data into two sets, 70% for the training data and 30% for the testing.\nIn the current iteration Take1, the script will focus on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Because the dataset has many attributes that are collinear with other attributes, we will eliminate the attributes that have a collinearity measurement of 99% or higher. Iteration Take1 will establish the baseline performance for accuracy and processing time.\nANALYSIS: In the current iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 88.52%. Two algorithms (Linear Discriminant Analysis and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top overall result and achieved an accuracy metric of 94.19%. By using the optimized parameters, the Linear Discriminant Analysis algorithm processed the testing dataset with an accuracy of 94.71%, which was even better than the training data.\nFrom the model-building perspective, the number of attributes decreased by 108, from 561 down to 453.\nCONCLUSION: For this iteration, the Linear Discriminant Analysis algorithm achieved the best overall results. For this dataset, we should consider using the Linear Discriminant Analysis algorithm for further modeling or production use.\nDataset Used: Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using Python and Scrapy","author_name":"David Lowe","blog_date":"Sun, 27 Jan 2019 13:35:32 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/27\/web-scraping-of-merely-do-it-blog-entries-using-python-and-scrapy\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web\nscraping by gathering specific pieces of information from a website. The web\nscraping code was written in Python and leveraged the Scrapy framework.\nINTRODUCTION: David Lowe hosts his blog at merelydoit.blog.\nThe purpose of this exercise is to practice web scraping by gathering the blog\nentries from Merely Do It\u2019s RSS feed. This iteration of the script\nautomatically traverses the RSS feed to capture all entries from the blog site.\nStarting URLs: https:\/\/merelydoit.blog\/feed or\nhttps:\/\/merelydoit.blog\/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 4","author_name":"David Lowe","blog_date":"Fri, 25 Jan 2019 13:26:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/25\/binary-classification-model-for-miniboone-particle-identification-using-python-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nFrom the previous iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nFrom the previous iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.\nFrom the previous iteration Take3, we examined the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.\nIn the current iteration Take4, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.\nFrom the previous iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 91.04%. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.84%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was just slightly below the training data.\nIn the current iteration Take4, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.11%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.68%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.70%, which was just slightly below the training data.\nFrom the model-building perspective, the number of attributes decreased by 10, from 50 down to 40 in iteration Take4. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 56 minutes in Take4, which was a reduction of 3.4% from Take1. It was a slight increase in comparison to Take3, which had a processing time of 6 hours 22 minutes. It was also a slight increase in comparison to Take2, which had a processing time of 6 hours 33 minutes.\nCONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results with an improved processing time after running Recursive Feature Elimination. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Time Series Model for Monthly Armed Robberies in Boston Using Python","author_name":"David Lowe","blog_date":"Wed, 23 Jan 2019 13:21:41 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/23\/time-series-model-for-monthly-armed-robberies-in-boston-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nCode Credit: Adapted from a blog post made available by Dr. Jason Brownlee of Machine Learning Mastery.\nPREFACE: This is a replication of Python code from Dr. Brownlee\u2019s blog post on time series. I have combined all the code snippets into one script so that I could turn the whole process into a template. The comments and analysis were also part of the blog post and annotated here to explain each coding block.\nSUMMARY: The purpose of this project is to construct a time series prediction model and document the end-to-end steps using a template. The Monthly Armed Robberies in Boston dataset is a time series situation where we are trying to forecast future outcomes based on the past data points.\nINTRODUCTION: The problem is to predict the number of monthly armed robberies in Boston, USA. The dataset provides the number of monthly armed robberies in Boston from January 1966 to October 1975, Monthly Boston armed robberies Jan.1966-Oct.1975 Deutsch and Alt (1977) and credited to McCleary & Hay (1980).\nANALYSIS: The baseline prediction (or persistence) for the dataset resulted in an RMSE of 51.8. The manually configured model was simplified to ARIMA(0,1,2) and produced an RMSE of 49.8, which was slightly better than the persistent model. After applying the power transformation (Box-Cox) to the dataset, the final RMSE of the model on the transformed data was 49.4. This is only a slightly smaller error than the ARIMA model on untransformed data, and it may or may not be statistically different.\nCONCLUSION: The final RMSE for the validation period was predicted at 53 robberies, and it was not too different from the expected error of 49. Although the forecast appears to have the characteristic of a persistence forecast, this does suggest that, while this time series does have an obvious trend, it is still a reasonably difficult problem.\nDataset Used: Monthly Armed Robberies in Boston\nDataset ML Model: Time series forecast with numerical attributes\nDataset Reference: https:\/\/datamarket.com\/data\/set\/22ob\/monthly-boston-armed-robberies-jan1966-oct1975-deutsch-and-alt-1977#!ds=22ob&display=line\nOne potential source of performance benchmark: https:\/\/machinelearningmastery.com\/time-series-forecast-case-study-python-monthly-armed-robberies-boston\/\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 4","author_name":"David Lowe","blog_date":"Mon, 21 Jan 2019 13:20:25 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/21\/binary-classification-model-for-miniboone-particle-identification-using-r-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nFrom the previous iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nFrom the previous iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.\nFrom the previous iteration Take3, we examined the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.\nIn the current iteration Take4, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain.\nANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.\nFrom the previous iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.49%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.52%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was even better than the training data.\nIn the current iteration Take4, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.79%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.??%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.??%, which was even better than the training data.\nFrom the model-building perspective, the number of attributes decreased by 10, from 50 down to 40 in iteration Take4. The processing time went from 17 hours 18 minutes in iteration Take1 down to 30 hours 58 minutes in Take3, which was an increase of 21.5% from Take1. It was a significant increase in comparison to Take2, which had a processing time of 12 hours 17 minutes. It was also a significant increase in comparison to Take3, which had a processing time of 6 hours 48 minutes.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results with an increased processing time after running Recursive Feature Elimination. For this dataset, the Stochastic Gradient Boosting and Random Forest algorithms should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Merely Do It Blog Entries Using R","author_name":"David Lowe","blog_date":"Sun, 20 Jan 2019 13:12:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/20\/web-scraping-of-merely-do-it-blog-entries-using-r\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: David Lowe hosts his blog at merelydoit.blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Merely Do It\u2019s RSS feed. This iteration of the script automatically traverses the RSS feed to capture all entries from the blog site.\nStarting URLs: https:\/\/merelydoit.blog\/feed or https:\/\/merelydoit.blog\/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 3","author_name":"David Lowe","blog_date":"Fri, 18 Jan 2019 13:56:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/18\/binary-classification-model-for-miniboone-particle-identification-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nIn iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.\nFor this iteration, we will examine the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.\nIn the current iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 91.04%. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.84%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was just slightly below the training data.\nFrom the model-building perspective, the number of attributes decreased by 23, from 50 down to 27 in iteration Take3. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 22 minutes in Take3, which was a reduction of 11.3% from Take1. It was also a slight decrease in comparison to Take2, which had a processing time of 6 hours 33 minutes.\nCONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results with an improved processing time after eliminating the unimportant features. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 16 Jan 2019 13:46:43 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/16\/web-scraping-of-machine-learning-mastery-blog-using-r-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: Dr. Jason Brownlee\u2019s Machine Learning Mastery hosts its tutorial lessons at https:\/\/machinelearningmastery.com\/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery\u2019s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.\nStarting URLs: https:\/\/machinelearningmastery.com\/blog\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 3","author_name":"David Lowe","blog_date":"Mon, 14 Jan 2019 13:46:03 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/14\/binary-classification-model-for-miniboone-particle-identification-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nIn iteration Take2, we examined the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iteration Take1.\nFor this iteration, we will examine the feature selection technique of attribute importance ranking. By taking only the most important attributes, we hoped to decrease the processing time and maintain a similar level of accuracy compared to iterations Take1 and Take2.\nANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.\nIn the current iteration Take3, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.49%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Random Forest turned in the top overall result and achieved an accuracy metric of 93.52%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.74%, which was even better than the training data.\nFrom the model-building perspective, the number of attributes decreased by 25, from 50 down to 25 in iteration Take3. The processing time went from 17 hours 18 minutes in iteration Take1 down to 6 hours 48 minutes in Take3, which was a reduction of 60.6% from Take1. It was also a significant decrease in comparison to Take2, which had a processing time of 12 hours 17 minutes.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results with an improved processing time after eliminating the unimportant features. For this dataset, the Stochastic Gradient Boosting and Random Forest algorithms should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Using R Take 1","author_name":"David Lowe","blog_date":"Sun, 13 Jan 2019 13:07:20 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/13\/web-scraping-of-machine-learning-mastery-blog-using-r-take-1\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: Dr. Jason Brownlee\u2019s Machine Learning Mastery hosts its tutorial lessons at https:\/\/machinelearningmastery.com\/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery\u2019s RSS feed. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.\nStarting URLs: https:\/\/machinelearningmastery.com\/feed or https:\/\/ machinelearningmastery.com \/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 2","author_name":"David Lowe","blog_date":"Fri, 11 Jan 2019 13:58:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/11\/binary-classification-model-for-miniboone-particle-identification-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nFor this iteration, we will explore the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hope to decrease the processing time while maintaining an acceptable level of accuracy loss.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.\nIn the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.16%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.92%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.82%, which was just slightly below the training data.\nFrom the model-building perspective, the number of attributes decreased by 13, from 50 down to 37. The processing time went from 7 hours 11 minutes in iteration Take1 down to 6 hours 33 minutes in Take2, which was a reduction of 8.8% from Take1.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Daines Analytics Blog Entries Using R","author_name":"David Lowe","blog_date":"Wed, 09 Jan 2019 13:01:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/09\/web-scraping-of-daines-analytics-blog-entries-using-r\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Daines Analytics\u2019 RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries.\nStarting URLs: https:\/\/dainesanalytics.blog\/feed or https:\/\/dainesanalytics.blog\/feed\/?paged=1\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 2","author_name":"David Lowe","blog_date":"Mon, 07 Jan 2019 13:44:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/07\/binary-classification-model-for-miniboone-particle-identification-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the model that produces the best overall metrics. Iteration Take1 established the baseline performance for accuracy and processing time.\nFor this iteration, we will explore the feature selection technique of eliminating collinear features. By eliminating the collinear features, we hope to decrease the processing time while maintaining an acceptable level of accuracy loss.\nANALYSIS: From the previous iteration Take1, the baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.\nIn the current iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 90.04%. Two algorithms (Stochastic Gradient Boosting and Random Forest) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.47%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.57%, which was even better than the training data.\nFrom the model-building perspective, the number of attributes decreased by 13, from 50 down to 37. The processing time went from 17 hours 18 minutes in iteration Take1 down to 12 hours 17 minutes in Take2, which was a reduction of 28% from Take1.\nCONCLUSION: For this iteration, the Stochastic Gradient Boostin algorithm achieved the best overall results with an improved processing time after eliminating the collinear features. For this dataset, the Stochastic Gradient Boostin algorithm should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python and BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 06 Jan 2019 13:47:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/06\/web-scraping-of-machine-learning-mastery-blog-entries-using-python-and-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the BeautifulSoup module.\nINTRODUCTION: Dr. Jason Brownlee\u2019s Machine Learning Mastery hosts its tutorial lessons at https:\/\/machinelearningmastery.com\/blog. The purpose of this exercise is to practice web scraping by gathering the blog entries from Machine Learning Mastery\u2019s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.\nStarting URLs: https:\/\/machinelearningmastery.com\/blog\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date":"Fri, 04 Jan 2019 13:02:41 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/04\/updated-machine-learning-templates-for-r-4\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 7 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support the following data cleaning and transformation tasks:\n\nFeature Selection via\u00a0Collinear Features Removal,\u00a0Attribute Importance Ranking, and Recursive Feature Elimination (RFE)\nImprove unbalanced datasets via Synthetic Minority Over-sampling Technique (SMOTE)\n\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date":"Wed, 02 Jan 2019 13:00:25 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2019\/01\/02\/updated-machine-learning-templates-for-python-4\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 7 of the templates contain minor adjustments and corrections to the prevision version of the templates. Also, the new templates added sample code to support the following data cleaning and transformation tasks:\n\nOne-Hot-Encoding for categorical variables\nFeature Selection via Collinear Features Removal,\u00a0Attribute Importance Ranking, and Recursive Feature Elimination (RFE)\nImprove unbalanced datasets via Synthetic Minority Over-sampling Technique (SMOTE)\n\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 31 Dec 2018 13:34:19 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/31\/binary-classification-model-for-miniboone-particle-identification-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nANALYSIS: The baseline performance of the machine learning algorithms achieved an average accuracy of 90.58%. Two algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy metrics after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top overall result and achieved an accuracy metric of 93.95%. By using the optimized parameters, the Stochastic Gradient Boosting algorithm processed the testing dataset with an accuracy of 93.85%, which was just slightly below than the training data.\nCONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the best overall results using the training and testing datasets. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of NeurIPS Proceedings Using Python and Scrapy","author_name":"David Lowe","blog_date":"Sun, 30 Dec 2018 13:41:56 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/30\/web-scraping-of-neurips-proceedings-using-python-and-scrapy\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping python code leverages the Scrapy framework.\nINTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https:\/\/papers.nips.cc\/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2017 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.\nStarting URLs: https:\/\/papers.nips.cc\/book\/advances-in-neural-information-processing-systems-30-2017\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for MiniBooNE Particle Identification Using R Take 1","author_name":"David Lowe","blog_date":"Fri, 28 Dec 2018 13:36:45 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/28\/binary-classification-model-for-miniboone-particle-identification-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The MiniBooNE Particle Identification dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset is taken from the MiniBooNE experiment and is used to distinguish electron neutrinos (signal) from muon neutrinos (background). The data file is set up as follows. In the first line is the number of signal events followed by the number of background events. The records with the signal events come first, followed by the background events. Each line, after the first line, has the 50 particle ID variables for one event.\nANALYSIS: The baseline performance of the eight algorithms achieved an average accuracy of 90.82%. Two algorithms (Bagged CART and Random Forest) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 93.74%. By optimizing the tuning parameters, the Random Forest algorithm processed the testing dataset with an accuracy of 93.91%, which was even better than the training data.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the best overall results using the training and testing datasets. For this dataset, the Random Forest algorithm should be considered for further modeling or production use.\nDataset Used: MiniBooNE particle identification Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/MiniBooNE+particle+identification\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of NeurIPS Proceedings Using R","author_name":"David Lowe","blog_date":"Wed, 26 Dec 2018 13:27:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/26\/web-scraping-of-neurips-proceedings-using-r\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping R code leverages the rvest package.\nINTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https:\/\/papers.nips.cc\/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2016 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.\nStarting URLs: https:\/\/papers.nips.cc\/book\/advances-in-neural-information-processing-systems-29-2016\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using R Take 2","author_name":"David Lowe","blog_date":"Mon, 24 Dec 2018 13:48:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/24\/binary-classification-model-for-caravan-insurance-marketing-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.\nThe data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.\nThe insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?\nIn iteration Take1, we had algorithms with high accuracy but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.\nANALYSIS: From the Take1 iteration, the baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0.7159. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5285, which was significant below the result from the training data.\nFrom the current iteration, the baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0. 9243. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5746, which was significant below the result from the training data.\nCONCLUSION: For this iteration, the SMOTE technique improved the unbalanced dataset we have but did not improve the algorithm\u2019s final performance metric. Overall, the Random Forest algorithm achieved the leading ROC scores using the training dataset, but the model failed to perform adequately using the validation dataset. For this dataset, Random Forest still should be considered for further modeling and testing before making it available for production use.\nDataset Used: Insurance Company Benchmark (COIL 2000) Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Insurance+Company+Benchmark+(COIL+2000)\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/caravan-insurance-challenge\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of NeurIPS Proceedings Using Python and BeautifulSoup","author_name":"David Lowe","blog_date":"Sun, 23 Dec 2018 13:13:57 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/23\/web-scraping-of-neurips-proceedings-using-python-and-beautifulsoup\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping python code leverages the BeautifulSoup module.\nINTRODUCTION: The Neural Information Processing Systems Conference (NeurIPS) hosts its collections of papers on the website, https:\/\/papers.nips.cc\/. This web scraping script will automatically traverse through the listing and individual paper pages of the 2015 conference and collect all links to the PDF documents. The script will also download the PDF documents as part of the scraping process.\nStarting URLs: https:\/\/papers.nips.cc\/book\/advances-in-neural-information-processing-systems-28-2015\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using Python Take 2","author_name":"David Lowe","blog_date":"Fri, 21 Dec 2018 13:18:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/21\/binary-classification-model-for-caravan-insurance-marketing-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.\nThe data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.\nThe insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?\nIn iteration Take1, we had algorithms with high accuracy but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.\nANALYSIS: From the previous Take1 iteration, the baseline performance of the ten algorithms achieved an average F1_Micro score of 0.9260. Two algorithms, Logistic Regression and Support Vector Machine, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an F1_Micro score of 0.9402. After using the optimized tuning parameters, the Support Vector Machine algorithm processed the validation dataset with an F1_Micro score of 0.9405, which was slightly better than using the training data.\nFrom the current iteration, the baseline performance of the eight algorithms achieved an average F1_Micro score of 0.9326. Two algorithms, Random Forest and Extra Trees, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an F1_Micro score of 0.9595. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with an F1_Micro score of 0.9165, which was noticeably worse than using the training data and perhaps due to overfitting.\nCONCLUSION: For this iteration, the SMOTE technique improved the unbalanced dataset we have but did not improve the algorithm\u2019s final performance metric. Overall, the Random Forest algorithm achieved the leading F1_Micro scores using the training dataset, but the model failed to perform adequately using the validation dataset. For this dataset, Random Forest still should be considered for further modeling and testing before making it available for production use.\nDataset Used: Insurance Company Benchmark (COIL 2000) Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Insurance+Company+Benchmark+(COIL+2000)\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/caravan-insurance-challenge\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python Take 2","author_name":"David Lowe","blog_date":"Wed, 19 Dec 2018 13:23:11 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/19\/web-scraping-of-machine-learning-mastery-blog-entries-using-python-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: Dr. Jason Brownlee\u2019s Machine Learning Mastery hosts its tutorial lessons at https:\/\/machinelearningmastery.com\/blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Machine Learning Mastery\u2019s RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries and store all captured information in a JSON output file.\nStarting URLs: https:\/\/machinelearningmastery.com\/blog\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 17 Dec 2018 13:59:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/17\/binary-classification-model-for-caravan-insurance-marketing-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.\nThe data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.\nThe insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?\nANALYSIS: The baseline performance of the seven algorithms achieved an average ROC score of 0.6965. Two algorithms, Decision Tree and Random Forest, achieved the top two ROC scores after the first round of modeling. After a series of tuning trials, Random Forest yielded the top result using the training data. It achieved a ROC score of 0.7159. After using the optimized tuning parameters, the Random Forest algorithm processed the validation dataset with a ROC score of 0.5285, which was significant below the result from the training data.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the leading ROC scores using the training and validation datasets. For this dataset, the Random Forest algorithm does not appear to be adequate for production use. Further modeling and testing are recommended for the next step.\nDataset Used: Insurance Company Benchmark (COIL 2000) Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Insurance+Company+Benchmark+(COIL+2000)\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/caravan-insurance-challenge\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Machine Learning Mastery Blog Entries Using Python Take 1","author_name":"David Lowe","blog_date":"Sun, 16 Dec 2018 13:13:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/16\/web-scraping-of-machine-learning-mastery-blog-entries-using-python-take-1\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leverages the Scrapy framework\u00a0maintained by Scrapinghub.\nINTRODUCTION: Dr. Jason Brownlee\u2019s Machine Learning Mastery hosts its tutorial lessons at https:\/\/machinelearningmastery.com\/blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Machine Learning Mastery\u2019s web pages. This iteration of the script automatically traverses the web pages to capture all blog entries and store all captured information in a JSON output file.\nStarting URLs: https:\/\/machinelearningmastery.com\/blog\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Caravan Insurance Marketing Using Python Take 1","author_name":"David Lowe","blog_date":"Fri, 14 Dec 2018 13:07:49 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/14\/binary-classification-model-for-caravan-insurance-marketing-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Insurance Company Benchmark dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data set was used in the CoIL 2000 Challenge that contains information on customers of an insurance company. The data consist of 86 variables and include product usage data and socio-demographic data derived from zip codes.\nThe data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information of whether they have a caravan insurance policy. A test dataset contains another 4000 customers whose information will be used to test the effectiveness of the machine learning models.\nThe insurance organization collected the data to answer the following question: Can we predict who would be interested in buying a caravan insurance policy and give an explanation why?\nANALYSIS: The baseline performance of the ten algorithms achieved an average F1_Micro score of 0.9260. Two algorithms, Logistic Regression and Support Vector Machine, achieved the top F1_Micro scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an F1_Micro score of 0.9402. After using the optimized tuning parameters, the Support Vector Machine algorithm processed the validation dataset with an F1_Micro score of 0.9405, which was slightly better than using the training data.\nCONCLUSION: For this iteration, the Support Vector Machine algorithm achieved the leading F1_Micro scores using the training and validation datasets. For this dataset, Support Vector Machine should be considered for further modeling or production use.\nDataset Used: Insurance Company Benchmark (COIL 2000) Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Insurance+Company+Benchmark+(COIL+2000)\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/caravan-insurance-challenge\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date":"Wed, 12 Dec 2018 13:37:02 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/12\/updated-machine-learning-templates-for-r-3\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 6 of the templates contain minor adjustments and corrections to the prevision version of the templates.\nThe new templates also added an email function that sends out email messages. The email message can aid the monitoring of the script, especially for those scripts with large datasets and long processing time.\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date":"Mon, 10 Dec 2018 13:35:27 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/10\/updated-machine-learning-templates-for-python-3\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 6 of the templates contain minor adjustments and corrections to the prevision version of the templates.\nThe new templates also added an email function that sends out email messages. The email message can aid the monitoring of the script, especially for those scripts with large datasets and long processing time.\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Daines Analytics Blog Entries using Python Take 2","author_name":"David Lowe","blog_date":"Sun, 09 Dec 2018 13:52:59 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/09\/web-scraping-of-daines-analytics-blog-entries-using-python-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Daines Analytics\u2019 RSS feed. This iteration of the script automatically traverses the RSS feed to capture all blog entries, not just the first ten as in Take1.\nStarting URLs: https:\/\/dainesanalytics.blog\/feed\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 07 Dec 2018 13:06:26 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/07\/regression-model-for-online-news-popularity-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average RMSE of 10409. Two algorithms (ElasticNet and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved the best RMSE of 10312. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 13007, which was worse than the accuracy of the training data and possibly due to over-fitting.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 10503. Two algorithms (Ridge, LASSO, and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 10320. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 13049, which was worse than the accuracy of the training data and possibly due to over-fitting.\nFrom the model-building activities, the number of attributes went from 58 down to 48 after eliminating ten attributes. The processing time went from 21 hours 7 minutes in iteration Take1 down to 14 hours 49 minutes in iteration Take3, which was a reduction of 29% from Take1. The processing time, however, was a noticeable increase from Take2, which processed the dataset in 11 hours 41 minutes.\nCONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 05 Dec 2018 13:57:20 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/05\/regression-model-for-online-news-popularity-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average RMSE of 13128. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11358. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12146, which was slightly worse than the accuracy of the training data.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 14468. Two algorithms (ElasticNet and Support Vector Machine) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11294. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12094, which was slightly worse than the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 58 down to 30 after eliminating 28 attributes. The processing time went from 15 minutes 1 second in iteration Take1 up to 58 minutes 16 seconds in iteration Take2, which was due to the additional time required for tuning the Support Vector Machine algorithm. It also was a significant increase in comparison to Take2, which had a processing time of 17 minutes 37 seconds.\nCONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the ElasticNet algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using R Take 2","author_name":"David Lowe","blog_date":"Mon, 03 Dec 2018 13:41:25 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/03\/regression-model-for-online-news-popularity-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 10409. Two algorithms (ElasticNet and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved the best RMSE of 10312. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 13007, which was worse than the accuracy of the training data and possibly due to over-fitting.\nFrom the model-building activities, the number of attributes went from 58 down to 35 after eliminating 23 attributes. The processing time went from 21 hours 7 minutes in iteration Take1 down to 11 hours 41 minutes in iteration Take2, which was a reduction of 44% from Take1.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this iteration, the Stochastic Gradient Boosting algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, Stochastic Gradient Boosting should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 4","author_name":"David Lowe","blog_date":"Sun, 02 Dec 2018 13:04:59 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/12\/02\/web-scraping-of-quotes-from-famous-people-using-r-take-4\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take4 iteration, the R script attempts to execute the login form and scrape the Goodreads links off each quote. The Goodreads links appear only after a successful authentication.\nStarting URLs: http:\/\/quotes.toscrape.com\/login\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using Python Take 2","author_name":"David Lowe","blog_date":"Fri, 30 Nov 2018 13:39:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/30\/regression-model-for-online-news-popularity-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with the Lasso algorithm. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average RMSE of 13128. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11358. Using the optimized tuning parameter available, the ElasticNet algorithm processed the validation dataset with an RMSE of 12146, which was slightly worse than the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 58 down to 30 after eliminating 28 attributes. The processing time went from 15 minutes 1 second in iteration Take1 up to 17 minutes 37 seconds in iteration Take2, which was due to the additional time required for the feature selection processing.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and yet still retained a comparable level of accuracy. For this dataset, ElasticNet should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using R Take 1","author_name":"David Lowe","blog_date":"Wed, 28 Nov 2018 13:10:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/28\/regression-model-for-online-news-popularity-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 10446. Two algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved the best RMSE of 10299. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 12978, which was slightly worse than the accuracy of the training data and possibly due to over-fitting.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, Random Forest should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Online News Popularity Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 26 Nov 2018 13:15:11 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/26\/regression-model-for-online-news-popularity-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a regression situation where we are trying to predict the value of a continuous variable.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nANALYSIS: The baseline performance of the machine learning algorithms achieved an average RMSE of 13020. Two algorithms (Linear Regression and ElasticNet) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, ElasticNet turned in the top result using the training data. It achieved the best RMSE of 11273. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an RMSE of 12089, which was slightly worse than the accuracy of the training data.\nCONCLUSION: For this iteration, the ElasticNet algorithm achieved the top training and validation results comparing to other machine learning algorithms. For this dataset, ElasticNet should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 3","author_name":"David Lowe","blog_date":"Sat, 24 Nov 2018 21:31:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/24\/web-scraping-of-quotes-from-famous-people-using-r-take-3\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take3 iteration, the R script attempts to scrape the quote information that is displayed via an infinite scrolling page.\nStarting URLs: http:\/\/quotes.toscrape.com\/scroll\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 3","author_name":"David Lowe","blog_date":"Fri, 23 Nov 2018 13:13:45 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/23\/binary-classification-model-for-online-news-popularity-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.\nANALYSIS: From the previous iteration Take1, the baseline performance of the algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.\nFrom the previous iteration Take2, the baseline performance of the algorithms achieved an average accuracy of 60.60%. Two ensemble algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.34%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.70%, which was just slightly below the accuracy of the training data.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 61.08%. Two algorithms (Support Vector Machine and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 65.25%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 64.17%, which was just slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 58 down to 44 after eliminating 14 attributes. The processing time went from 5 hours 56 minutes in iteration Take1 down to 1 hour 34 minutes in iteration Take3, which was a reduction of 73% from Take1. It also was a slight decrease in comparison to Take2, which reduced the processing time down to 1 hour 59 minutes.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and either the dimensionality reduction techniques should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 3","author_name":"David Lowe","blog_date":"Wed, 21 Nov 2018 13:15:09 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/21\/binary-classification-model-for-online-news-popularity-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 40.\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.\nFrom the previous iteration Take2, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.29%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.51%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.53%, which was just slightly below the accuracy of the training data.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.22%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.41%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.69%, which was just slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes was kept at 58 after the RFE processing. The processing time went from 6 hours 31 minutes in iteration Take1 up to 11 hours 17 minutes in iteration Take3, which was an increase of 87% from Take1.\nCONCLUSION: The two feature selection techniques yielded different attribute selection sets and outcomes. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique from iteration Take2 should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 19 Nov 2018 13:33:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/19\/binary-classification-model-for-online-news-popularity-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nANALYSIS: From the previous iteration Take1, the baseline performance of the algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.\nIn the current iteration, the baseline performance of the algorithms achieved an average accuracy of 60.60%. Two ensemble algorithms (Bagged CART and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.34%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.70%, which was just slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 58 down to 44 after eliminating 14 attributes. The processing time went from 5 hours 56 minutes in iteration Take1 down to 1 hour 59 minutes in iteration Take2, which was a reduction of 66% from Take1.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 2","author_name":"David Lowe","blog_date":"Sun, 18 Nov 2018 13:24:16 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/18\/web-scraping-of-quotes-from-famous-people-using-r-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take2 iteration, the R script attempts to follow the links to the author page and scrape the author information.\nStarting URLs: http:\/\/quotes.toscrape.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 2","author_name":"David Lowe","blog_date":"Fri, 16 Nov 2018 13:13:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/16\/binary-classification-model-for-online-news-popularity-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content but some statistics associated with it. The original content can be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor this iteration, we will examine the feasibility of using a dimensionality reduction technique of ranking the attribute importance with a gradient boosting tree method. Afterward, we will eliminate the features that do not contribute to the cumulative importance of 0.99 (or 99%).\nANALYSIS: From the previous iteration Take1, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.\nIn the current iteration, the baseline performance of the machine learning algorithms achieved an average accuracy of 64.29%. Two ensemble algorithms (Random Forest and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.51%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.53%, which was just slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 58 down to 42 after eliminating 16 attributes. The processing time went from 6 hours 31 minutes in iteration Take1 down to 3 hours 18 minutes in iteration Take2, which was a reduction of 49% from Take1.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained a comparable level of accuracy. For this dataset, the Stochastic Gradient Boosting algorithm and the attribute importance ranking technique should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using Python Take 1","author_name":"David Lowe","blog_date":"Wed, 14 Nov 2018 13:38:41 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/14\/binary-classification-model-for-online-news-popularity-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nANALYSIS: The baseline performance of the ten algorithms achieved an average accuracy of 59.95%. Three algorithms (Bagged CART, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.89%, which was just slightly worse than the training data.\nCONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the top-tier training and validation results. For this dataset, Stochastic Gradient Boosting should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Online News Popularity Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 12 Nov 2018 13:37:10 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/12\/binary-classification-model-for-online-news-popularity-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Online News Popularity dataset is a binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article\u2019s popularity level in social networks. The dataset does not contain the original content, but some statistics associated with it. The original content be publicly accessed and retrieved using the provided URLs.\nMany thanks to K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal, for making the dataset and benchmarking information available.\nANALYSIS: The baseline performance of the eight algorithms achieved an average accuracy of 64.53%. Three algorithms (Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 67.48%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 66.71%, which was just slightly below the training data.\nCONCLUSION: For this iteration, the Stochastic Gradient Boosting algorithm achieved the top-tier training and validation results. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: Online News Popularity Dataset\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using R Take 1","author_name":"David Lowe","blog_date":"Sun, 11 Nov 2018 14:18:05 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/11\/web-scraping-of-quotes-from-famous-people-using-r-take-1\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the R script attempts to follow the page links and scrape the quote information off each page.\nStarting URLs: http:\/\/quotes.toscrape.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 09 Nov 2018 14:21:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/09\/binary-classification-model-for-truck-aps-failure-detection-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset\u2019s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will impute the blank cells with the value zero.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using a dimensionality reduction technique to reduce the processing time while still maintaining an adequate level of prediction accuracy. The technique was to eliminate collinear attributes based on a threshold of 75%.\nFor this iteration, we will explore the Recursive Feature Elimination (RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 50.\nANALYSIS: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.\nFrom the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 98.99%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.40%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.19%, which was slightly below the accuracy of the training data.\nIn the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.94%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.31%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.09%, which was slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 170 down to 25 after eliminating 145 attributes. The processing time went from 63 hours 22 minutes in iteration Take 1 down to 9 hours 33 minutes in iteration Take3, which was a reduction of 84% from Take1. That was also a noticeable reduction in comparison to Take2, which reduced the processing time down to 39 hours 52 minutes.\nCONCLUSION: The feature selection techniques helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained an acceptable level of accuracy. For this dataset, the Random Forest algorithm and the Recursive Feature Elimination (RFE) technique should be considered for further modeling or production use.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 07 Nov 2018 14:16:10 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/07\/binary-classification-model-for-truck-aps-failure-detection-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset\u2019s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will impute the blank cells with the value zero.\nIn iteration Take1, the script focused on evaluating various machine learning algorithms and identifying the algorithm that produces the best accuracy result. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor this iteration, we will examine the feasibility of using a dimensionality reduction technique to reduce the processing time while still maintaining an adequate level of prediction accuracy. The technique is to eliminate collinear attributes based on a threshold of 75%.\nANALYSIS: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.\nIn the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.99%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.40%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.19%, which was slightly below the accuracy of the training data.\nFrom the model-building activities, the number of attributes went from 170 down to 100 after eliminating 70 attributes. The processing time went from 63 hours 22 minutes in iteration Take1 down to 39 hours 52 minutes in iteration Take2, which was a reduction of 37% from Take1.\nCONCLUSION: The feature selection technique helped by cutting down the attributes and reduced the training time. Furthermore, the modeling took a much shorter time to process yet still retained an almost-identical level of accuracy. For this dataset, the Random Forest algorithm and the technique of eliminating collinear attributes should be considered for further modeling or production use.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 05 Nov 2018 14:40:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/05\/binary-classification-model-for-truck-aps-failure-detection-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset\u2019s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of baseline results by imputing the blank cells with the value zero.\nANALYSIS: The baseline performance of the ten algorithms achieved an average accuracy of 99.02%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.39%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.22%, which was slightly below the accuracy of the training data.\nCONCLUSION: For this iteration, the Random Forest algorithm achieved the top-tier training and validation results. For this dataset, the Random Forest algorithm should be considered for further modeling or production use.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of ScrapingHub Blog Entries using R","author_name":"David Lowe","blog_date":"Sun, 04 Nov 2018 14:38:16 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/04\/web-scraping-of-scrapinghub-blog-entries-using-r\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: ScrapingHub, the maker of the Scrapy framework, hosts its blog at blog.scrapinghub.com. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Scrapinghub. The script also would automatically traverse from one page of the blog entries to the next page.\nStarting URLs: https:\/\/blog.scrapinghub.com\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date":"Fri, 02 Nov 2018 12:07:41 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/11\/02\/updated-machine-learning-templates-for-python-2\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 5 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.\nThe new templates also standardized the dataframes used in the script as follow:\noriginalDataset: This dataframe contains the original data imported from the data source.\nxy_train: Training dataframe that has the attributes and the target\/class variable.\nx_train: Training dataframe that has the attributes only.\ny_train: Training dataframe that has the target\/class variable only.\nxy_test: Test dataframe that has the attributes and the target\/class variable.\nx_test: Test dataframe that has the attributes only.\ny_test: Test dataframe that has the target\/class variable only.\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date":"Wed, 31 Oct 2018 12:06:32 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/31\/updated-machine-learning-templates-for-r-2\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 5 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.\nThe new templates also standardized the dataframes used in the script as follow:\noriginalDataset: This dataframe contains the original data imported from the data source.\nxy_train: Training dataframe that has the attributes and the target\/class variable.\nx_train: Training dataframe that has the attributes only.\ny_train: Training dataframe that has the target\/class variable only.\nxy_test: Test dataframe that has the attributes and the target\/class variable.\nx_test: Test dataframe that has the attributes only.\ny_test: Test dataframe that has the target\/class variable only.\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 4","author_name":"David Lowe","blog_date":"Mon, 29 Oct 2018 12:14:50 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/29\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we explored was to eliminate collinear attributes based on a threshold of 85%.\nIn iteration Take3, we explored the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to cumulative importance of 0.99.\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time manageable, we will limit the number of attributes to 50.\nCONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.\nFrom the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%, which was slightly worse than the accuracy from the training data and possibly due to over-fitting.\nFrom the previous iteration Take3, the baseline performance of the ten algorithms achieved an average accuracy of 91.59%. the Random Forest and Stochastic Gradient Boosting algorithms achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.74%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.42%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 90.62%. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 97.75%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 87.21%. The accuracy on the validation dataset was noticeably worse than the training data and possibly due to over-fitting.\nFrom the model-building activities, the number of attributes went from 561 down to 41 after eliminating 520 variables. The processing time went from 8 hours 16 minutes in iteration Take1 down to 2 hours and 25 minutes in iteration Take4. That was a noticeable reduction in comparison to Take2, which reduced the processing time down to 7 hours 15 minutes. It also was a noticeable reduction in comparison to Take3, which reduced the processing time down to 5 hours 22 minutes.\nIn conclusion, the attribute importance ranking technique helped by cutting down the attributes and reduce the training time. Furthermore, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm with attribute importance ranking should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 4","author_name":"David Lowe","blog_date":"Sun, 28 Oct 2018 12:12:18 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/28\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance in terms of accuracy and processing time.\nIn iteration Take2, we examined the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.\nIn iteration Take3, we explored the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Afterward, we eliminated the features that do not contribute to cumulative importance of 0.99.\nFor this iteration, we will explore the Recursive Feature Elimination (or RFE) technique by recursively removing attributes and building a model on those attributes that remain. To keep the training time managable, we will limit the number of attributes to 50.\nCONCLUSION: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.\nFrom the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.\nFrom the previous iteration Take3, the baseline performance of the ten algorithms achieved an average accuracy of 85.49%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.52%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.06%, which was slightly better than the accuracy from the training data.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 86.76%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 95.83%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 94.19%, which was slightly below the accuracy from the training data.\nFrom the model-building activities, the number of attributes went from 561 down to 50 after eliminating 511 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 1 hours and 16 minutes in iteration Take4. That was a minor reduction in comparison to Take2, which reduced the processing time down to 2 hours 7 minutes. It also was a noticeable reduction in comparison to Take3, which reduced the processing time down to 8 hours and 9 minutes.\nIn conclusion, the importance ranking technique should have benefited the tree methods the most, but the Linear Discriminant Analysis algorithm held its own for this modeling iteration. Furthermore, by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Linear Discriminant Analysis and Support Vector Machine algorithms should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 26 Oct 2018 12:07:50 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/26\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time.\nFor iteration Take2, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we explored was to eliminate collinear attributes based on a threshold of 85%.\nFor this iteration, we will explore the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Next, we eliminate the features which do not contribute to the cumulative importance of 0.99.\nCONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.\nFrom the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 91.59%. The Random Forest and Stochastic Gradient Boosting algorithms achieved the top two accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.74%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.42%. The accuracy on the validation dataset was slightly worse than the training data and possibly due to over-fitting.\nFrom the model-building activities, the number of attributes went from 561 down to 79 after eliminating 482 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 5 hours and 22 minutes in iteration Take3. That was also a noticeable reduction in comparison to Take2, which reduced the processing time down to 7 hours 15 minutes.\nIn conclusion, the importance ranking technique should have benefited the tree methods the most, and it did. Furthermore, by reducing the number of attributes, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 24 Oct 2018 12:38:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/24\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time. For this iteration, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.\nCONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 90.83%. Three algorithms (Random Forest, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.07%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 93.96%, which was slightly worse than the accuracy from the training data and possibly due to over-fitting.\nFrom the model-building activities, the number of attributes went from 561 down to 192 after eliminating 369 variables that are at least 85% collinear. The processing time went from 21 hours 43 minutes in iteration Take1 down to 7 hours and 15 minutes in iteration Take2. That was a reduction in model training and processing time of 66%.\nIn conclusion, the reduction in the number of attributes used still achieved an acceptable level of accuracy. by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Stochastic Gradient Boosting algorithm should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 22 Oct 2018 12:00:52 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/22\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nFor this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 91.67%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 98.84%. Stochastic Gradient Boosting also processed the validation dataset with an accuracy of 95.49%, which was slightly below the accuracy from the training data.\nFrom the model-building activities, the Stochastic Gradient Boosting ensemble algorithm achieved the top-notch training and validation results. For the project, Stochastic Gradient Boosting should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 3","author_name":"David Lowe","blog_date":"Fri, 19 Oct 2018 12:11:23 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/19\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-3\/","blog_text":"\n\n\u00a0\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance in terms of accuracy and processing time.\nIn iteration Take2, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.\nFor this iteration, we will explore the dimensionality reduction technique of ranking the importance of the attributes with a gradient boosting tree method. Next, we eliminate the features which do not contribute to cumulative importance of 0.99.\nCONCLUSION: From the previous iteration Take1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.\nFrom the previous iteration Take2, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 85.49%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.52%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.06%, which was slightly better than the accuracy from the training data.\nFrom the model-building activities, the number of attributes went from 561 down to 486 after eliminating 75 variables that fell below the required importance. The processing time went from 8 hours 16 minutes in iteration Take1 down to 8 hours and 9 minutes in iteration Take3. That was a minor reduction in comparison to Take2, which reduced the processing time down to 2 hours 7 minutes.\nIn conclusion, the importance ranking technique should have benefited the tree methods the most, but the Linear Discriminant Analysis algorithm held its own for this modeling iteration. Furthermore, by reducing the collinearity, the modeling took a much shorter time to process yet still retained decent accuracy. For this dataset, the Linear Discriminant Analysis and Support Vector Machine algorithms should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 2","author_name":"David Lowe","blog_date":"Wed, 17 Oct 2018 12:36:11 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/17\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% of the test data.\nIn iteration Take1, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric. Iteration Take1 established a baseline performance regarding accuracy and processing time. For this iteration, we will examine the feasibility of using dimensionality reduction techniques to reduce the processing time while still maintaining an adequate level of prediction accuracy. The first technique we will explore is to eliminate collinear attributes based on a threshold of 85%.\nCONCLUSION: From the previous iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the Linear Discriminant Analysis algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy of 83.54%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 93.34%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 93.82%, which was slightly better than the accuracy from the training data.\nFrom the model-building activities, the number of attributes went from 561 down to 172 after eliminating 389 variables that are at least 85% collinear. The processing time went from 8 hours 16 minutes in iteration Take1 down to 2 hours and 7 minutes in iteration Take2. That was a reduction in model training and processing time of 74%.\nIn conclusion, the reduction in the number of attributes used still achieved an acceptable level of accuracy. Furthermore, the Support Vector Machine algorithm achieved the top-notch training and validation results. For the project, Support Vector Machine should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Human Activity Recognition with Smartphone Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 15 Oct 2018 12:12:22 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/15\/multi-class-classification-model-for-human-activity-recognition-with-smartphone-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Human Activities with Smartphone Dataset is a multi-class classification situation where we are trying to predict one of the six possible outcomes.\nINTRODUCTION: Researchers collected the datasets from experiments that consist of a group of 30 volunteers with each person performed six activities wearing a smartphone on the waist. With its embedded accelerometer and gyroscope, the research captured measurement for the activities of WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING. The dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data.\nFor this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 84.68%. Three algorithms (Linear Discriminant Analysis, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Linear Discriminant Analysis turned in the top result using the training data. It achieved an average accuracy of 95.43%. Using the optimized tuning parameter available, the algorithm processed the validation dataset with an accuracy of 96.23%, which was even better than the accuracy from the training data.\nFrom the model-building activities, the Linear Discriminant Analysis algorithm achieved the top-notch training and validation results. For the project, Linear Discriminant Analysis should be considered for further modeling or production use.\nDataset Used: Human Activity Recognition Using Smartphone Data Set\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Registry of Open Data on AWS Using R","author_name":"David Lowe","blog_date":"Fri, 12 Oct 2018 12:29:20 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/12\/web-scraping-of-registry-of-open-data-on-aws-using-r\/","blog_text":"\n\nSUMMARY: The purpose of this project is to gather data about the open datasets on AWS. The web scraping code was written in R and leveraged the rvest package.\nINTRODUCTION: The Open Data registry exists to help people discover and share datasets that are available via AWS resources. This page lists all usage examples for datasets listed in the registry.\nStarting URLs: https:\/\/registry.opendata.aws\/\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Daines Analytics Blog Entries using Python Take 1","author_name":"David Lowe","blog_date":"Wed, 10 Oct 2018 12:23:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/10\/4487\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: Daines Analytics hosts its blog at dainesanalytics.blog. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Daines Analytics\u2019 RSS feed. This iteration of the script can capture only the most recent ten blog entries. A future iteration of the script would automatically traverse the RSS feed to capture all blog entries, not just the first ten.\nStarting URLs: https:\/\/dainesanalytics.blog\/feed\/\n\nimport scrapy\n\nclass DainesBlogRSSSpider(scrapy.Spider):\n    name = 'dainesblogrss'\n    allowed_domains = ['dainesanalytics.blog\/feed\/']\n    start_urls = ['https:\/\/dainesanalytics.blog\/feed\/']\n\n    # Setting up for the JSON output file\n    custom_settings = {\n        'FEED_URI' : 'dainesblogrss.json'\n    }\n\n    def parse(self, response):\n        self.log('I just visited: ' + response.url)\n\n        # Remove the XML namespaces\n        response.selector.remove_namespaces()\n\n        # Extract article information\n        titles = response.xpath('\/\/item\/title\/text()').extract()\n        authors = response.xpath('\/\/item\/creator\/text()').extract()\n        dates = response.xpath('\/\/item\/pubDate\/text()').extract()\n        links = response.xpath('\/\/item\/link\/text()').extract()\n        description = response.xpath('\/\/item\/description\/text()').extract()\n\n        for item in zip(titles, authors, dates, links, description):\n            scraped_info = {\n                'Title' : item[0],\n                'Author' : item[1],\n                'Publish_Date' : item[2],\n                'Link' : item[3],\n                'Description' : item[4]\n            }\n            yield scraped_info\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of ScrapingHub Blog Entries using Python","author_name":"David Lowe","blog_date":"Mon, 08 Oct 2018 12:54:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/08\/web-scraping-of-scrapinghub-blog-entries-using-python\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: ScrapingHub, the maker of the Scrapy framework, hosts its blog at blog.scrapinghub.com. The purpose of this exercise is to practice web scraping using Scrapy by gathering the blog entries from Scrapinghub. The script also would automatically traverse from one page of the blog entries to the next page.\nStarting URLs: https:\/\/blog.scrapinghub.com\/\n\nimport scrapy\n\nclass ScrapinghubSpider(scrapy.Spider):\n    name = 'scrapinghub'\n    allowed_domains = ['scrapinghub.com']\n    start_urls = ['https:\/\/blog.scrapinghub.com\/']\n\n    def parse(self, response):\n        self.log('I just visited: ' + response.url)\n        for blog in response.css('div.post-item'):\n            item = {\n                'blog_title': blog.css('div.post-header > h2 > a::text').extract_first(),\n                'blog_url': blog.css('div.post-header > h2 > a::attr(href)').extract_first(),\n                'date': blog.css('div.post-header > div.byline > span.date > a::text').extract_first(),\n                'author': blog.css('div.post-header > div.byline > span.author > a::text').extract_first(),\n                'summary': blog.css('div.post-content > p::text').extract_first(),\n            }\n            yield item\n\n        # follow pagination link\n        next_page_url = response.css('div.blog-pagination > a.next-posts-link').xpath('@href').extract_first()\n        if next_page_url:\n            self.log('Moving on to next page: ' + next_page_url)\n            yield scrapy.Request(url=next_page_url, callback=self.parse)\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 4","author_name":"David Lowe","blog_date":"Fri, 05 Oct 2018 12:27:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/05\/web-scraping-of-quotes-from-famous-people-using-python-take-4\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take4 iteration, the Python script attempts to execute the login form and scrape the Goodreads links off each quote. The Goodreads links appear only after a successful authentication.\nStarting URLs: http:\/\/quotes.toscrape.com\/login\n\nimport scrapy\n\nclass LoginSpider(scrapy.Spider):\n    name = \"login\"\n    login_url = 'http:\/\/quotes.toscrape.com\/login'\n    start_urls = [login_url]\n\n    def parse(self, response):\n        # Extract the CSRF token value\n        token = response.css('input[name=\"csrf_token\"]::attr(value)').extract_first()\n        # Create a Python dictionary with the form values\n        data = {\n            'csrf_token' : token,\n            'username' : 'abc',\n            'password' : 'abc',\n        }\n        # Submit a Post request to login\n        yield scrapy.FormRequest(url=self.login_url, formdata=data, callback=self.parse_quotes)\n\n    def parse_quotes(self, response):\n        # Parse the items on page after login\n        self.log('I just visited: ' + response.url)\n        for quote in response.css('div.quote'):\n            item = {\n                'author_name': quote.css('small.author::text').extract_first(),\n                'goodreads_url': quote.css('small.author ~ a[href*=\"goodreads.com\"]::attr(href)').extract_first(),\n            }\n            yield item\n\n        # follow pagination link\n        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n        if next_page_url:\n            next_page_url = response.urljoin(next_page_url)\n            self.log('Moving on to: ' + next_page_url)\n            yield scrapy.Request(url=next_page_url, callback=self.parse_quotes)\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 03 Oct 2018 12:29:07 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/03\/web-scraping-of-quotes-from-famous-people-using-python-take-3\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by extracting specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework [https:\/\/scrapy.org\/] maintained by Scrapinghub [https:\/\/scrapinghub.com\/].\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take3 iteration, the Python script attempts to scrape the quote information that is displayed via an infinite scrolling page.\nStarting URLs: http:\/\/quotes.toscrape.com\/scroll\n\nimport json\nimport scrapy\n\nclass ScrollSpider(scrapy.Spider):\n    name = \"scroll\"\n    api_url = 'http:\/\/quotes.toscrape.com\/api\/quotes?page={}'\n    start_urls = [api_url.format(1)]\n\n    def parse(self, response):\n        data = json.loads(response.text)\n        for quote in data['quotes']:\n            yield {\n                'author_name': quote['author']['name'],\n                'text': quote['text'],\n                'tags': quote['tags'],\n                'author_url': quote['author']['goodreads_link'],\n            }\n\n        # follow pagination link\n        if data['has_next']:\n            next_page = data['page'] + 1\n            yield scrapy.Request(url=self.api_url.format(next_page), callback=self.parse)\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 01 Oct 2018 12:46:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/10\/01\/web-scraping-of-quotes-from-famous-people-using-python-take-2\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take2 iteration, the Python script attempts to follow the links to the author page and scrape the author information.\nStarting URLs: http:\/\/quotes.toscrape.com\/\n\nimport scrapy\n\nclass AuthorSpider(scrapy.Spider):\n    name = 'authors'\n    start_urls = ['http:\/\/quotes.toscrape.com\/']\n\n    def parse(self, response):\n        # follow links to author pages\n        for href in response.css('.author + a::attr(href)'):\n            yield response.follow(href, self.parse_author)\n\n        # follow pagination links\n        for href in response.css('li.next a::attr(href)'):\n            yield response.follow(href, self.parse)\n\n    def parse_author(self, response):\n        def extract_with_css(query):\n            return response.css(query).extract_first().strip()\n\n        yield {\n            'name': extract_with_css('h3.author-title::text'),\n            'birthdate': extract_with_css('.author-born-date::text'),\n            'bio': extract_with_css('.author-description::text'),\n        }\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Quotes from Famous People using Python Take 1","author_name":"David Lowe","blog_date":"Fri, 28 Sep 2018 12:20:58 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/28\/web-scraping-of-quotes-from-famous-people-using-python-take-1\/","blog_text":"\n\nSUMMARY: The purpose of this project is to practice web scraping by gathering specific pieces of information from a website. The web scraping code was written in Python 3 and leveraged the Scrapy framework maintained by Scrapinghub.\nINTRODUCTION: A demo website, created by Scrapinghub, lists quotes from famous people. It has many endpoints showing the quotes in different ways, and each endpoint presents a different scraping challenge for practicing web scraping. For this Take1 iteration, the Python script attempts to follow the page links and scrape the quote information off each page.\nStarting URLs: http:\/\/quotes.toscrape.com\/\n\nimport scrapy\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = ['http:\/\/quotes.toscrape.com\/']\n    def parse(self, response):\n        self.log('I just visited: ' + response.url)\n        for quote in response.css('div.quote'):\n            item = {\n                'author_name': quote.css('small.author::text').extract_first(),\n                'text': quote.css('span.text::text').extract_first(),\n                'tags': quote.css('div.tags a.tag::text').extract(),\n                'author_url': quote.css('div.quote > span > a::attr(href)').extract_first(),\n            }\n            yield item\n        # follow pagination link\n        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n        if next_page_url:\n            next_page_url = response.urljoin(next_page_url)\n            yield scrapy.Request(url=next_page_url, callback=self.parse)\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using Python Take 2","author_name":"David Lowe","blog_date":"Wed, 26 Sep 2018 12:45:18 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/26\/binary-class-classification-model-for-german-credit-risks-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset contains 1,000 entries with 20 categorial\/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.\nBecause the case study also stipulated that it is worse to classify a customer as good when they are bad (weight of 5), than it is to classify a customer as bad when they are good (weight of 1). For this iteration, the script focuses on tuning various machine learning algorithms and identify the algorithm that can produce the best cost-and-accuracy tradeoffs.\nCONCLUSION: From the previous iteration Take 1, the baseline performance of the eight algorithms achieved an average accuracy of 71.80%. Three algorithms (Logistic Regression, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 76.14%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.66%, which was slightly better than the accuracy from the training data.\nFrom the cost vs accuracy comparison, both the Logistic Regression and Stochastic Gradient Boosting achieved high accuracy while keeping the costs of incorrect predictions low. Either algorithm should be considered for further modeling or production use.\nDataset Used: German Credit Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28German+Credit+Data%29\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/german-credit\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 24 Sep 2018 12:02:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/24\/binary-class-classification-model-for-german-credit-risks-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset contains 1,000 entries with 20 categorial\/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.\nFor this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 71.80%. Three algorithms (Logistic Regression, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 76.14%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.66%, which was slightly better than the accuracy from the training data.\nFrom the model-building activities, the Stochastic Gradient Boosting ensemble algorithm yielded the top-notch training and validation results. It is the recommended algorithm to use from the accuracy perspective.\nDataset Used: German Credit Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28German+Credit+Data%29\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/german-credit\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Web Scraping of Registry of Open Data on AWS Using Python","author_name":"David Lowe","blog_date":"Fri, 21 Sep 2018 12:58:09 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/21\/web-scraping-of-registry-of-open-data-on-aws\/","blog_text":"\n\nSUMMARY: The purpose of this project is to gather data about the open datasets on AWS. The web scraping code was written in Python 3 and leveraged the Scrapy framework\u00a0maintained by Scrapinghub.\nINTRODUCTION: The Open Data registry exists to help people discover and share datasets that are available via AWS resources. This page lists all usage examples for datasets listed in the registry.\nStarting URLs: https:\/\/registry.opendata.aws\/\n\nimport scrapy\nclass ListdatasetsSpider(scrapy.Spider):\n    name = 'listdatasets'\n    start_urls = ['https:\/\/registry.opendata.aws\/']\n\n    def parse(self, response):\n        for dataset in response.css('div.dataset'):\n            item = {\n                'dataset_name': dataset.css('h3 > a::text').extract_first(),\n                'detail_url': response.urljoin(dataset.css('h3 > a::attr(href)').extract_first()),\n                'tags': dataset.css('p > span::text').extract(),\n                'description': dataset.css('p')[1].extract(),\n            }\n            yield item\n\nThe source code and JSON output can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 19 Sep 2018 12:49:49 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/19\/binary-class-classification-model-for-german-credit-risks-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset contains 1,000 entries with 20 categorial\/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.\nBecause the case study also stipulated that it is worse to classify a customer as good when they are bad (weight of 5), than it is to classify a customer as bad when they are good (weight of 1). For this iteration, the script focuses on tuning various machine learning algorithms and identify the algorithm that can produce the best cost-and-accuracy tradeoffs.\nCONCLUSION: From the previous iteration Take 1, The baseline performance of the eight algorithms achieved an average accuracy of 72.69%. Three algorithms (Logistic Regression, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 75.00%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 75.67%, which was slightly better than the accuracy from the training data.\nFrom the cost vs accuracy comparison, both the Logistic Regression and AdaBoost achieved identical accuracy while keeping the costs of incorrect predictions low. Either algorithm should be considered for further modeling or production use.\nDataset Used: German Credit Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28German+Credit+Data%29\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/german-credit\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for German Credit Risks Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 17 Sep 2018 12:34:19 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/17\/binary-class-classification-model-for-german-credit-risks-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The German Credit Risks Dataset is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This dataset contains 1,000 entries with 20 categorial\/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes on credit risk by a German bank. Each person is classified as good or bad credit risks according to the set of attributes.\nFor this iteration, the script focuses on evaluating various machine learning algorithms and identify the algorithm that produces the best accuracy metric.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 72.69%. Three algorithms (Logistic Regression, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 75.00%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 75.67%, which was slightly better than the accuracy from the training data.\nFrom the model-building activities, the Stochastic Gradient Boosting ensemble algorithm yielded the top-notch training and validation results. It is the recommended algorithm to use from the accuracy perspective.\nDataset Used: German Credit Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28German+Credit+Data%29\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/german-credit\/home\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for Python","author_name":"David Lowe","blog_date":"Fri, 14 Sep 2018 12:03:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/14\/updated-machine-learning-templates-for-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.\nVersion 4 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.\nThe new templates also consolidated and moved the parameters used script-wide to a new section of its own (1.c), rather having them spread out all over the script.\nYou will find the Python templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for R","author_name":"David Lowe","blog_date":"Wed, 12 Sep 2018 12:00:35 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/12\/updated-machine-learning-templates-for-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, [https:\/\/machinelearningmastery.com\/machine-learning-project-template-in-r\/] I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.\nVersion 4 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.\nThe new templates also consolidated and moved the parameters used script-wide to a new section of its own (1.c), rather having them spread out all over the script.\nYou will find the R templates from the Machine Learning Project Templates page.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 3 Using Python","author_name":"David Lowe","blog_date":"Mon, 10 Sep 2018 12:33:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/10\/binary-class-classification-model-for-seismic-bumps-take-3-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nIn iterations Take1 and Take2, we had three algorithms with high accuracy and ROC results but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.\nCONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset. With an imbalanced dataset we have on-hand, we needed to look for another metric or another approach to evaluate the models.\nFrom the previous Take2 iteration, the baseline performance of the eight algorithms achieved an average ROC score of 66.16%. Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 77.98%, but with a dismal precision and recall scores.\nFrom the current iteration, the baseline performance of the ten algorithms achieved an average accuracy score of 75.83%. Three algorithms (Random Forest, Extra Trees, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the best accuracy result of 96.38%, but with still-very-low precision and recall scores.\nThe ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. Also, the SMOTE technique helped to make the model evaluation more realistic with the imbalanced dataset we have. For this project however, the modeling choice was inconclusive due to the data imbalance issue that still requires a resolution.\nDataset Used: Seismic Bumps Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/seismic-bumps\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 2 Using Python","author_name":"David Lowe","blog_date":"Fri, 07 Sep 2018 12:13:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/07\/binary-class-classification-model-for-seismic-bumps-take-2-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nIn iteration Take1, we had three algorithms with high accuracy results but with dismal precision and recall scores. For this iteration, we will examine the viability of using the ROC scores to rank and choose the models.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset. With an imbalanced dataset we have on-hand, we needed to look for another metric or another approach to evaluate the models.\nFrom the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 66.16%. Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 77.98%, but with a dismal precision and recall scores.\nThe ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. However, with an imbalanced dataset we have on-hand, we still need to look for another approach to further validate our modeling effort.\nDataset Used: Seismic Bumps Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/seismic-bumps\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 1 Using Python","author_name":"David Lowe","blog_date":"Wed, 05 Sep 2018 12:33:38 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/05\/binary-class-classification-model-for-seismic-bumps-take-1-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Seismic Bumps Data Set is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 91.94%. Three algorithms (Logistic Regression, Support Vector Machine, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the support vector machine algorithm turned in the best accuracy result of 93.36%, but with very low precision and recall scores for the positive cases when processing the validation dataset.\nWith an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.\nDataset Used: Seismic Bumps Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/seismic-bumps\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 3 Using R","author_name":"David Lowe","blog_date":"Mon, 03 Sep 2018 12:13:43 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/09\/03\/binary-class-classification-model-for-seismic-bumps-take-3-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nIn iterations Take1 and Take2, we had three algorithms with high accuracy and ROC results but with strong biases due to the imbalance of our dataset. For this iteration, we will examine the feasibility of using the SMOTE technique to balance the dataset.\nCONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0. With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.\nFrom the previous Take2 iteration, the baseline performance of the eight algorithms achieved an average ROC score of 71.99%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 78.59%, but with a dismal sensitivity score of 0.88%.\nFrom the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 87.33%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the best ROC result of 92.68%, but with a much-better sensitivity score of 70.87%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with a ROC of 84.57%, which was slightly below the ROC score of the training data.\nThe ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. Also, the SMOTE technique helped to make the model evaluation more realistic with the imbalanced dataset we have. For this project, the Random Forest appeared to be the most suitable algorithm for the dataset.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 2 Using R","author_name":"David Lowe","blog_date":"Fri, 31 Aug 2018 12:36:27 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/31\/binary-class-classification-model-for-seismic-bumps-take-2-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nIn iteration Take1, we had three algorithms with high accuracy results but with dismal Kappa scores. For this iteration, we will examine the viability of using the ROC scores to rank and choose the models.\nCONCLUSION: From the previous Take1 iteration, the baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0. With an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.\nFrom the current iteration, the baseline performance of the eight algorithms achieved an average ROC score of 71.99%. Three algorithms (Random Forest, Adaboost, and Stochastic Gradient Boosting) achieved the top three ROC scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the best ROC result of 78.59, but with a dismal sensitivity score of 0.88%.\nThe ROC metric has given us a more viable way to evaluate the models, other than using the accuracy scores. However, with an imbalanced dataset that we have on-hand, we still need to look for another approach to further validate our modeling effort.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary-Class Classification Model for Seismic Bumps Take 1 Using R","author_name":"David Lowe","blog_date":"Wed, 29 Aug 2018 12:16:23 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/29\/binary-class-classification-model-for-seismic-bumps-take-1-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Seismic Bumps Data Set is a binary-class classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: Mining activity has always been connected with the occurrence of dangers which are commonly called mining hazards. A special case of such a threat is a seismic hazard which frequently occurs in many underground mines. Seismic hazard is the hardest detectable and predictable of natural hazards, and it is comparable to an earthquake. The complexity of seismic processes and big disproportion between the number of low-energy seismic events and the number of high-energy phenomena causes the statistical techniques to be insufficient to predict seismic hazard. Therefore, it is essential to search for new opportunities for better hazard prediction, also using machine learning methods.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 93.11%. Three algorithms (Random Forest, Support Vector Machine, and Adaboost) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, all three algorithms turned in the identical accuracy result of 93.42%, with an identical Kappa score of 0.0.\nWith an imbalanced dataset we have on-hand, we will need to look for another metric or another approach to evaluate the models.\nDataset Used: Seismic Bumps Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/seismic-bumps\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Letter Recognition Using Python","author_name":"David Lowe","blog_date":"Mon, 27 Aug 2018 12:39:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/27\/multi-class-classification-model-for-letter-recognition-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Letter Recognition Data Set is a multi-class classification situation where we are trying to predict one of the several possible outcomes.\nINTRODUCTION: The objective is to identify each of many black-and-white rectangular-pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 80.98%. Three algorithms (k-Nearest Neighbors, Support Vector Machine, and Extra Trees) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Support Vector Machine turned in the top result using the training data. It achieved an average accuracy of 97.37%. Using the optimized tuning parameter available, the Support Vector Machine algorithm processed the validation dataset with an accuracy of 97.46%, which was even slightly better the accuracy of the training data.\nFor this project, the Support Vector Machine algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Letter Recognition\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Letter+Recognition\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/c\/ci-letter-recognition\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Letter Recognition Using R","author_name":"David Lowe","blog_date":"Fri, 24 Aug 2018 12:44:26 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/24\/multi-class-classification-model-for-letter-recognition-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Letter Recognition DataSet is a multi-class classification situation where we are trying to predict one of the several possible outcomes.\nINTRODUCTION: The objective is to identify each of many black-and-white rectangular-pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\nCONCLUSION: The baseline performance of the eight algorithms achieved an average accuracy of 79.30%. Three algorithms (Bagged CART, Random Forest, and k-Nearest Neighbors) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 96.32%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 96.45%, which was even slightly better the accuracy of the training data.\nFor this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Letter Recognition\nDataset ML Model: Multi-class classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Letter+Recognition\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/c\/ci-letter-recognition\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 22 Aug 2018 12:45:20 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/22\/binary-classification-model-for-truck-aps-failure-detection-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset\u2019s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of results by imputing the blank cells with the value of -1. We will compare the results from Take 1 and Take 2, where we imputed the blank cells with the zero and the mean value.\nCONCLUSION: From the Take1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.\nFrom the Take2 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8348%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.\nFrom the current iteration (Take3), the baseline performance of the ten algorithms achieved an average accuracy of 98.8003%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2312%, which was slightly below the accuracy of the training data.\nFor this iteration, imputing the missing cells with the -1 value improved the average performance of all models only slightly. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 20 Aug 2018 12:51:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/20\/binary-classification-model-for-truck-aps-failure-detection-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The datasets\u2019 positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of results by imputing the blank cells with the mean value. We will compare the results from Take 1, where we imputed the blank cells with the value zero.\nCONCLUSION: From the Take1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.\nFrom the current iteration (Take2), the baseline performance of the ten algorithms achieved an average accuracy of 98.8348%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3967%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.\nFor this iteration, imputing the missing cells with the mean value improved the average performance of all models slightly, but not so much for the Random Forest algorithm. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Truck APS Failure Detection Using Python Take 1","author_name":"David Lowe","blog_date":"Fri, 17 Aug 2018 12:37:11 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/17\/binary-classification-model-for-truck-aps-failure-detection-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The APS Failure at Scania Trucks dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The dataset\u2019s positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\nThis dataset has many cells with missing values, so it is not practical to simply delete the rows with missing cells. This iteration of the project will produce a set of baseline results by imputing the blank cells with the value zero.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 98.8001%. The ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) all achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 99.3983%. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 99.2187%, which was slightly below the accuracy of the training data.\nFor this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: APS Failure at Scania Trucks Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/APS+Failure+at+Scania+Trucks\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Provisioning AWS Linux AMI-based EC2 for R Server","author_name":"David Lowe","blog_date":"Wed, 15 Aug 2018 12:22:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/15\/provisioning-aws-linux-ami-based-ec2-for-r-server\/","blog_text":"\n\nBelow are my notes for provisioning an R server using AWS\u2019 Linux 2 AMI. These notes captured my installation process during the week of 23 July 2018.\nMy goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do to get past those obstacles.\nYou can find the installation and configuration notes here on the website.\nAbbreviations\n\nAWS: Amazon Web Services\nVPC: Virtual Private Cloud\nEC2: Elastic Compute Cloud\nIAM: Identity and Access Management\nAMI: Amazon Machine Image\nDLAMI-DG: Deep Learning AMI Developer Guide\n\nRequirements\nNeeded to find workable configurations for modeling machine learning problems by exploring the use of AWS Ec2 instances. The source code language was in R and contained in an RMD script format.\nThe baseline performance was defined by running the Python script on a Dell Latitude E7450 with Intel i7-5600U CPU at 2.60GHz, 16GB RAM, and Windows 10. While this can be a decent configuration for ML modeling, occasionally we may need a system with larger memory capacity or more CPUs for the tasks at hand.\nBackground and Prerequisite Information\nThe following tools and assumptions were present prior to the provisioning of the cloud instance.\n\nAWS Console with the necessary rights and configuration elements to launch an instance. I had configured a VPC subnet, an IAM role, a security group, and a key pair for setting up the instance.\nAWS Deep Learning AMI Developer Guide, released June 6, 2018\nWeb browsers\nPuTTY\n\nAWS Configuration Notes\nAMI: I performed the following steps using Amazon Linux 2 AMI with an m5.large general-purpose instance.\nVPC: This exercise requires only a subnet that is accessible via the Internet.\nSecurity Group: I configured the security group to allow only TCP ports 22 from any IP address because I had planned to use an SSH tunnel to access the R server.\nIAM Role: I assign all my AWS instances to an IAM role by default. For this exercise, an IAM role is not critical.\nKey Pair: I attached the instance to an existing key pair. The key pair is necessary to access the instance via the SSH protocol.\n\u00a0\nProvision an instance with the Amazon Deep Learning AMI\nStep 1) Create and launch the instance. I used an m5.large instance as the starting point.\nStep 2) Install R base package (R3.4 as of this writing).\n$ sudo amazon-linux-extra install R3.4\nStep 3) Install R Server with the following commands. Check www.rstudio.org for the latest release of the server.\n$ wget https:\/\/download2.rstudio.org\/rstudio-server-rhel-1.1.456-x86_64.rpm\n$ sudo yum install rstudio-server-rhel-1.1.456-x86_64.rpm\nStep 4) Configure the client workstation to connect to the R server. I configured my Windows workstation to connect to the R server using an SSH tunnel. The DLAMI-DG document has a write-up on how to do this for Windows, Linux, and MacOS clients (pages 15-20).\nSee the PuTTY screenshot below for configuring an SSH tunnel.\n\nStep 5) Install Git.\n$ sudo yum install -y git\n$ git clone https:\/\/github.com\/daines-analytics\/tabular-data-projects.git\nStep 6) Add an user to access the R server.\n$ sudo useradd rstudio\n$ echo rstudio:rstudio | sudo chpasswd\nStep 6) Start a browser on the workstation running the SSH tunnel and point to the URL http:\/\/localhost:8787. A login screen should appear.\n\nStep 7) Go to the Terminal tab and run the \u201cgit clone\u201d commands to copy my R scripts from GitHub to the cloud server. Locate the R script and run it.\n\nThere you have it! A working R server on an AWS cloud instance that you can access via a secured protocol. Now install your favorite packages and let the scripts run.\nWhen compared to a client workstation, the right types of cloud instance can help our modeling effort. For anyone who will be attempting a similar installation, I hope these instructions can help in some way. My next step is to automate the instance creation with a CloudFormation script further. I will write down what I run into and share my findings later.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Provisioning AWS Deep Learning AMI-based EC2 for Jupyter","author_name":"David Lowe","blog_date":"Mon, 13 Aug 2018 12:13:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/13\/provisioning-aws-deep-learning-ami-based-ec2-for-jupyter\/","blog_text":"\n\nBelow are my notes for provisioning a Jupyter server using AWS\u2019 Deep Learning AMI. These notes captured my installation process during the week of 23 July 2018.\nMy goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do to get past those obstacles.\nYou can find the installation and configuration notes here on the website.\nAbbreviations\n\nAWS: Amazon Web Services\nVPC: Virtual Private Cloud\nEC2: Elastic Compute Cloud\nIAM: Identity and Access Management\nAMI: Amazon Machine Image\nDLAMI-DG: Deep Learning AMI Developer Guide\n\nRequirements:\nNeeded to find workable configurations for modeling machine learning problems by exploring the use of AWS Ec2 instances. The source code language was in Python and contained in a Jupyter notebook.\nThe baseline performance was defined by running the Python script on a Dell Latitude E7450 with Intel i7-5600U CPU at 2.60GHz, 16GB RAM, and Windows 10. While this can be a decent configuration for ML modeling, occasionally we may need a system with larger memory capacity or more CPUs for the tasks at hand.\nThe performance of the end-to-end script processing time on cloud instance should be comparable or even better than the baseline workstation.\nBackground and Prerequisite Information\nThe following tools and assumptions were present prior to the provisioning of the cloud instance.\n\nAWS Console with the necessary rights and configuration elements to launch an instance. I had configured a VPC subnet, an IAM role, a security group, and a key pair for setting up the instance.\nAWS Deep Learning AMI Developer Guide, released June 6, 2018\nWeb browsers\nPuTTY\n\nAWS Configuration Notes\nAMI: I performed the following steps using both the Ubuntu-based and the Amazon Linux-based AMIs with an m5.large general-purpose instance. The AMIs were designed to take advantage of instances with GPUs. I found no issue running either AMI without the GPU; however, some pre-supplied tutorial examples probably need to be tweaked before they would work on a general-purpose instance.\nVPC: This exercise requires only a subnet that is accessible via the Internet.\nSecurity Group: I configured the security group to allow only TCP ports 22 from any IP address because I had planned to use an SSH tunnel to access the Jupyter server.\nIAM Role: I assign all my AWS instances to an IAM role by default. For this exercise, an IAM role is not critical.\nKey Pair: I attached the instance to an existing key pair. The key pair is necessary to access the instance via the SSH protocol.\nProvision an instance with the Amazon Deep Learning AMI\nStep 1) Create and launch the instance. I used an m5.large instance as the starting point.\nStep 2) Configure the client workstation to connect to the Jupyter server. I configured my Windows workstation to connect to the Jupyter server using an SSH tunnel. The DLAMI-DG document has a write-up on how to do this for Windows, Linux, and MacOS clients (pages 15-20).\nSee the PuTTY screenshot below for configuring an SSH tunnel.\n\nStep 3) Run the \u201cgit\u201d commands to copy my Python scripts from GitHub to the cloud server.\n$ git clone https:\/\/github.com\/daines-analytics\/sandbox-projects.git\nStep 4) Activate the Python 3 environment by running the command:\n$ source activate python3\nStep 5) Because my Python script required numpy, pandas, scipy, scikit-learn, and matplotlib packages, I needed to install some additional packages.\nOn the Ubuntu AMI, I ran the command \u201cconda install <package>\u201d to check or to install them.\nOn the Amazon Linux AMI, I ran the command \u201cpip install <package>\u201d to check or to install them.\nStep 6) Start the Jupyter server by running the command:\n$ jupyter notebook\n\nStep 7) Make a note of the Jupyter server URL and use that on the workstation browser running the SSH tunnel.\n\nStep 8) Locate the Python script and run it (my own Git folder circled below).\n\nStep 9) Compare the run-time script lengths. Not rigidly scientific but probably good enough.\nWindows Workstation: 1 hour 50 minutes\nUbuntu\/Deep Learning AMI: 1 hour 22 minutes\nAmazon Linux\/Deep Learning AMI: 1 hour 24 minutes\nThere you have it! A working Jupyter server on an AWS cloud instance that you can access via a secured protocol.\nWhen compared to a client workstation, the right types of cloud instance can help our modeling effort. For anyone who will be attempting a similar installation, I hope these instructions can help in some way. My next step is to automate the instance creation with a CloudFormation script further. I will write down what I run into and share my findings later.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using R Take 3","author_name":"David Lowe","blog_date":"Fri, 10 Aug 2018 12:51:06 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/10\/binary-classification-model-for-census-income-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has a categorical attribute, native-country, that contains over 40 different values. We will examine the models by removing the native-country attribute and see how the removed attribute might have an impact on the modeling. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1 and Take 2.\nCONCLUSION: From iteration Take 1, the baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on-par with the accuracy of the training data.\nFrom the previous iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.93%, which was slightly better than the accuracy of the training data.\nFrom this iteration (Take 3), the baseline performance of the ten algorithms achieved an average accuracy of 84.37%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.00%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2\u2019s 24 hours and 33 minutes down to Take 3\u2019s 14 hours and 2 minutes. That was a time improvement of 42%.\nFor this project, dropping the native-country attribute had no impact to the overall accuracy of the training model but contributed to a noticeable improvement of the model training time. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 08 Aug 2018 12:28:20 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/08\/binary-classification-model-for-census-income-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has many cells with missing values, so we will examine the models by imputing the missing cells with a default value. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1.\nCONCLUSION: From the previous iteration (Take 1), the baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on par with the accuracy of the training data.\nFrom this iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.60%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.93%, which was slightly better than the accuracy of the training data.\nFor this project, imputing the missing values appeared to have contributed to a slight improvement of the overall accuracy of the training model. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 06 Aug 2018 12:23:54 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/06\/binary-classification-model-for-census-income-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has many cells with missing values, so we will examine the models by deleting the rows with missing cells. This iteration of the project will produce a set of baseline results that we can use to compare with other data cleaning methods.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 83.79%. Four ensemble algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.27%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 86.29%, which was on par with the accuracy of the training data.\nFor this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using Python Take 4","author_name":"David Lowe","blog_date":"Fri, 03 Aug 2018 12:34:18 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/03\/binary-classification-model-for-census-income-using-python-take-4\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has a continuous numeric attribute, fnlwgt. The term estimate refers to population totals derived from CPS by creating \u201cweighted tallies\u201d of any specified socio-economic characteristics of the population. For this iteration, we will examine the models by removing the fnlwgt attribute and see how much of impact will the removed attribute have on the modeling. This iteration of the project will produce a set of results with which we will use to compare with the results from the first three iterations of the project.\nCONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.\nFrom iteration Take 2, the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.\nFrom iteration Take 3, the baseline performance of the ten algorithms achieved an average accuracy of 81.95%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.29%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.50%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2\u2019s 5 hours and 12 minutes down to Take 3\u2019s 3 hours and 34 minutes. That was a time improvement of 34%.\nFor this iteration (Take 4), the baseline performance of the ten algorithms achieved an average accuracy of 84.19%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.38%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.47%, which was slightly better than the accuracy of the training data. Moreover, the length of the script run-time decreased from Take 3\u2019s 3 hours and 34 minutes down to Take 4\u2019s 3 hours and 25 minutes. The time improvement was very small.\nFor this project, dropping the native-country and fnlwgt attributes improved both the training time and accuracy of the models. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 01 Aug 2018 12:44:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/08\/01\/binary-classification-model-for-census-income-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has a categorical attribute, native-country, that contains over 40 different values. We will examine the models by removing the native-country attribute and see how the removed attribute might have an impact on the modeling. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1 and Take 2.\nCONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.\nFrom iteration Take 2, the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.\nFrom this iteration (Take 3), the baseline performance of the ten algorithms achieved an average accuracy of 81.95%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.29%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.50%, which was slightly better than the accuracy of the training data. More importantly, the length of the script run-time decreased from Take 2\u2019s 5 hours and 12 minutes down to Take 3\u2019s 3 hours and 34 minutes. That was a time improvement of 34%.\nFor this project, dropping the native-country attribute had no impact to the overall accuracy of the training model but contributed to a noticeable improvement of the model training time. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 30 Jul 2018 12:49:23 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/30\/binary-classification-model-for-census-income-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has many cells with missing values, so we will examine the models by imputing the missing cells with a default value. This iteration of the project will produce a set of results that we will use to compare with the baseline models from Take 1.\nCONCLUSION: From the previous iteration (Take 1), The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.\nFrom this iteration (Take 2), the baseline performance of the ten algorithms achieved an average accuracy of 81.93%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 87.31%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.57%, which was slightly better than the accuracy of the training data.\nFor this project, imputing the missing values appeared to have contributed to a slight improvement of the overall accuracy of the training model. The Stochastic Gradient Boosting ensemble algorithm continued to yield consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Census Income Using Python Take 1","author_name":"David Lowe","blog_date":"Fri, 27 Jul 2018 12:26:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/27\/binary-classification-model-for-census-income-using-python-take-1\/","blog_text":"\n\n\u00a0\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Census Income dataset is a classic binary classification situation where we are trying to predict one of the two possible outcomes.\nINTRODUCTION: This data was extracted from the 1994 Census Bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K a year.\nThis dataset has many cells with missing values, so we will examine the models by deleting the rows with missing cells. This iteration of the project will produce a set of baseline results that we can use to compare with other data cleaning methods.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.37%. Four ensemble algorithms (Bagged CART, Random Forest, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 86.99%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm further processed the validation dataset with an accuracy of 87.23%, which was slightly better than the accuracy of the training data.\nFor this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Census Income Data Set\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/adult-census-income\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Wine Quality Using R","author_name":"David Lowe","blog_date":"Wed, 25 Jul 2018 12:46:05 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/25\/multi-class-classification-model-for-wine-quality-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset is a multi-class classification situation where we are trying to predict one of the three possible outcomes (cheap, average, and good).\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nFrom the previous iteration, we approached the dataset as a regression problem and tried to predict the wine quality (a continuous numeric variable) with the least amount of mean squared error. While regression is one approach for assessing the wine quality, expressing quality in pure numbers and fractions are difficult for people to grasp fully.\nFor this iteration of the project, we will approach this dataset as a multi-class problem and attempt to classify the wine quality into one of the three rating categories: 1-Good (quality 7 or above), 2-Average (quality of 5-6), and 3-Cheap (quality 4 or below).\nCONCLUSION: The baseline performance of the seven algorithms achieved an average accuracy of 80.08%. Three ensemble algorithms (Bagged Decision Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average accuracy of 85.03%. With the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an accuracy of 83.98%, which was slightly worse than the accuracy of the training data.\nFor this project, predicting whether a bottle of wine would be good, average, or cheap appears to be more intuitive than to predict simply a numerical quality score. The Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Wine Quality Using R","author_name":"David Lowe","blog_date":"Mon, 23 Jul 2018 12:39:26 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/23\/regression-model-for-wine-quality-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset can be approached as a regression situation where we are trying to predict the rating of the wine.\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nCONCLUSION: The baseline performance of the seven algorithms achieved an average RMSE of 0.7119. Three algorithms (Support Vector Machine, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Random Forest turned in the top result using the training data. It achieved an average RMSE of 0.6088. Using the optimized tuning parameter available, the Random Forest algorithm processed the validation dataset with an RMSE of 0.6416, which was slightly worse than the RMSE of the training data. For this project, the Random Forest ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Wine Quality Using Python","author_name":"David Lowe","blog_date":"Fri, 20 Jul 2018 12:08:59 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/20\/multi-class-classification-model-for-wine-quality-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nSUMMARY: The purpose of this project is to construct a prediction model using various machine learning algorithms and to document the end-to-end steps using a template. The Wine Quality dataset is a multi-class classification situation where we are trying to predict one of the three possible outcomes (cheap, average, and good).\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nFrom the previous iteration, we approached the dataset as a regression problem and tried to predict the wine quality (a continuous numeric variable) with the least amount of mean squared error. While regression is one approach for assessing the wine quality, expressing quality in pure numbers and fractions are difficult for people to grasp fully.\nFor this iteration of the project, we will approach this dataset as a multi-class problem and attempt to classify the wine quality into one of the three rating categories: 1-Good (quality 7 or above), 2-Average (quality of 5-6), and 3-Cheap (quality 4 or below).\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 78.10%. Three ensemble algorithms (Bagged Decision Trees, Random Forest, and Extra Trees) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average accuracy of 84.78%. With the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an accuracy of 86.00%, which was even better than the accuracy of the training data.\nFor this project, predicting whether a bottle of wine would be good, average, or cheap appears to be more intuitive than to predict simply a numerical quality score. The Extra Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Wine Quality Using Python Take 3","author_name":"David Lowe","blog_date":"Wed, 18 Jul 2018 12:35:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/18\/regression-model-for-wine-quality-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nFrom the first iteration (Take 1) with the red wine dataset, the baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data.\nFrom the second iteration (Take 2) with the white wine dataset, the baseline performance of the 11 algorithms achieved an average RMSE of 0.6111. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3869. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3574, which was even better than the RMSE of the training data.\nFor this iteration of the project, we will perform the modeling using the datasets from both wine types. We will observe the results and learn whether the combined dataset would improve the overall prediction.\nCONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.5882. Three ensemble algorithms (Random Forest, Extra Trees, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3663. Using the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3490, which was even better than the RMSE of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nIn summary, combining the red and white wine data does not appear to have a noticeable difference on the prediction effectiveness of the wine quality.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Wine Quality Using Python Take 2","author_name":"David Lowe","blog_date":"Mon, 16 Jul 2018 12:56:54 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/16\/regression-model-for-wine-quality-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nFrom the previous iteration, the baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data.\nFor this iteration of the project, we will perform the modeling using only the data for the white wine. For the subsequent iterations, we will analyze the combined data from both types of wine.\nCONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.6111. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3869. After optimizing the tuning parameters, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3574, which was even better than the RMSE of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nIn summary, modeling red wine prediction appears to be slightly more accurate than modeling the prediction for white wines.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Wine Quality Using Python Take 1","author_name":"David Lowe","blog_date":"Fri, 13 Jul 2018 09:45:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/13\/regression-model-for-wine-quality-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Wine Quality Data Set\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\nINTRODUCTION: The two datasets are related to red and white variants of the Portuguese \u201cVinho Verde\u201d wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). The goal is to model wine quality based on physicochemical tests.\nFor this iteration of the project, we will perform the modeling using only the data from the red wine. For the subsequent iterations, we will analyze the white wine data and the combined data from both types of wine.\nCONCLUSION: The baseline performance of the 11 algorithms achieved an average RMSE of 0.5094. The four ensemble algorithms (AdaBoost, Extra Trees, Random Forest, and Stochastic Gradient Boosting) achieved the top RMSE scores after the first round of modeling. After a series of tuning trials, Extra Trees turned in the top result using the training data. It achieved an average RMSE of 0.3453. Using the optimized tuning parameter available, the Extra-Trees algorithm processed the validation dataset with an RMSE of 0.3089, which was even better than the accuracy of the training data. For this project, the Extra-Trees ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Credit Card Default Using R Take 2","author_name":"David Lowe","blog_date":"Wed, 11 Jul 2018 09:14:35 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/11\/binary-classification-model-for-credit-card-default-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Default of Credit Card Clients Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\nINTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nPreviously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 81.05%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 82.18%. Using the optimized tuning parameter available, Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 81.94%, which was just slightly lower than the accuracy of the training data.\nFor the Take No.2 iteration, we will perform the binning operation for the credit limit and age attributes and observe the effects on the models.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.07%. Three algorithms (Decision Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, AdaBoost turned in the top result using the training data. It achieved an average accuracy of 82.22%. Using the optimized tuning parameter available, the AdaBoost algorithm processed the validation dataset with an accuracy of 82.06%, which was just slightly lower than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nFor this round of modeling, converting the credit limit and age attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.\nThe HTML formatted report can be found here on Github.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Credit Card Default Using R Take 1","author_name":"David Lowe","blog_date":"Mon, 09 Jul 2018 09:04:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/09\/binary-classification-model-for-credit-card-default-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Default of Credit Card Clients Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\nINTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 81.05%. Three algorithms (Support Vector Machine, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 82.18%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 81.94%, which was just slightly lower than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on Github.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 3","author_name":"David Lowe","blog_date":"Fri, 06 Jul 2018 09:40:02 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/06\/binary-classification-model-for-credit-card-default-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Default of Credit Card Clients Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\nINTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nPreviously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data.\nFor the Take No.2 iteration, we converted the Sex\/Gender, Education, and Marital Status attributes into categorical variables and observed the effects on the models. After the conversion, the baseline performance of the ten algorithms achieved an average accuracy of 74.39%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.83%, which was slightly better than the accuracy of the training data.\nFor the Take No.3 iteration, we will perform the binning operation for the credit limit and age attributes and observe the effects on the models.\nCONCLUSION: After the conversion, the baseline performance of the ten algorithms achieved an average accuracy of 74.34%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.75%, which was slightly better than the accuracy of the training data.\nFor this round of modeling, converting the credit limit and age attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 2","author_name":"David Lowe","blog_date":"Wed, 04 Jul 2018 09:57:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/04\/binary-classification-model-for-credit-card-default-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Default of Credit Card Clients Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\nINTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nPreviously on the Take No.1 iteration, the baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data.\nFor the Take No.2 iteration, we will convert the Age, Education, and Marital Status attributes into categorical variables and observe the change\u2019s effects on the modeling.\nCONCLUSION: After converting the Age, Education, and Marital Status attributes into categorical variables, the baseline performance of the ten algorithms achieved an average accuracy of 74.39%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.96%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.83%, which was slightly better than the accuracy of the training data.\nFor this round of modeling, converting the Age, Education, and Marital Status attributes from ordinal to categorical did not have a noticeable effect on the accuracy of the models.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Credit Card Default Using Python Take 1","author_name":"David Lowe","blog_date":"Mon, 02 Jul 2018 09:52:51 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/07\/02\/binary-classification-model-for-credit-card-default-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Default of Credit Card Clients Data Set\nDataset ML Model: Binary classification with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\nOne potential source of performance benchmark: https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\nINTRODUCTION: This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\nCONCLUSION: The baseline performance of the ten algorithms achieved an average accuracy of 74.38%. The group of ensemble algorithms (Bagged CART, Random Forest, Extra Trees, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 81.97%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 82.91%, which was slightly better than the accuracy of the training data. For this round of modeling, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Templates for Python and R","author_name":"David Lowe","blog_date":"Fri, 29 Jun 2018 09:40:53 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/29\/updated-machine-learning-templates-for-python-and-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python and R.\nVersion 3 of the templates contain several minor adjustments and corrections to address discrepancies in the prevision versions of the template.\nYou will find the templates on the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on the website.\nRegression:\u00a0Python template\/report\u00a0or R template\/report\nBinary Classification:\u00a0Python template\/report or\u00a0R template\/report\nMulti-Class Classification:\u00a0Python template\/report or\u00a0R template\/report\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Classification Model for Faulty Steel Plates Using R","author_name":"David Lowe","blog_date":"Tue, 26 Jun 2018 09:30:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/26\/multi-class-classification-model-for-faulty-steel-plates-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Faulty Steel Plates\nDataset ML Model: Multi-Class classification with numerical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/steel+plates+faults\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/faulty-steel-plates\nINTRODUCTION: This dataset comes from research by Semeion, Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus \u201cother\u201d). The Input vector was made up of 27 indicators that approximately the geometric shape of the defect and its outline. According to the research paper, Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.\nCONCLUSION: The baseline performance of the seven algorithms achieved an average accuracy of 69.69%. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, Stochastic Gradient Boosting turned in the top result using the training data. It achieved an average accuracy of 77.78%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting algorithm processed the validation dataset with an accuracy of 77.20%, which was slightly below the accuracy of the training data. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Classification Model for Faulty Steel Plates Using Python","author_name":"David Lowe","blog_date":"Fri, 22 Jun 2018 09:45:29 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/22\/classification-model-for-faulty-steel-plates-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Faulty Steel Plates\nDataset ML Model: Multi-Class classification with numerical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/steel+plates+faults\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/faulty-steel-plates\nINTRODUCTION: This dataset comes from research by Semeion, Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus \u201cother\u201d). The Input vector was made up of 27 indicators that approximately the geometric shape of the defect and its outline. According to the research paper, Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.\nCONCLUSION: The baseline performance of the 10 algorithms achieved an average accuracy of 60.92%. Three algorithms (Bagged Decision Trees, Extra Trees, and Stochastic Gradient Boosting) achieved the top three accuracy scores after the first round of modeling. After a series of tuning trials, the top result achieved using the training data was from Stochastic Gradient Boosting. It achieved an average accuracy of 78.05%. Using the optimized tuning parameter available, the Stochastic Gradient Boosting processed the validation dataset with an accuracy of 80.10%, which was slightly better than with the training data alone. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Edible Mushrooms Using Python","author_name":"David Lowe","blog_date":"Tue, 19 Jun 2018 09:48:40 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/19\/binary-classification-model-for-edible-mushrooms-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Mushroom Data Set\nDataset ML Model: Binary classification with categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mushroom\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/mushroom-classification\nINTRODUCTION: This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible or definitely poisonous. The Guide, The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, clearly states that there is no simple rule for determining the edibility of a mushroom.\nCONCLUSION: It was interesting to observe that just about all algorithms (except Naive Bayes and Support Vector Machine) scored an accuracy of 100% on the training data using a training\/validation split of 70%\/30%. Furthermore, all eight algorithms also scored a 100% accuracy rate using the validation dataset.\nI reduced the training and validation to only a 50%-50% split, so the algorithms had less training data to work with. The same eight algorithms turned in a 100% accuracy on the larger validation dataset. After reducing the training and validation to a 30%-70% split, the Stochastic Gradient Boosting model dropped out of the race for predictive perfection. After a 20%-80% of training and validation split, Random Forest was the only model that was able to maintain a perfect prediction score for both the training and validation datasets.\nFor future studies, we can examine and see whether the machine learning algorithms can be trained with fewer features but still maintain the high prediction accuracy. For now, the Random Forest algorithm appeared to be the best-performing model for determining whether mushroom species are edible.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Edible Mushrooms Using R","author_name":"David Lowe","blog_date":"Fri, 15 Jun 2018 09:30:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/15\/binary-classification-model-for-edible-mushrooms-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Mushroom Data Set\nDataset ML Model: Binary classification with categorical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mushroom\nOne potential source of performance benchmarks: https:\/\/www.kaggle.com\/uciml\/mushroom-classification\nINTRODUCTION: This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible or definitely poisonous. The Guide, The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, clearly states that there is no simple rule for determining the edibility of a mushroom.\nCONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 98.65%, which was very encouraging. Four algorithms (Logistic Regression, Random Forest, AdaBoost, and Stochastic Gradient Boosting) yielded the top accuracy result of 100% using the training dataset alone. The training dataset contained 65% of the records from the original dataset (or 5,282 records), whereas the validation dataset had the remainder 35% or 2,842 records.\nAfter applying the validation dataset to the four top training algorithms, all four algorithms continued to perform and achieved the accuracy of 100% with the validation data. Considering the Logistic Regression models required the least amount of training time, the recommendation is to consider using the Logistic Regression model for all future mushroom predictions.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using Python \u2013 Take 3","author_name":"David Lowe","blog_date":"Tue, 12 Jun 2018 09:14:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/12\/regression-model-for-bike-sharing-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model.\nFor the Take No.3 of the project, we will leverage the hourly data, instead of the daily data from Take No.1. We will examine the algorithm performance and see how the hourly dataset performs against the daily data for modeling algorithms.\nCONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 118 (vs. RMSE of 1483 from the daily dataset. Three algorithms (k-Nearest Neighbors, Random Forest, and Extra Trees) achieved the lowest RMSE values during the initial modeling round. After a series of tuning trials with these three algorithms, Extra Trees produced the best RMSE value of 69 (vs. 1233 using the daily data).\nExtra Trees also processed the validation dataset with an RMSE value of 68 (vs. 1293 using the daily data), which was better than the average training result. For this project, the Extra-Trees ensemble algorithm yielded top-notch training and validation results, which warrant the additional processing required by the algorithm.\nFurthermore, the use of hourly data generally yielded significantly better RMSE values for all algorithms vs. daily data. It is, therefore, a recommended approach to leverage the predictive models by using the hourly data whenever possible.\nThe HTML-formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using Python \u2013 Take 2","author_name":"David Lowe","blog_date":"Fri, 08 Jun 2018 09:58:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/08\/regression-model-for-bike-sharing-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration (Take No.2) of the project, we attempt to use the data available, apply the one-hot-encoding transformation on each categorical attribute, and apply the Stochastic Gradient Boosting algorithm to examine the modeling effectiveness. Again, the goal of this iteration is to examine various data transformation options and find a sufficiently accurate (low error) combination for future prediction tasks.\nThis iteration of the project will test the following six modeling scenarios:\n\nScenario No.1: Perform one-hot-encoding on the categorical variable \u201cmnth\u201d and observe the change in regression accuracy.\nScenario No.2: Perform one-hot-encoding on the categorical variable \u201choliday\u201d and observe the change in regression accuracy.\nScenario No.3: Perform one-hot-encoding on the categorical variable \u201cweekday\u201d and observe the change in regression accuracy.\nScenario No.4: Perform one-hot-encoding on the categorical variable \u201cworkingday\u201d and observe the change in regression accuracy.\nScenario No.5: Perform one-hot-encoding on the categorical variable \u201cweathersit\u201d and observe the change in regression accuracy.\nScenario No.6: Perform one-hot-encoding on the all categorical variables and observe the change in regression accuracy.\n\nFor all scenarios, steps from sections No.3 and No.4 will be repeated for each scenario.\nCONCLUSION: The baseline performance of the Stochastic Gradient Boosting stands at an RMSE value of 1255 using the training data. The various scenarios achieved an average RMSE value of between 1243 and 1261. For this iteration of the project, the one-hot-encoding transformation apparently did not improve the model performance with noticeable differences.\nThe HTML-formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using Python \u2013 Take 1","author_name":"David Lowe","blog_date":"Tue, 05 Jun 2018 09:47:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/05\/regression-model-for-bike-sharing-using-python-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor available performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model. Again, the goal of this iteration is to find a sufficiently accurate (best Root Mean Squared Error or RMSE) algorithm for the future prediction tasks.\nCONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 1,483. Three algorithms (AdaBoost, Random Forest, and Stochastic Gradient Boosting) achieved the better NMSE values during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting produced the lowest RMSE value of 1,233 using the training data.\nStochastic Gradient Boosting also processed the validation dataset with an RMSE value of 1,293, which was slightly worse than the best training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using R \u2013 Take 3","author_name":"David Lowe","blog_date":"Fri, 01 Jun 2018 12:25:47 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/06\/01\/regression-model-for-bike-sharing-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model.\nFor the Take No.3 of the project, we will leverage the hourly data, instead of the daily data from Take No.1. We will examine the algorithm performance and see how the hourly dataset performs against the daily data for modeling algorithms.\nCONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 105 (vs. RMSE of 1322 from the daily dataset. Three algorithms (k-Nearest Neighbors, Random Forest, and Stochastic Gradient Boosting) achieved the lower RMSE and higher R-square values during the initial modeling round. After a series of tuning trials with these three algorithms, Random Forest produced the lowest RMSE value of 67 (vs. 1213 using the daily data) and the highest R-square value at 0.8648 (vs. 0.6093 using the daily data).\nRandom Forest also processed the validation dataset with an RMSE value of 64 (vs. 1177 using the daily data) and an R-square value of 0.8778 (vs. 0.6329 using the daily data), which was better than the average training result. For this project, the Random Forest ensemble algorithm yielded top-notch training and validation results, which warrant the additional processing required by the algorithm.\nFurthermore, the use of hourly data (vs. daily data) generally yielded significantly higher R-square values for all algorithms. It would be a recommended approach to leverage the predictive models by using the hourly data whenever possible.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using R \u2013 Take 2","author_name":"David Lowe","blog_date":"Tue, 29 May 2018 12:24:42 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/29\/regression-model-for-bike-sharing-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor available performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration (Take No.2) of the project, we attempt to use the data available, transform as necessary, and apply the Stochastic Gradient Boosting algorithm to examine the modeling effectiveness. Again, the goal of this iteration is to examine various data transformation options and find a sufficiently accurate (low error) combination for future prediction tasks.\nThis iteration of the project will test the following four modeling scenarios:\nScenario No.1: Remove the attribute \u201catemp\u201d since it was highly correlated with the attribute \u201ctemp.\u201d\nScenario No.2: Perform one-hot-encoding on the variable \u201cseason.\u201d\nScenario No.3: Perform one-hot-encoding on the variable \u201cmnth.\u201d\nFor scenarios 2-3, steps from section No.3 and No.4 will be repeated for each scenario.\nCONCLUSION: The baseline performance of the Stochastic Gradient Boosting stands at an RMSE value of 1240 and an R-square value of 0.5943 using the training data. Scenario No.1 did slightly better with an RMSE value of 1233 and an R-square value of 0.5991. As the result, we will leverage scenario No.1 to training the final model and observe how it will do with the validation dataset.\nThe final Stochastic Gradient Boosting model processed the validation dataset with an RMSE value of 1180 and an R-square value of 0.6320, which was slightly worse than the Take No.1 result of 1177 for RMSE and 0.6329 for R-square. For this iteration of the project, data transformation did not improve the model performance with a noticeable outcome.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Model for Bike Sharing Using R \u2013 Take 1","author_name":"David Lowe","blog_date":"Fri, 25 May 2018 12:32:51 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/25\/regression-model-for-bike-sharing-using-r-take-1\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nDataset Used: Bike Sharing Dataset\nDataset ML Model: Regression with numerical attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset\nFor available performance benchmarks, please consult: https:\/\/www.kaggle.com\/contactprad\/bike-share-daily-data\nINTRODUCTION: Using the data generated by a bike sharing system, this project attempts to predict the daily demand for bike sharing. For this iteration of the project, we attempt to use the data available for discovering a suitable machine learning algorithm that future predictions can use. We have kept the data transformation activities to a minimum and drop the several attributes that do not make sense to keep or simply will not help in training the model. Again, the goal of this iteration is to find a sufficiently accurate (low error) algorithm for the future prediction tasks.\nCONCLUSION: The baseline performance of predicting the target variable achieved an average RMSE value of 1322. Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the lower RMSE and higher R-square values during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting produced the lowest RMSE value of 1213 and the highest R-square value at 0.6093 using the training data.\nStochastic Gradient Boosting also processed the validation dataset with an RMSE value of 1177 and an R-square value of 0.6329, which was better than the average training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Template Using R","author_name":"David Lowe","blog_date":"Tue, 22 May 2018 12:39:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/22\/updated-machine-learning-template-using-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using R.\nVersion 2 of the templates contain time stamps at various locations within the script. I have been using the timing benchmarks to test my modeling and tuning efficiency. You will find the templates on the Machine Learning Project Templates page.\nYou can also check out the templates and sample HTML-formatted reports\u00a0here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Updated Machine Learning Template Using Python","author_name":"David Lowe","blog_date":"Fri, 18 May 2018 12:29:11 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/18\/updated-machine-learning-template-using-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a set of project templates that can be used to support regression ML problems using Python.\nVersion 2 of the templates contain time stamps at various locations within the script. I have been using the timing benchmarks to test my modeling and tuning efficiency. You will find the templates on the Machine Learning Project Templates page.\nYou can also check out the templates and sample HTML-formatted reports here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Bank Marketing Using Python, Take 3","author_name":"David Lowe","blog_date":"Tue, 15 May 2018 12:27:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/15\/binary-classification-model-for-bank-marketing-using-python-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Dataset\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 41,000 observations with 19 input variables and 1 output variable. There are no missing values within the dataset. This dataset is based on \u201cBank Marketing\u201d UCI dataset and is enriched by the addition of five new social and economic features\/attributes. This dataset is almost identical to the one without the five new attributes.\nCONCLUSION: The take No.3 version of this banking dataset aims to test the addition of five additional social-economical attributes to the dataset and the effect. You can see the results from the take No.2 here on the website.\nThe baseline performance of the ten algorithms achieved an average accuracy of 88.32% (vs. 87.68% from the take No.2 version). Three algorithms (Logistic Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved the top accuracy\/Kappa result using the training data. It produced an average accuracy of 90.06% (vs. 89.49% from the take No.2 version) using the training data.\nStochastic Gradient Boosting also processed the validation dataset with an accuracy of 90.25%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The addition of the social-economical attributes did not seem to have a substantial effect on the overall accuracy of the prediction models.\nThe HTML formatted report can be found here on the website.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Bank Marketing Using R, Take 3","author_name":"David Lowe","blog_date":"Fri, 11 May 2018 12:31:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/11\/binary-classification-model-for-bank-marketing-using-r-take-3\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Data Set\nData Set ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 41,000 observations with 19 input variables and 1 output variable. There are no missing values within the dataset. This dataset is based on \u201cBank Marketing\u201d UCI dataset and is enriched by the addition of five new social and economic features\/attributes. This dataset is almost identical to the one without the five new attributes.\nCONCLUSION: The take No.3 version of this banking dataset aims to test the addition of five additional social-economical attributes to the dataset and the effect. You can see the results from the take No.2 here on the website.\nThe baseline performance of the seven algorithms achieved an average accuracy of 89.70% (vs. 89.22% from the take No.2 version). Three algorithms (Logistic Regression, Bagged CART, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved\u00a0the top accuracy\/Kappa result using the training data. It produced an average accuracy of 90.11% (vs. 89.46% from the take No.2 version) using the training data.\nStochastic Gradient Boosting also processed the validation dataset with an accuracy of 90.00%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The addition of the social-economical attributes did not seem to have a substantial effect on the overall accuracy of the prediction models.\nThe HTML formatted report can be found here on the website.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Bank Marketing Using R, Take 2","author_name":"David Lowe","blog_date":"Tue, 08 May 2018 12:11:54 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/08\/binary-classification-model-for-bank-marketing-using-r-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Data Set\nData Set ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values within the dataset.\nCONCLUSION: The take No.2 version of this banking dataset aims to test the removal of one attribute from the dataset and the effect. You can see the results from the take No.1 here on the website.\nThe data removed was the \u201cduration\u201d attribute. According to the dataset documentation, this attribute highly affects the output target (e.g., if duration=0 then y=\u201cno\u201d). However, the duration is not known before a call is performed. Also, after the end of the call, the target variable is naturally identified. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nThe baseline performance of the seven algorithms achieved an average accuracy of 89.22% (vs. 89.99% from the take No.1). Three algorithms (Bagged CART, Random Forest, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting achieved the top accuracy\/Kappa result using the training data. It produced an average accuracy of 89.46% (vs. 90.63% from the take No.1) using the training data.\nStochastic Gradient Boosting also processed the validation dataset with an accuracy of 89.18%, which was sufficiently close to the training result. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The elimination of the \u201cduration\u201d attribute did not seem to have a substantial adverse effect on the overall accuracy of the prediction models.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Model for Bank Marketing Using Python, Take 2","author_name":"David Lowe","blog_date":"Fri, 04 May 2018 12:16:07 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/04\/binary-classification-model-for-bank-marketing-using-python-take-2\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Dataset\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.\nCONCLUSION: The take No.2 version of this banking dataset aims to test the removal of one attribute from the dataset and the effect. You can see the results from the take No.1 here on the website.\nThe data removed was the \u201cduration\u201d attribute. According to the dataset documentation, this attribute highly affects the output target (e.g., if duration=0 then y=\u201dno\u201d). However, the duration is not known before a call is performed. Also, after the end of the call, the target variable is naturally identified. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nThe baseline performance of the ten algorithms achieved an average accuracy of 87.68% (vs. 89.13% from the take No.1). Three algorithms (Linear Regression, AdaBoost, and Stochastic Gradient Boosting) achieved the top accuracy and Kappa scores during the initial modeling round. After a series of tuning trials with these three algorithms, Stochastic Gradient Boosting (SGB) achieve the top result using the training data. It produced an average accuracy of 89.49% (vs. 91.00% from the take No.1) using the training data.\nSGB also processed the validation dataset with an accuracy of 89.21% (vs. 90.58% from the take No.1). For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm. The elimination of the \u201cduration\u201d attribute did not seem to have a substantial adverse effect on the overall accuracy of the prediction models.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Installing Jupyter and R on Fedora","author_name":"David Lowe","blog_date":"Tue, 01 May 2018 12:19:44 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/05\/01\/installing-jupyter-and-r-on-fedora\/","blog_text":"\n\nThis is a replication and extension to Dr. Jason Brownlee\u2019s post. I am making these notes, so I can repeat the installation process at a later time.\nStep 1) Provision a Fedora instance using your favorite VM Manager. I happened to use VMware, but Dr. Brownlee\u2019s VirtualBox worked just fine. I recommend fully patch the Fedora installation before proceeding to the Python and R steps.\nStep 2) Verify the Python 3 installation and install the packages described by Dr. Brownlee\u2019s post.\n\n\nStep 3) Install Jupyter using pip:\n\n$ sudo python3 -m pip install \u2013upgrade pip\n$ sudo python3 -m pip install jupyter\nReview the web page http:\/\/jupyter.org\/install for more information.\n\n\n\n\n\nStep 4) Start up the Jupyter notebook server: $ jupyter notebook\n\n\nStep 5) The default browser should kick in and display the Jupyter notebook web page.\n\n\nStep 6) Install R using the command line with: $ sudo dnf install R\n\n\nStep 7) Browse RStudio web page and find the Linux client appropriate for your installation. In my case, it was the installer for \u201cFedora 19+\/RedHat 7+\/openSUSE 13.1+ (64-bit).\u201d\n\n\nStep 8) Click on \u201cInstall\u201d and then \u201cLaunch.\u201d The familiar RStudio interface should appear.\n\n\n\n\n\n\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Machine Learning Template Using Python","author_name":"David Lowe","blog_date":"Sun, 29 Apr 2018 12:04:01 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/29\/regression-machine-learning-template-using-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support regression ML problems using Python.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Bank Marketing Using Python","author_name":"David Lowe","blog_date":"Fri, 27 Apr 2018 12:25:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/27\/simple-classification-model-for-bank-marketing-using-python\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Dataset\nDataset ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.\nCONCLUSION: The baseline performance of the 11 algorithms achieved an average accuracy of 89.13%. Three algorithms (Stochastic Gradient Boosting, Random Forest, and AdaBoost) achieved the top accuracy and Kappa scores. The top result achieved using the training data was from Stochastic Gradient Boosting. It achieved an average accuracy of 91.00% after a series of tuning trials, and its accuracy in processing the validation dataset was 90.58%. For this project, the Stochastic Gradient Boosting ensemble algorithm yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Machine Learning Template Using Python","author_name":"David Lowe","blog_date":"Tue, 24 Apr 2018 12:23:27 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/24\/multi-class-machine-learning-template-using-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support multi-class ML problems using Python.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Machine Learning Template Using Python","author_name":"David Lowe","blog_date":"Sun, 22 Apr 2018 12:21:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/22\/binary-classification-machine-learning-template-using-python\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support binary classification ML problems using Python.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Bank Marketing Using R","author_name":"David Lowe","blog_date":"Fri, 20 Apr 2018 12:44:51 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/20\/simple-classification-model-for-bank-marketing-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/)\nDataset Used: Bank Marketing Data Set\nData Set ML Model: Binary classification with numerical and categorical attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\nOne source of potential performance benchmarks: https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\nINTRODUCTION: The Bank Marketing dataset involves predicting the whether the bank clients will subscribe (yes\/no) a term deposit (target variable). It is a binary (2-class) classification problem. There are over 45,000 observations with 16 input variables and 1 output variable. There are no missing values in the dataset.\nCONCLUSION: The baseline performance of eight algorithms achieved an average accuracy of 89.99%. Three algorithms (Random Forest, Stochastic Gradient Boosting, and Bagged CART) achieved the top accuracy and Kappa scores. The top result achieved using the training data was from Random Forest. It achieved an average accuracy of 90.65% after a series of tuning trials, and its accuracy in processing the validation dataset was 90.91%. For this project, the Random Forest ensemble algorithms yielded consistently top-notch training and validation results, which warrant the additional processing required by the algorithm.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Regression Machine Learning Template Using R","author_name":"David Lowe","blog_date":"Tue, 17 Apr 2018 12:34:55 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/17\/regression-machine-learning-template-using-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions for creating a machine learning template, I have pulled together a project template that can be used to support regression ML problems using R.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Multi-Class Machine Learning Template Using R","author_name":"David Lowe","blog_date":"Sun, 15 Apr 2018 12:32:35 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/15\/multi-class-machine-learning-template-using-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support multi-class ML problems using R.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Binary Classification Machine Learning Template Using R","author_name":"David Lowe","blog_date":"Fri, 13 Apr 2018 12:40:30 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/13\/binary-classification-machine-learning-template-using-r\/","blog_text":"\n\nAs I work on practicing and solving machine learning (ML) problems, I find myself repeating a set of steps and activities repeatedly.\nThanks to Dr. Jason Brownlee\u2019s suggestions on creating a machine learning template, I have pulled together a project template that can be used to support binary classification ML problems using R.\nThe purpose of the template is to show the data preparation, ML modeling using algorithms, and performance tuning steps. You can download the template from the Machine Learning Project Templates page.\nYou can also check out the sample HTML-formatted report here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Machine Learning Algorithms Catalog","author_name":"David Lowe","blog_date":"Wed, 11 Apr 2018 12:08:38 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/11\/machine-learning-algorithms-catalog\/","blog_text":"\n\nAs I work on practicing and solving machine learning problems, I often forget which algorithms I should consider applying.\nThanks to Dr. Jason Brownlee\u2019s suggestions\u00a0on learning machine learning algorithms, I have pulled together a table that summarizes some of the popular algorithms with key information about them.\nHere are the explanations of the column attributes.\n\nRegression: Is this algorithm suitable for solving regression problems, yes or no.\nTwo-Class: Is this algorithm suitable for solving binary classification problems, yes or no.\nMulti-Class: Is this algorithm suitable for solving general classification problems, yes or no.\nClustering: Is this algorithm suitable for solving clustering or unsupervised problems, yes or no.\nClass (Weka): The intended class variables designed to be solved by the algorithm. I obtained the information from Weka [http:\/\/www.cs.waikato.ac.nz\/ml\/weka].\nAttributes (Weka): The intended attributes or features that the algorithm can use. I obtained the information from Weka.\nData Prep Tips (MLM): Some key things to know handling the data for the algorithm. I credit the information to Dr. Jason Brownlee and his Machine Learning Mastery website.\nLearning Style: Supervised or Unsupervised\nAlgorithm Class: Linear, Nonlinear, Ensemble for supervised algorithms or Clustering for unsupervised algorithms\nWeka Library: The name of the algorithm implemented in Weka.\nR Caret Library: The name of the algorithm implemented in the Caret package for R [http:\/\/caret.r-forge.r-project.org].\nPython Library: The name of the algorithm implemented in the scikit-learn library for Python [http:\/\/scikit-learn.org\/].\n\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Diabetes Prediction Using R","author_name":"David Lowe","blog_date":"Fri, 06 Apr 2018 12:36:14 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/04\/06\/simple-classification-model-for-diabetes-prediction-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Pima Indians Diabetes Database\nData Set ML Model: Classification with numerical attributes\nDataset Reference: https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\nFor more information on performance benchmarks, please consult: https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\nINTRODUCTION: The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values.\nCONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 75.85%. The top accuracy result achieved via Logistic Regression was 77.73% after a series of tuning trials. The ensemble algorithms, in this case, did not yield a better result than the non-ensemble algorithms to justify the additional processing required.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Glass Type Using R","author_name":"David Lowe","blog_date":"Fri, 30 Mar 2018 12:13:13 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/30\/simple-classification-model-for-glass-type-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nhttps:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Glass Identification Data Set\nData Set ML Model: Classification with real number attributes\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Glass+Identification\nFor more information on this case study project and performance benchmarks, please consult: https:\/\/www.kaggle.com\/uciml\/glass\nThe glass identification dataset involves predicting the six types of glass, defined by their oxide content (i.e., Na, Fe, K, .and so forth). The criminological investigation was the motivation for the study of classification of types of glass. At the scene of the crime, the glass left can be used as evidence, if it is correctly identified!\nCONCLUSION: The baseline performance of predicting the class variable achieved an average accuracy of 71.45%. The top accuracy result achieved via RandomForest was 80.11% after a series of tuning trials. The ensemble algorithm, in this case, yielded a better result than the non-ensemble algorithms to justify the additional processing and tuning.\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Regression Model for Predicting Abalone Age Using R","author_name":"David Lowe","blog_date":"Sat, 24 Mar 2018 02:30:32 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/23\/simple-regression-model-for-predicting-abalone-age-using-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Abalone Data Set\nData Set ML Model: Regression with Categorical, Integer, Real attributes\nDataset Reference: http:\/\/archive.ics.uci.edu\/ml\/datasets\/Abalone\nThe Abalone Dataset involves predicting the age of abalone given objective measures of individuals. Although it was presented as a multi-class classification problem, this exercise will frame it using regression. The baseline performance of predicting the mean value is an RMSE of approximately 3.2 rings.\nCONCLUSION: The baseline performance of predicting the most prevalent class achieved an RMSE of approximately 2.28 rings. The top RMSE result achieved via SVM was 2.13 rings after a series of tuning. The ensemble algorithm did not yield a better result than SVM to justify the additional processing and tuning necessary.\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Provision a LAMP Server in the AWS GovCloud Region","author_name":"David Lowe","blog_date":"Tue, 20 Mar 2018 12:10:18 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/20\/provision-a-lamp-server-in-the-aws-govcloud-region\/","blog_text":"\n\nThis is my notes for provisioning a LAMP server in the Amazon Web Services (AWS) GovCloud region using Amazon\u2019s Linux AMI. I had to set up a LAMP server and secured it with HTTPS. These notes outlined my installation process during the week of 12 March 2018.\nMy goal for this document is to list the various reference points where you can find the step-by-step setup instructions for this provisioning task. This post will also comment on the obstacles I had run into during the provisioning, and what I needed to do get past those obstacles.\nYou can find the installation and configuration notes below.\nAbbreviations\n\nVPC-UG: Amazon Virtual Private Cloud User Guide, released 29 November 2017.\nEC2-UG: Amazon Elastic Compute Cloud User Guide for Linux Instances. There are a number of revisions for this user guide. For this project, I used the version that was released on 9 February 2018. This version talks about setting up the LAMP stack and SSL\/TLS with the original Amazon Linux AMI. The more recent user guides cover the newer Amazon Linux 2 AMI.\nROUTE53-DG: Amazon Route 53 Developer Guide, released 5 December 2017.\n\nRequirements:\nNeed to provision a Linux server with the LAMP stack in AWS GovCloud region. The server will host PHP-based web applications. All HTTP traffic will be re-directed and forced to use HTTPS.\nBackground and Prerequisite Information\n\nNeeded to sign-up for an AWS GovCloud account. The GovCloud account is going to be a separate account from your regular AWS account with a different account ID. See the web page, https:\/\/docs.aws.amazon.com\/govcloud-us\/latest\/UserGuide\/getting-started-sign-up.html, for signing up.\n\nTools Used\n\nWeb browsers\nAWS Console\nPuTTY\nWinSCP\n\nConfigure virtual private cloud (VPC)\nWhen setting up the GovCloud account, AWS created a default VPC for that account. That default VPC had three subnets configured with an Internet gateway attached to each one of them.\nFor the one and only public-Internet-accessible server I needed, the default VPC would have worked. I did not use the default VPC. Instead, I rolled my own VPC with one public subnet and one private subnet.\nIf you need more info on Amazon VPC, check out their VPC-UG or many documented instructions available on the net. For training purposes, I personally use the AWS course with Linux Academy. [https:\/\/linuxacademy.com\/]\nProvision an instance with the Amazon Linux AMI and Elastic IP\n\nProvision a Linux instance with Elastic IP: Because the Amazon Linux 2 AMI is not yet available on GovCloud, I had to use the version 1 of the Linux AMI.\nSet up the DNS entry: I had an inactive domain, so I used Route 53 to configure the DNS zone and host entries. Please note that for a server hosted in GovCloud, the Route 53 settings will need to be configured from the non-GovCloud console. Another word, you will not find Route 53 in the GovCloud console. I used the instructions outline in ROUTE53-DG pages 213-216.\nInstall the LAMP Stack: The instructions on pages 41-46 held up well for my installation. I configured Apache for HTTPS in the following section.\nEnable SSL\/TLS: Refer to pages 58-72 in EC2-UG. Since I did not have a certificate to use, I opted to obtain a cert with EFF through the Let\u2019s Encrypt program. The certbot instructions described on pages 69-72 worked only partially. Certbot aborted in the middle of the installation and complained about not able to find a virtual host. Repeated attempts of certbot -auto option did not work for me. I finally resorted to this blog post [https:\/\/nouveauframework.org\/blog\/installing-letsencrypts-free-ssl-amazon-linux\/] as it helped me generate the three cert files I needed. I updated the ssl.conf manually and got the HTTPS going. At this point, the HTTP was not being redirected to HTTPS. Another similar inquiry on Stack Exchange pointed me to run the \u201ccertbot -apache\u201d command. By doing that, I was able to force the HTTP to HTTPS redirect without needing to go hunt down the config file I needed to update. I also configured certbot in crontab to check for auto-renew daily.\nInstall phpMyAdmin: Needed to do a few things with PHP config because phpMyAdmin was complaining about the BlowFish secret and configuration storage. This blog post got me through the configuration tasks [https:\/\/www.digitalocean.com\/community\/questions\/phpmyadmin-or-alternative-for-php7-nginx-mysql-5-7-ubuntu-16-04].\n\nPost-AppGini App Deployment\nAfter deploying the AppGini app, two more things surfaced.\n\nThe GD support in PHP was requested. It was a matter of installing the correct GD version by using the commands \u201cphp -version\u201d and \u201csudo yum install php70-gd\u201d for the PHP 7.0 environment I assembled.\nI also needed to reset the ownership and file permissions for the \u201cimages\u201d folder inside my AppGini application. AppGini was complaining about not able to write into it. I followed the instructions from page 44 of EC2-UG and resolved that.\n\nThis is what I wrote and could think of so far. If you will be attempting a similar installation, I hope these can help in some way. My next project is doing a similar install using Amazon Linux 2 AMI in a non-GovCloud environment. I will be checking out the Linux AMI version 2 instructions in the user guide. Will write down what I run into and share my findings later.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Ensemble Classification Model for the Sonar Dataset with R","author_name":"David Lowe","blog_date":"Fri, 16 Mar 2018 12:33:28 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/16\/ensemble-classification-model-for-the-sonar-dataset-with-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set\nML Model: Classification, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29\nThe Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.\nCONCLUSION: The baseline performance of predicting the most prevalent class achieved an accuracy of approximately 76.0%. Top results achieved via SVM was approximately 85.06% after a series of tuning. The RandomForest ensemble algorithm, also after tuning, yielded an accuracy of 85.09%. The very slight improvement between RF and SVM was too small to justify the additional processing and tuning required by the ensemble algorithm.\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for the Sonar Dataset with R","author_name":"David Lowe","blog_date":"Tue, 13 Mar 2018 12:28:56 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/13\/simple-classification-model-for-the-sonar-dataset-with-r\/","blog_text":"\n\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set\nML Model: Classification, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29\nThe Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.\nThe baseline performance of predicting the most prevalent class is a classification accuracy of approximately 76.0%. Top results achieve a classification accuracy of approximately 84.7%.\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Text Messages with Python","author_name":"David Lowe","blog_date":"Sun, 11 Mar 2018 05:10:38 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/10\/simple-classification-model-for-text-messages-with-python\/","blog_text":"\n\nMethodology Credit: Re-produced and adapted from a tutorial made available by Evgeny Volkov, SMS Spam Detection with Various Classifiers.\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nData Set Description: https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset\nOriginal Reference: http:\/\/www.dt.fee.unicamp.br\/~tiago\/smsspamcollection\/\nModeling Approach: binary classification\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according to being ham (legitimate) or spam.\nWe will be spot-checking a suite of linear and nonlinear machine learning algorithms and comparing the estimated accuracy of algorithms. For this project, we will evaluate 9 different algorithms:\nLinear Algorithms: Logistic Regression (LR)\nNonlinear Algorithms: Decision Tree (DTC), Support Vector Machine (SVC), Multinomial Native Bayes (MNB) and k-Nearest Neighbors (KNC)\nEnsemble Algorithms: Random Forest (RFC), AdaBoost (ABC), Bagging (BC), and ExtraTree (ETC)\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for Text Messages with R","author_name":"David Lowe","blog_date":"Wed, 07 Mar 2018 23:43:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/03\/07\/simple-classification-model-for-text-messages\/","blog_text":"\n\nMethodology Credit: Re-produced and adapted from a tutorial made available by Anish Singh Walia, Text Message Classification.\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nData Set Description: https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset\nOriginal Reference: http:\/\/www.dt.fee.unicamp.br\/~tiago\/smsspamcollection\/\nModeling Approach: binary classification\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according to being ham (legitimate) or spam.\nWorking through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.\nAny predictive modeling machine learning project can be broken down into about 6 common tasks:\n\nDefine Problem\nSummarize Data (Use the word cloud visualization technique for this project)\nPrepare Data (Not required for this project)\nEvaluate Algorithms (Use Naive Bayes classifier and measure accuracy)\nImprove Accuracy or Results\nFinalize Model and Present Results\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification Model for the Sonar Dataset with Python","author_name":"David Lowe","blog_date":"Wed, 28 Feb 2018 13:43:15 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/28\/simple-classification-model-for-the-sonar-dataset\/","blog_text":"\n\nTemplate Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/standard-machine-learning-datasets\/.\nDataset Used: Connectionist Bench (Sonar, Mines vs. Rocks) Data Set\nML Model: Classification, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29\nThe Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem.\nThe baseline performance of predicting the most prevalent class is a classification accuracy of approximately 53%. Top results achieve a classification accuracy of approximately 88%.\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Regression Ensemble Model for Boston Housing with Python","author_name":"David Lowe","blog_date":"Sat, 24 Feb 2018 13:02:36 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/24\/simple-regression-ensemble-model-for-boston-housing-with-python\/","blog_text":"\n\nCredit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/regression-machine-learning-tutorial-weka\/.\nDataset Used: Housing Values in Suburbs of Boston\nML Model: Regression, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Housing\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore feature selection options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nFor this \u201cTake-2\u201d version of the project, we added the ensemble models to the exploration.\n\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Regression Baseline Model for Boston Housing Price with Python","author_name":"David Lowe","blog_date":"Wed, 21 Feb 2018 13:01:01 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/21\/simple-regression-baseline-model-for-boston-housing-price-with-python\/","blog_text":"\n\nCredit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/regression-machine-learning-tutorial-weka\/.\nDataset Used: Housing Values in Suburbs of Boston\nML Model: Regression, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Housing\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\nDocument a regression predictive modeling problem end-to-end.\nExplore feature selection options for improving model performance\nExplore algorithm tuning techniques for improving model performance\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification of Titanic Dataset, Take 3","author_name":"David Lowe","blog_date":"Sat, 17 Feb 2018 13:53:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/17\/simple-classification-of-titanic-dataset-take-3\/","blog_text":"\n\nMethodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/).\nData Set Description: https:\/\/www.kaggle.com\/c\/titanic\/data\nBenchmark References: https:\/\/www.kaggle.com\/c\/titanic\/data\nFor the take #3 version of the project, we will add the fourth and fifth iterations of experimenting with several machine learning algorithms. We will see whether the machine learning algorithms can improve our predictions.\n\nLabel all passengers dead or the attribute $Survived = 0 (The worst-case scenario)\nLabel all female passengers survived or the attribute $Survived = 1\nLabel all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0\nLeverage machine learning algorithms to generate predictions\nTune the best-performing algorithm by experimenting various parameters\nLeverage ensembles of algorithm to generate predictions\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification of Titanic Dataset, Take 2","author_name":"David Lowe","blog_date":"Wed, 14 Feb 2018 13:47:46 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/14\/simple-classification-of-titanic-dataset-take-2\/","blog_text":"\n\nMethodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/).\nData Set Description: https:\/\/www.kaggle.com\/c\/titanic\/data\nBenchmark References: https:\/\/www.kaggle.com\/c\/titanic\/data\nFor the take #2 version of the project, we will add the fourth and fifth iterations of experimenting with several machine learning algorithms. We will see whether the machine learning algorithms can improve our predictions.\n\nLabel all passengers dead or the attribute $Survived = 0 (The worst-case scenario)\nLabel all female passengers survived or the attribute $Survived = 1\nLabel all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0\nLeverage machine learning algorithms to generate predictions\nTune the best-performing algorithm by experimenting various parameters\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Classification of Titanic Dataset, Take 1","author_name":"David Lowe","blog_date":"Sat, 10 Feb 2018 13:29:28 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/10\/simple-classification-of-titanic-dataset-take-1\/","blog_text":"\n\nMethodology Credit: Adapted from a tutorial made available by Trevor Stephens, Titanic: Getting Started With R.\nTemplate Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery (http:\/\/machinelearningmastery.com\/).\nData Set Description: https:\/\/www.kaggle.com\/c\/titanic\/data\nBenchmark References: https:\/\/www.kaggle.com\/c\/titanic\/data\nFor the take #1 version of the project, I have done a few iterations of generating predictions and submitting them to Kaggle to gather the accuracy metric. The iterations are:\n\nLabel all passengers dead or the attribute $Survived = 0 (The worst-case scenario)\nLabel all female passengers survived or the attribute $Survived = 1\nLabel all female passengers with Pclass=3 and Fare > 20 dead or the attribute $Survived = 0\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Binary Classification for Breast Cancer with Ensemble Models","author_name":"David Lowe","blog_date":"Mon, 05 Feb 2018 04:23:08 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/02\/04\/simple-binary-classification-for-breast-cancer-with-ensemble-models\/","blog_text":"\n\nTemplate Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nData Set Description: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Original)\nBenchmark References: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\nModeling Approach: binary classification, converting categorical to numerical attributes\nWorking through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.\nWe will compare several different algorithms and determine which one would yield the best results. The project aims to touch on the following areas:\n\nDocument a classification predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nFor this \u201cTake-2\u201d version of the project, we added the ensemble models to the exploration.\n\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Binary Classification for Breast Cancer Dataset","author_name":"David Lowe","blog_date":"Sun, 28 Jan 2018 13:59:37 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/01\/28\/simple-binary-classification-for-breast-cancer-dataset\/","blog_text":"\n\nTemplate Credit: Adapted from template made available by Dr. Jason Brownlee of Machine Learning Mastery.\nData Set Description: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Original)\nBenchmark References: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\nModeling Approach: binary classification, converting categorical to numerical attributes\nWorking through machine learning problems from end-to-end requires a structured modeling approach. Working problems through a project template can encourage you to think about the problem more critically, to challenge your assumptions, and to get good at all parts of a modeling project.\nWe will compare several different algorithms and determine which one would yield the best results. The project aims to touch on the following areas:\n\nDocument a classification predictive modeling problem end-to-end.\nExplore data transformation options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Regression Model for Boston Housing with Ensemble Models","author_name":"David Lowe","blog_date":"Sun, 21 Jan 2018 13:53:27 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/01\/21\/simple-regression-model-for-boston-housing-with-ensemble-models\/","blog_text":"\n\nCredit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/regression-machine-learning-tutorial-weka\/.\nDataset Used: Housing Values in Suburbs of Boston\nML Model: Regression, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Housing\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore feature selection options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nFor this \u201cTake-2\u201d version of the project, we added the ensemble models to the exploration.\n\nExplore using and tuning ensemble methods for improving model performance\n\nThe HTML formatted report can be found here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"},{"blog_title":"Simple Regression Model for Boston Housing Price","author_name":"David Lowe","blog_date":"Mon, 15 Jan 2018 00:21:00 +0000","blog_url":"https:\/\/dainesanalytics.blog\/2018\/01\/14\/simple-regression-model-for-boston-housing-price\/","blog_text":"\n\nCredit: Template and study cases were adapted from blog posts made available by Dr. Jason Brownlee of Machine Learning Mastery.\nFor more information on this case study project, please consult Dr. Brownlee\u2019s blog post at https:\/\/machinelearningmastery.com\/regression-machine-learning-tutorial-weka\/.\nDataset Used: Housing Values in Suburbs of Boston\nML Model: Regression, numeric inputs\nDataset Reference: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Housing\nThe purpose of this project is to analyze a dataset using various machine learning algorithms and to document the steps using a template. The project aims to touch on the following areas:\n\nDocument a regression predictive modeling problem end-to-end.\nExplore feature selection options for improving model performance\nExplore algorithm tuning techniques for improving model performance\n\nThe HTML formatted report can be found\u00a0here on GitHub.\nShare this:EmailLinkedInTwitterFacebookPrintLike this:Like Loading... \n"}]